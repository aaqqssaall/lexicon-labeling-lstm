{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckyKVkqCGvjd"
      },
      "source": [
        "#Text Classification - Lexicon-based Auto Labelling - LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYmQ0sSarBNK"
      },
      "source": [
        "## Tahapan :\n",
        "1. Import dan Install Library\n",
        "2. Data Information\n",
        "3. Auto Labelling\n",
        "4. Preprocessing\n",
        "5. Weighting / Sentence Conversion / Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0by_tBHOFyn"
      },
      "source": [
        "##1. Import and Install Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF7JX6NZOJnm",
        "outputId": "12b96b77-8293-48cd-aaea-1639d61c4817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.7/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: afinn in /usr/local/lib/python3.7/dist-packages (0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import numpy as np\n",
        "from urllib.request import urlopen\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# !pip install textattack\n",
        "# from textattack.augmentation import EasyDataAugmenter\n",
        "# eda_aug = EasyDataAugmenter()\n",
        "\n",
        "#preprocessing\n",
        "##casefolding\n",
        "import re \n",
        "##tokenize, stopword, stemming\n",
        "import nltk\n",
        "import gensim\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "\n",
        "#vader sentiment\n",
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "#textblog\n",
        "# !pip install textblob\n",
        "# import textblob\n",
        "\n",
        "#afinn\n",
        "!pip install afinn\n",
        "from afinn import Afinn\n",
        "afn = Afinn(emoticons = True)\n",
        "\n",
        "#sentiwordnet\n",
        "import ssl\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "\n",
        "import time\n",
        "import os\n",
        "\n",
        "#vectorize and data split\n",
        "##tfidf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "##data split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#word embedding & LSTM\n",
        "import joblib\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import gensim\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.layers import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LSTM,Dropout\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
        "\n",
        "#classsify \n",
        "##evaulate\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#export model\n",
        "import pickle\n",
        "\n",
        "# #validate\n",
        "# ##wordcloud\n",
        "# from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "#k-fold cross validation\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPe02Y2rjJBp"
      },
      "source": [
        "##2. Data Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIYQQyvByDS6"
      },
      "source": [
        "###Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTm1zuaimCDh",
        "outputId": "0fd4a3f4-df68-4cc3-9f05-b1d3d4325791"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
              "0  570306133677760513           neutral                        1.0000   \n",
              "1  570301130888122368          positive                        0.3486   \n",
              "2  570301083672813571           neutral                        0.6837   \n",
              "3  570301031407624196          negative                        1.0000   \n",
              "4  570300817074462722          negative                        1.0000   \n",
              "\n",
              "  negativereason  negativereason_confidence         airline  \\\n",
              "0            NaN                        NaN  Virgin America   \n",
              "1            NaN                     0.0000  Virgin America   \n",
              "2            NaN                        NaN  Virgin America   \n",
              "3     Bad Flight                     0.7033  Virgin America   \n",
              "4     Can't Tell                     1.0000  Virgin America   \n",
              "\n",
              "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
              "0                    NaN     cairdin                 NaN              0   \n",
              "1                    NaN    jnardino                 NaN              0   \n",
              "2                    NaN  yvonnalynn                 NaN              0   \n",
              "3                    NaN    jnardino                 NaN              0   \n",
              "4                    NaN    jnardino                 NaN              0   \n",
              "\n",
              "                                                text tweet_coord  \\\n",
              "0                @VirginAmerica What @dhepburn said.         NaN   \n",
              "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
              "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
              "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
              "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
              "\n",
              "               tweet_created tweet_location               user_timezone  \n",
              "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
              "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
              "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
              "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
              "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ea278218-0a29-4c6a-9338-33a3e0d62ce7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea278218-0a29-4c6a-9338-33a3e0d62ce7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ea278218-0a29-4c6a-9338-33a3e0d62ce7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ea278218-0a29-4c6a-9338-33a3e0d62ce7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "url = 'https://drive.google.com/file/d/165stcwTpdrkQPzYNOrickao7s5_No6q1/view?usp=sharing'\n",
        "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "\n",
        "\n",
        "df = pd.read_csv(path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcxQdGQuL6LR"
      },
      "source": [
        "###Get Required Column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX8N356FjPgp",
        "outputId": "af8450ef-1c8e-4085-b45b-dc93e9d40d63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text     label  \\\n",
              "0                                        What  said.   neutral   \n",
              "1   plus you've added commercials to the experien...  positive   \n",
              "2   I didn't today... Must mean I need to take an...   neutral   \n",
              "3   it's really aggressive to blast obnoxious \"en...  negative   \n",
              "4           and it's a really big bad thing about it  negative   \n",
              "\n",
              "   airline_sentiment_confidence  \n",
              "0                        1.0000  \n",
              "1                        0.3486  \n",
              "2                        0.6837  \n",
              "3                        1.0000  \n",
              "4                        1.0000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11d5b1c0-a612-4342-86d3-5e3ce05c2fe4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What  said.</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>plus you've added commercials to the experien...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I didn't today... Must mean I need to take an...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it's really aggressive to blast obnoxious \"en...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>and it's a really big bad thing about it</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11d5b1c0-a612-4342-86d3-5e3ce05c2fe4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-11d5b1c0-a612-4342-86d3-5e3ce05c2fe4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-11d5b1c0-a612-4342-86d3-5e3ce05c2fe4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data = df[['text', 'airline_sentiment','airline_sentiment_confidence']].copy()\n",
        "data = data.rename(columns={'airline_sentiment':'label'})\n",
        "data['text'] = data['text'].apply(lambda x : x.encode('ISO-8859-1', 'ignore').decode(\"utf-8\", 'ignore'))\n",
        "\n",
        "username_pattern = re.compile(r\"@(\\w+)\")\n",
        "data['text'] = data['text'].apply(lambda x : re.sub(username_pattern, '', x))\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W8AXk4e2GGv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1QVvAT-UBAr"
      },
      "source": [
        "###General Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHb2FThDiCOY",
        "outputId": "fbb40110-d31c-430f-8a62-5759786b62b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14640, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10J7EAHvYuj3",
        "outputId": "665ff6fe-febc-4dae-aedd-d3155d25c2a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "negative    9178\n",
              "neutral     3099\n",
              "positive    2363\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data.groupby('label')['label'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzuGrPyHf43H",
        "outputId": "9918c6c3-916e-472c-eba5-588255e6f652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14640 entries, 0 to 14639\n",
            "Data columns (total 3 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   text                          14640 non-null  object \n",
            " 1   label                         14640 non-null  object \n",
            " 2   airline_sentiment_confidence  14640 non-null  float64\n",
            "dtypes: float64(1), object(2)\n",
            "memory usage: 343.2+ KB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCbrWT3-f75G",
        "outputId": "72ac502b-3684-4227-807d-befcf7192ea7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       airline_sentiment_confidence\n",
              "count                  14640.000000\n",
              "mean                       0.900169\n",
              "std                        0.162830\n",
              "min                        0.335000\n",
              "25%                        0.692300\n",
              "50%                        1.000000\n",
              "75%                        1.000000\n",
              "max                        1.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8f67ded6-7120-40ca-a834-ea88d228cddb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>14640.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.900169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.162830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.335000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.692300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f67ded6-7120-40ca-a834-ea88d228cddb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8f67ded6-7120-40ca-a834-ea88d228cddb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8f67ded6-7120-40ca-a834-ea88d228cddb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xPV96Jhg6k1"
      },
      "source": [
        "### Check Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxiq5EX6g-y1",
        "outputId": "0224e931-e445-4417-b565-ed500c34487b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text                            0\n",
              "label                           0\n",
              "airline_sentiment_confidence    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp6QKfdkNOKy"
      },
      "source": [
        "##3. Automatic Labelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD0PmqweYQik"
      },
      "source": [
        "### VADER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtg_RQcBNbFB"
      },
      "source": [
        "####Create Sentiment Score Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iipq3TbXNieD"
      },
      "outputs": [],
      "source": [
        "analyzer = SentimentIntensityAnalyzer()\n",
        "def vader_score(sentence):\n",
        "    sntmnt = analyzer.polarity_scores(sentence)['compound']\n",
        "    return sntmnt\n",
        "\n",
        "def vader_label(score):\n",
        "    if (score >= 0.05):\n",
        "        return 'positive'\n",
        "    elif ((score > -0.05) & (score < 0.05)):\n",
        "        return 'neutral'\n",
        "    elif (score <= -0.05):\n",
        "        return 'negative'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "telLcPzPNtYI"
      },
      "source": [
        "####Calculate Sentiment Score & Create Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh7TlDC6NmhK"
      },
      "outputs": [],
      "source": [
        "# start = time.time()\n",
        "\n",
        "# eng_snt_score =  []\n",
        "\n",
        "# for text in data.text.to_list():\n",
        "#     snts_score = calculate_sentiment_scores(text)\n",
        "#     eng_snt_score.append(snts_score)\n",
        "\n",
        "data['vader_score'] = data.text.apply(vader_score)\n",
        "data['vader_label'] = data.vader_score.apply(vader_label)\n",
        "    \n",
        "# end = time.time()\n",
        "\n",
        "# # total time taken\n",
        "# print(f\"Runtime of the program is {(end - start)/60} minutes or {(end - start)} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "daHnbOon8mzG",
        "outputId": "60e2603a-7a71-4a0d-bb94-21e93abb74f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text     label  \\\n",
              "0                                        What  said.   neutral   \n",
              "1   plus you've added commercials to the experien...  positive   \n",
              "2   I didn't today... Must mean I need to take an...   neutral   \n",
              "3   it's really aggressive to blast obnoxious \"en...  negative   \n",
              "4           and it's a really big bad thing about it  negative   \n",
              "\n",
              "   airline_sentiment_confidence  vader_score vader_label  \n",
              "0                        1.0000       0.0000     neutral  \n",
              "1                        0.3486       0.0000     neutral  \n",
              "2                        0.6837       0.0000     neutral  \n",
              "3                        1.0000      -0.2716    negative  \n",
              "4                        1.0000      -0.5829    negative  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c879160d-d182-47dc-9692-5105d83fb16a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>vader_score</th>\n",
              "      <th>vader_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What  said.</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>plus you've added commercials to the experien...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I didn't today... Must mean I need to take an...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it's really aggressive to blast obnoxious \"en...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>-0.2716</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>and it's a really big bad thing about it</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>-0.5829</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c879160d-d182-47dc-9692-5105d83fb16a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c879160d-d182-47dc-9692-5105d83fb16a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c879160d-d182-47dc-9692-5105d83fb16a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo5bV1CiLc_l",
        "outputId": "ea9cc73a-2c5a-44c4-db35-d4f938f54ac2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.981"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "data['vader_score'].max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zugYznWT8qfg"
      },
      "source": [
        "### AFINN-111"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0IjCgBg8qfp"
      },
      "source": [
        "####Create Afinn-111 Classification Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhXd0IVY8qfp"
      },
      "outputs": [],
      "source": [
        "# def make_sentences(text):\n",
        "#     sentences = [sent for sent in split_single(text)]\n",
        "#     return sentences\n",
        "def afinn_score(sentence):\n",
        "    score = afn.score(sentence)\n",
        "    return score\n",
        "\n",
        "def afinn_label(score):\n",
        "    if (score > 0):\n",
        "        return 'positive'\n",
        "    elif (score < 0):\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "# def get_predict_result(sentences):\n",
        "#     \"\"\" Call predict on every sentence of a text \"\"\"\n",
        "#     flair_confidence = []\n",
        "    \n",
        "#     for i in range(0, len(sentences)): \n",
        "#         flair_confidence.append(flair_predict(sentences[i]))\n",
        "#     return flair_confidence\n",
        "\n",
        "# def flair_predict_confidence(sentence):\n",
        "#     text = Sentence(sentence)\n",
        "#     classifier.predict(text)\n",
        "#     return text.labels[0].to_dict()['confidence']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(afinn_score('omg'))\n",
        "print(afinn_score('finally'))\n",
        "print(afinn_label(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5uvZ2P3-icg",
        "outputId": "bf3903bb-d7ae-47a9-b19a-eb0e800c5c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "0.0\n",
            "neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH7ITQHB8qfq"
      },
      "source": [
        "####Calculate Sentiment Score & Create Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "891gE_zZ8qfq"
      },
      "outputs": [],
      "source": [
        "# data['sentences'] = data.text.apply(make_sentences)\n",
        "data['afinn_score'] = data.text.apply(afinn_score)\n",
        "data['afinn_label'] = data.afinn_score.apply(afinn_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7YM7T3pL8qfq",
        "outputId": "79f0db65-eeb6-4f61-fea4-6f2b9fe12410"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text     label  \\\n",
              "0                                        What  said.   neutral   \n",
              "1   plus you've added commercials to the experien...  positive   \n",
              "2   I didn't today... Must mean I need to take an...   neutral   \n",
              "3   it's really aggressive to blast obnoxious \"en...  negative   \n",
              "4           and it's a really big bad thing about it  negative   \n",
              "\n",
              "   airline_sentiment_confidence  vader_score vader_label  afinn_score  \\\n",
              "0                        1.0000       0.0000     neutral          0.0   \n",
              "1                        0.3486       0.0000     neutral          0.0   \n",
              "2                        0.6837       0.0000     neutral          0.0   \n",
              "3                        1.0000      -0.2716    negative         -5.0   \n",
              "4                        1.0000      -0.5829    negative         -2.0   \n",
              "\n",
              "  afinn_label  \n",
              "0     neutral  \n",
              "1     neutral  \n",
              "2     neutral  \n",
              "3    negative  \n",
              "4    negative  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce93df8c-b524-405f-83c4-2b63b66c1879\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>vader_score</th>\n",
              "      <th>vader_label</th>\n",
              "      <th>afinn_score</th>\n",
              "      <th>afinn_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What  said.</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>plus you've added commercials to the experien...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I didn't today... Must mean I need to take an...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it's really aggressive to blast obnoxious \"en...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>-0.2716</td>\n",
              "      <td>negative</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>and it's a really big bad thing about it</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>-0.5829</td>\n",
              "      <td>negative</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce93df8c-b524-405f-83c4-2b63b66c1879')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ce93df8c-b524-405f-83c4-2b63b66c1879 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ce93df8c-b524-405f-83c4-2b63b66c1879');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt-VXnBTHuwt"
      },
      "source": [
        "###SentiWordNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD_QR1hMKEQs"
      },
      "source": [
        "####Create SWN Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZd-cgZtHuCG"
      },
      "outputs": [],
      "source": [
        "#SWN postagging\n",
        "pos=neg=obj=count=0\n",
        "\n",
        "postagging = []\n",
        "\n",
        "for text in data['text']:\n",
        "    list = nltk.word_tokenize(text)\n",
        "    postagging.append(nltk.pos_tag(list))\n",
        "\n",
        "data['pos_tags'] = postagging\n",
        "\n",
        "def penn_to_wn(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    elif tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    return None\n",
        "\n",
        "# Returns list of pos-neg and objective score. But returns empty list if not present in senti wordnet.\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def get_sentiment(word,tag):\n",
        "    wn_tag = penn_to_wn(tag)\n",
        "    \n",
        "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
        "        return []\n",
        "\n",
        "    #Lemmatization\n",
        "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
        "    if not lemma:\n",
        "        return []\n",
        "\n",
        "    #Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. \n",
        "    #Synset instances are the groupings of synonymous words that express the same concept. \n",
        "    #Some of the words have only one Synset and some have several.\n",
        "    synsets = wn.synsets(word, pos=wn_tag)\n",
        "    if not synsets:\n",
        "        return []\n",
        "\n",
        "    # Take the first sense, the most common\n",
        "    synset = synsets[0]\n",
        "    swn_synset = swn.senti_synset(synset.name())\n",
        "\n",
        "    return [synset.name(), swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n",
        "\n",
        "    pos=neg=obj=count=0\n",
        "    \n",
        "    ###################################################################################\n",
        "def swn_label(score):\n",
        "    if score>= 0.05:\n",
        "        return 'positive'\n",
        "    elif score<= -0.05:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4fYd7JXKNjH"
      },
      "source": [
        "####Calculate SWN Score & Create Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A48KwS1iKBpd"
      },
      "outputs": [],
      "source": [
        "swn_score = []\n",
        "\n",
        "for pos_val in data['pos_tags']:\n",
        "    senti_val = [get_sentiment(x,y) for (x,y) in pos_val]\n",
        "    for score in senti_val:\n",
        "        try:\n",
        "            pos = pos + score[1]  #positive score is stored at 2nd position\n",
        "            neg = neg + score[2]  #negative score is stored at 3rd position\n",
        "        except:\n",
        "            continue\n",
        "    swn_score.append(pos - neg)\n",
        "    pos=neg=0   \n",
        "\n",
        "data = data.drop('pos_tags', 1)\n",
        "    \n",
        "data['swn_score'] = swn_score\n",
        "data['swn_label'] = data.swn_score.apply(swn_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4O1XbaoK1JU",
        "outputId": "eb80e490-13ab-40a1-b5c3-ba00a455f102"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text     label  \\\n",
              "14540   tried ringing but told me to try again Late F...  negative   \n",
              "14541   You guys did an amazing job today! Know its h...  positive   \n",
              "14542  . cover a rental car or refund our ticket? - I...  negative   \n",
              "14543   should reconsider #usairways acquisition. Fli...  negative   \n",
              "14544     You neglected to mention the $200 fee per t...  negative   \n",
              "...                                                  ...       ...   \n",
              "14635   thank you we got on a different flight to Chi...  positive   \n",
              "14636   leaving over 20 minutes Late Flight. No warni...  negative   \n",
              "14637    Please bring American Airlines to #BlackBerry10   neutral   \n",
              "14638   you have my money, you change my flight, and ...  negative   \n",
              "14639   we have 8 ppl so we need 2 know how many seat...   neutral   \n",
              "\n",
              "       airline_sentiment_confidence  vader_score vader_label  afinn_score  \\\n",
              "14540                        1.0000       0.0000     neutral          0.0   \n",
              "14541                        1.0000       0.7835    positive          2.0   \n",
              "14542                        1.0000       0.1759    positive         -2.0   \n",
              "14543                        1.0000      -0.5562    negative          0.0   \n",
              "14544                        1.0000      -0.5267    negative         -2.0   \n",
              "...                             ...          ...         ...          ...   \n",
              "14635                        0.3487       0.3612    positive          2.0   \n",
              "14636                        1.0000      -0.4043    negative         -7.0   \n",
              "14637                        1.0000       0.3182    positive          1.0   \n",
              "14638                        1.0000       0.5027    positive          2.0   \n",
              "14639                        0.6771       0.0772    positive          0.0   \n",
              "\n",
              "      afinn_label  swn_score swn_label  \n",
              "14540     neutral      0.625  positive  \n",
              "14541    positive     -0.875  negative  \n",
              "14542    negative      0.250  positive  \n",
              "14543     neutral      0.000   neutral  \n",
              "14544    negative     -0.125  negative  \n",
              "...           ...        ...       ...  \n",
              "14635    positive      0.625  positive  \n",
              "14636    negative     -1.125  negative  \n",
              "14637    positive      0.000   neutral  \n",
              "14638    positive     -0.125  negative  \n",
              "14639     neutral      0.375  positive  \n",
              "\n",
              "[100 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eeecaeeb-df1d-43e1-9e48-23dddcc8674f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>vader_score</th>\n",
              "      <th>vader_label</th>\n",
              "      <th>afinn_score</th>\n",
              "      <th>afinn_label</th>\n",
              "      <th>swn_score</th>\n",
              "      <th>swn_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14540</th>\n",
              "      <td>tried ringing but told me to try again Late F...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.625</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14541</th>\n",
              "      <td>You guys did an amazing job today! Know its h...</td>\n",
              "      <td>positive</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.7835</td>\n",
              "      <td>positive</td>\n",
              "      <td>2.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.875</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14542</th>\n",
              "      <td>. cover a rental car or refund our ticket? - I...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.1759</td>\n",
              "      <td>positive</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.250</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14543</th>\n",
              "      <td>should reconsider #usairways acquisition. Fli...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>-0.5562</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.000</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14544</th>\n",
              "      <td>You neglected to mention the $200 fee per t...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>-0.5267</td>\n",
              "      <td>negative</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.125</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14635</th>\n",
              "      <td>thank you we got on a different flight to Chi...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3487</td>\n",
              "      <td>0.3612</td>\n",
              "      <td>positive</td>\n",
              "      <td>2.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.625</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14636</th>\n",
              "      <td>leaving over 20 minutes Late Flight. No warni...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>-0.4043</td>\n",
              "      <td>negative</td>\n",
              "      <td>-7.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-1.125</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14637</th>\n",
              "      <td>Please bring American Airlines to #BlackBerry10</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.3182</td>\n",
              "      <td>positive</td>\n",
              "      <td>1.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.000</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14638</th>\n",
              "      <td>you have my money, you change my flight, and ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.5027</td>\n",
              "      <td>positive</td>\n",
              "      <td>2.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.125</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14639</th>\n",
              "      <td>we have 8 ppl so we need 2 know how many seat...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6771</td>\n",
              "      <td>0.0772</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.375</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eeecaeeb-df1d-43e1-9e48-23dddcc8674f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eeecaeeb-df1d-43e1-9e48-23dddcc8674f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eeecaeeb-df1d-43e1-9e48-23dddcc8674f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "data.tail(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J98M83DPvQs"
      },
      "source": [
        "###Bing Liu "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imioK5uLgJ26"
      },
      "source": [
        "####Load Bing Liu Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCUmvxzLP021",
        "outputId": "94f878f1-b434-4134-c440-ada24d54d4e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a+', 'abound', 'abounds', 'abundance', 'abundant', 'accessable', 'accessible', 'acclaim', 'acclaimed', 'acclamation', 'accolade', 'accolades', 'accommodative', 'accomodative', 'accomplish', 'accomplished', 'accomplishment', 'accomplishments', 'accurate', 'accurately', 'achievable', 'achievement', 'achievements', 'achievible', 'acumen', 'adaptable', 'adaptive', 'adequate', 'adjustable', 'admirable', 'admirably', 'admiration', 'admire', 'admirer', 'admiring', 'admiringly', 'adorable', 'adore', 'adored', 'adorer', 'adoring', 'adoringly', 'adroit', 'adroitly', 'adulate', 'adulation', 'adulatory', 'advanced', 'advantage', 'advantageous', 'advantageously', 'advantages', 'adventuresome', 'adventurous', 'advocate', 'advocated', 'advocates', 'affability', 'affable', 'affably', 'affectation', 'affection', 'affectionate', 'affinity', 'affirm', 'affirmation', 'affirmative', 'affluence', 'affluent', 'afford', 'affordable', 'affordably', 'afordable', 'agile', 'agilely', 'agility', 'agreeable', 'agreeableness', 'agreeably', 'all-around', 'alluring', 'alluringly', 'altruistic', 'altruistically', 'amaze', 'amazed', 'amazement', 'amazes', 'amazing', 'amazingly', 'ambitious', 'ambitiously', 'ameliorate', 'amenable', 'amenity', 'amiability', 'amiabily', 'amiable', 'amicability', 'amicable', 'amicably', 'amity', 'ample', 'amply', 'amuse', 'amusing', 'amusingly', 'angel', 'angelic', 'apotheosis', 'appeal', 'appealing', 'applaud', 'appreciable', 'appreciate', 'appreciated', 'appreciates', 'appreciative', 'appreciatively', 'appropriate', 'approval', 'approve', 'ardent', 'ardently', 'ardor', 'articulate', 'aspiration', 'aspirations', 'aspire', 'assurance', 'assurances', 'assure', 'assuredly', 'assuring', 'astonish', 'astonished', 'astonishing', 'astonishingly', 'astonishment', 'astound', 'astounded', 'astounding', 'astoundingly', 'astutely', 'attentive', 'attraction', 'attractive', 'attractively', 'attune', 'audible', 'audibly', 'auspicious', 'authentic', 'authoritative', 'autonomous', 'available', 'aver', 'avid', 'avidly', 'award', 'awarded', 'awards', 'awe', 'awed', 'awesome', 'awesomely', 'awesomeness', 'awestruck', 'awsome', 'backbone', 'balanced', 'bargain', 'beauteous', 'beautiful', 'beautifullly', 'beautifully', 'beautify', 'beauty', 'beckon', 'beckoned', 'beckoning', 'beckons', 'believable', 'believeable', 'beloved', 'benefactor', 'beneficent', 'beneficial', 'beneficially', 'beneficiary', 'benefit', 'benefits', 'benevolence', 'benevolent', 'benifits', 'best', 'best-known', 'best-performing', 'best-selling', 'better', 'better-known', 'better-than-expected', 'beutifully', 'blameless', 'bless', 'blessing', 'bliss', 'blissful', 'blissfully', 'blithe', 'blockbuster', 'bloom', 'blossom', 'bolster', 'bonny', 'bonus', 'bonuses', 'boom', 'booming', 'boost', 'boundless', 'bountiful', 'brainiest', 'brainy', 'brand-new', 'brave', 'bravery', 'bravo', 'breakthrough', 'breakthroughs', 'breathlessness', 'breathtaking', 'breathtakingly', 'breeze', 'bright', 'brighten', 'brighter', 'brightest', 'brilliance', 'brilliances', 'brilliant', 'brilliantly', 'brisk', 'brotherly', 'bullish', 'buoyant', 'cajole', 'calm', 'calming', 'calmness', 'capability', 'capable', 'capably', 'captivate', 'captivating', 'carefree', 'cashback', 'cashbacks', 'catchy', 'celebrate', 'celebrated', 'celebration', 'celebratory', 'champ', 'champion', 'charisma', 'charismatic', 'charitable', 'charm', 'charming', 'charmingly', 'chaste', 'cheaper', 'cheapest', 'cheer', 'cheerful', 'cheery', 'cherish', 'cherished', 'cherub', 'chic', 'chivalrous', 'chivalry', 'civility', 'civilize', 'clarity', 'classic', 'classy', 'clean', 'cleaner', 'cleanest', 'cleanliness', 'cleanly', 'clear', 'clear-cut', 'cleared', 'clearer', 'clearly', 'clears', 'clever', 'cleverly', 'cohere', 'coherence', 'coherent', 'cohesive', 'colorful', 'comely', 'comfort', 'comfortable', 'comfortably', 'comforting', 'comfy', 'commend', 'commendable', 'commendably', 'commitment', 'commodious', 'compact', 'compactly', 'compassion', 'compassionate', 'compatible', 'competitive', 'complement', 'complementary', 'complemented', 'complements', 'compliant', 'compliment', 'complimentary', 'comprehensive', 'conciliate', 'conciliatory', 'concise', 'confidence', 'confident', 'congenial', 'congratulate', 'congratulation', 'congratulations', 'congratulatory', 'conscientious', 'considerate', 'consistent', 'consistently', 'constructive', 'consummate', 'contentment', 'continuity', 'contrasty', 'contribution', 'convenience', 'convenient', 'conveniently', 'convience', 'convienient', 'convient', 'convincing', 'convincingly', 'cool', 'coolest', 'cooperative', 'cooperatively', 'cornerstone', 'correct', 'correctly', 'cost-effective', 'cost-saving', 'counter-attack', 'counter-attacks', 'courage', 'courageous', 'courageously', 'courageousness', 'courteous', 'courtly', 'covenant', 'cozy', 'creative', 'credence', 'credible', 'crisp', 'crisper', 'cure', 'cure-all', 'cushy', 'cute', 'cuteness', 'danke', 'danken', 'daring', 'daringly', 'darling', 'dashing', 'dauntless', 'dawn', 'dazzle', 'dazzled', 'dazzling', 'dead-cheap', 'dead-on', 'decency', 'decent', 'decisive', 'decisiveness', 'dedicated', 'defeat', 'defeated', 'defeating', 'defeats', 'defender', 'deference', 'deft', 'deginified', 'delectable', 'delicacy', 'delicate', 'delicious', 'delight', 'delighted', 'delightful', 'delightfully', 'delightfulness', 'dependable', 'dependably', 'deservedly', 'deserving', 'desirable', 'desiring', 'desirous', 'destiny', 'detachable', 'devout', 'dexterous', 'dexterously', 'dextrous', 'dignified', 'dignify', 'dignity', 'diligence', 'diligent', 'diligently', 'diplomatic', 'dirt-cheap', 'distinction', 'distinctive', 'distinguished', 'diversified', 'divine', 'divinely', 'dominate', 'dominated', 'dominates', 'dote', 'dotingly', 'doubtless', 'dreamland', 'dumbfounded', 'dumbfounding', 'dummy-proof', 'durable', 'dynamic', 'eager', 'eagerly', 'eagerness', 'earnest', 'earnestly', 'earnestness', 'ease', 'eased', 'eases', 'easier', 'easiest', 'easiness', 'easing', 'easy', 'easy-to-use', 'easygoing', 'ebullience', 'ebullient', 'ebulliently', 'ecenomical', 'economical', 'ecstasies', 'ecstasy', 'ecstatic', 'ecstatically', 'edify', 'educated', 'effective', 'effectively', 'effectiveness', 'effectual', 'efficacious', 'efficient', 'efficiently', 'effortless', 'effortlessly', 'effusion', 'effusive', 'effusively', 'effusiveness', 'elan', 'elate', 'elated', 'elatedly', 'elation', 'electrify', 'elegance', 'elegant', 'elegantly', 'elevate', 'elite', 'eloquence', 'eloquent', 'eloquently', 'embolden', 'eminence', 'eminent', 'empathize', 'empathy', 'empower', 'empowerment', 'enchant', 'enchanted', 'enchanting', 'enchantingly', 'encourage', 'encouragement', 'encouraging', 'encouragingly', 'endear', 'endearing', 'endorse', 'endorsed', 'endorsement', 'endorses', 'endorsing', 'energetic', 'energize', 'energy-efficient', 'energy-saving', 'engaging', 'engrossing', 'enhance', 'enhanced', 'enhancement', 'enhances', 'enjoy', 'enjoyable', 'enjoyably', 'enjoyed', 'enjoying', 'enjoyment', 'enjoys', 'enlighten', 'enlightenment', 'enliven', 'ennoble', 'enough', 'enrapt', 'enrapture', 'enraptured', 'enrich', 'enrichment', 'enterprising', 'entertain', 'entertaining', 'entertains', 'enthral', 'enthrall', 'enthralled', 'enthuse', 'enthusiasm', 'enthusiast', 'enthusiastic', 'enthusiastically', 'entice', 'enticed', 'enticing', 'enticingly', 'entranced', 'entrancing', 'entrust', 'enviable', 'enviably', 'envious', 'enviously', 'enviousness', 'envy', 'equitable', 'ergonomical', 'err-free', 'erudite', 'ethical', 'eulogize', 'euphoria', 'euphoric', 'euphorically', 'evaluative', 'evenly', 'eventful', 'everlasting', 'evocative', 'exalt', 'exaltation', 'exalted', 'exaltedly', 'exalting', 'exaltingly', 'examplar', 'examplary', 'excallent', 'exceed', 'exceeded', 'exceeding', 'exceedingly', 'exceeds', 'excel', 'exceled', 'excelent', 'excellant', 'excelled', 'excellence', 'excellency', 'excellent', 'excellently', 'excels', 'exceptional', 'exceptionally', 'excite', 'excited', 'excitedly', 'excitedness', 'excitement', 'excites', 'exciting', 'excitingly', 'exellent', 'exemplar', 'exemplary', 'exhilarate', 'exhilarating', 'exhilaratingly', 'exhilaration', 'exonerate', 'expansive', 'expeditiously', 'expertly', 'exquisite', 'exquisitely', 'extol', 'extoll', 'extraordinarily', 'extraordinary', 'exuberance', 'exuberant', 'exuberantly', 'exult', 'exultant', 'exultation', 'exultingly', 'eye-catch', 'eye-catching', 'eyecatch', 'eyecatching', 'fabulous', 'fabulously', 'facilitate', 'fair', 'fairly', 'fairness', 'faith', 'faithful', 'faithfully', 'faithfulness', 'fame', 'famed', 'famous', 'famously', 'fancier', 'fancinating', 'fancy', 'fanfare', 'fans', 'fantastic', 'fantastically', 'fascinate', 'fascinating', 'fascinatingly', 'fascination', 'fashionable', 'fashionably', 'fast', 'fast-growing', 'fast-paced', 'faster', 'fastest', 'fastest-growing', 'faultless', 'fav', 'fave', 'favor', 'favorable', 'favored', 'favorite', 'favorited', 'favour', 'fearless', 'fearlessly', 'feasible', 'feasibly', 'feat', 'feature-rich', 'fecilitous', 'feisty', 'felicitate', 'felicitous', 'felicity', 'fertile', 'fervent', 'fervently', 'fervid', 'fervidly', 'fervor', 'festive', 'fidelity', 'fiery', 'fine', 'fine-looking', 'finely', 'finer', 'finest', 'firmer', 'first-class', 'first-in-class', 'first-rate', 'flashy', 'flatter', 'flattering', 'flatteringly', 'flawless', 'flawlessly', 'flexibility', 'flexible', 'flourish', 'flourishing', 'fluent', 'flutter', 'fond', 'fondly', 'fondness', 'foolproof', 'foremost', 'foresight', 'formidable', 'fortitude', 'fortuitous', 'fortuitously', 'fortunate', 'fortunately', 'fortune', 'fragrant', 'free', 'freed', 'freedom', 'freedoms', 'fresh', 'fresher', 'freshest', 'friendliness', 'friendly', 'frolic', 'frugal', 'fruitful', 'ftw', 'fulfillment', 'fun', 'futurestic', 'futuristic', 'gaiety', 'gaily', 'gain', 'gained', 'gainful', 'gainfully', 'gaining', 'gains', 'gallant', 'gallantly', 'galore', 'geekier', 'geeky', 'gem', 'gems', 'generosity', 'generous', 'generously', 'genial', 'genius', 'gentle', 'gentlest', 'genuine', 'gifted', 'glad', 'gladden', 'gladly', 'gladness', 'glamorous', 'glee', 'gleeful', 'gleefully', 'glimmer', 'glimmering', 'glisten', 'glistening', 'glitter', 'glitz', 'glorify', 'glorious', 'gloriously', 'glory', 'glow', 'glowing', 'glowingly', 'god-given', 'god-send', 'godlike', 'godsend', 'gold', 'golden', 'good', 'goodly', 'goodness', 'goodwill', 'goood', 'gooood', 'gorgeous', 'gorgeously', 'grace', 'graceful', 'gracefully', 'gracious', 'graciously', 'graciousness', 'grand', 'grandeur', 'grateful', 'gratefully', 'gratification', 'gratified', 'gratifies', 'gratify', 'gratifying', 'gratifyingly', 'gratitude', 'great', 'greatest', 'greatness', 'grin', 'groundbreaking', 'guarantee', 'guidance', 'guiltless', 'gumption', 'gush', 'gusto', 'gutsy', 'hail', 'halcyon', 'hale', 'hallmark', 'hallmarks', 'hallowed', 'handier', 'handily', 'hands-down', 'handsome', 'handsomely', 'handy', 'happier', 'happily', 'happiness', 'happy', 'hard-working', 'hardier', 'hardy', 'harmless', 'harmonious', 'harmoniously', 'harmonize', 'harmony', 'headway', 'heal', 'healthful', 'healthy', 'hearten', 'heartening', 'heartfelt', 'heartily', 'heartwarming', 'heaven', 'heavenly', 'helped', 'helpful', 'helping', 'hero', 'heroic', 'heroically', 'heroine', 'heroize', 'heros', 'high-quality', 'high-spirited', 'hilarious', 'holy', 'homage', 'honest', 'honesty', 'honor', 'honorable', 'honored', 'honoring', 'hooray', 'hopeful', 'hospitable', 'hot', 'hotcake', 'hotcakes', 'hottest', 'hug', 'humane', 'humble', 'humility', 'humor', 'humorous', 'humorously', 'humour', 'humourous', 'ideal', 'idealize', 'ideally', 'idol', 'idolize', 'idolized', 'idyllic', 'illuminate', 'illuminati', 'illuminating', 'illumine', 'illustrious', 'ilu', 'imaculate', 'imaginative', 'immaculate', 'immaculately', 'immense', 'impartial', 'impartiality', 'impartially', 'impassioned', 'impeccable', 'impeccably', 'important', 'impress', 'impressed', 'impresses', 'impressive', 'impressively', 'impressiveness', 'improve', 'improved', 'improvement', 'improvements', 'improves', 'improving', 'incredible', 'incredibly', 'indebted', 'individualized', 'indulgence', 'indulgent', 'industrious', 'inestimable', 'inestimably', 'inexpensive', 'infallibility', 'infallible', 'infallibly', 'influential', 'ingenious', 'ingeniously', 'ingenuity', 'ingenuous', 'ingenuously', 'innocuous', 'innovation', 'innovative', 'inpressed', 'insightful', 'insightfully', 'inspiration', 'inspirational', 'inspire', 'inspiring', 'instantly', 'instructive', 'instrumental', 'integral', 'integrated', 'intelligence', 'intelligent', 'intelligible', 'interesting', 'interests', 'intimacy', 'intimate', 'intricate', 'intrigue', 'intriguing', 'intriguingly', 'intuitive', 'invaluable', 'invaluablely', 'inventive', 'invigorate', 'invigorating', 'invincibility', 'invincible', 'inviolable', 'inviolate', 'invulnerable', 'irreplaceable', 'irreproachable', 'irresistible', 'irresistibly', 'issue-free', 'jaw-droping', 'jaw-dropping', 'jollify', 'jolly', 'jovial', 'joy', 'joyful', 'joyfully', 'joyous', 'joyously', 'jubilant', 'jubilantly', 'jubilate', 'jubilation', 'jubiliant', 'judicious', 'justly', 'keen', 'keenly', 'keenness', 'kid-friendly', 'kindliness', 'kindly', 'kindness', 'knowledgeable', 'kudos', 'large-capacity', 'laud', 'laudable', 'laudably', 'lavish', 'lavishly', 'law-abiding', 'lawful', 'lawfully', 'lead', 'leading', 'leads', 'lean', 'led', 'legendary', 'leverage', 'levity', 'liberate', 'liberation', 'liberty', 'lifesaver', 'light-hearted', 'lighter', 'likable', 'like', 'liked', 'likes', 'liking', 'lionhearted', 'lively', 'logical', 'long-lasting', 'lovable', 'lovably', 'love', 'loved', 'loveliness', 'lovely', 'lover', 'loves', 'loving', 'low-cost', 'low-price', 'low-priced', 'low-risk', 'lower-priced', 'loyal', 'loyalty', 'lucid', 'lucidly', 'luck', 'luckier', 'luckiest', 'luckiness', 'lucky', 'lucrative', 'luminous', 'lush', 'luster', 'lustrous', 'luxuriant', 'luxuriate', 'luxurious', 'luxuriously', 'luxury', 'lyrical', 'magic', 'magical', 'magnanimous', 'magnanimously', 'magnificence', 'magnificent', 'magnificently', 'majestic', 'majesty', 'manageable', 'maneuverable', 'marvel', 'marveled', 'marvelled', 'marvellous', 'marvelous', 'marvelously', 'marvelousness', 'marvels', 'master', 'masterful', 'masterfully', 'masterpiece', 'masterpieces', 'masters', 'mastery', 'matchless', 'mature', 'maturely', 'maturity', 'meaningful', 'memorable', 'merciful', 'mercifully', 'mercy', 'merit', 'meritorious', 'merrily', 'merriment', 'merriness', 'merry', 'mesmerize', 'mesmerized', 'mesmerizes', 'mesmerizing', 'mesmerizingly', 'meticulous', 'meticulously', 'mightily', 'mighty', 'mind-blowing', 'miracle', 'miracles', 'miraculous', 'miraculously', 'miraculousness', 'modern', 'modest', 'modesty', 'momentous', 'monumental', 'monumentally', 'morality', 'motivated', 'multi-purpose', 'navigable', 'neat', 'neatest', 'neatly', 'nice', 'nicely', 'nicer', 'nicest', 'nifty', 'nimble', 'noble', 'nobly', 'noiseless', 'non-violence', 'non-violent', 'notably', 'noteworthy', 'nourish', 'nourishing', 'nourishment', 'novelty', 'nurturing', 'oasis', 'obsession', 'obsessions', 'obtainable', 'openly', 'openness', 'optimal', 'optimism', 'optimistic', 'opulent', 'orderly', 'originality', 'outdo', 'outdone', 'outperform', 'outperformed', 'outperforming', 'outperforms', 'outshine', 'outshone', 'outsmart', 'outstanding', 'outstandingly', 'outstrip', 'outwit', 'ovation', 'overjoyed', 'overtake', 'overtaken', 'overtakes', 'overtaking', 'overtook', 'overture', 'pain-free', 'painless', 'painlessly', 'palatial', 'pamper', 'pampered', 'pamperedly', 'pamperedness', 'pampers', 'panoramic', 'paradise', 'paramount', 'pardon', 'passion', 'passionate', 'passionately', 'patience', 'patient', 'patiently', 'patriot', 'patriotic', 'peace', 'peaceable', 'peaceful', 'peacefully', 'peacekeepers', 'peach', 'peerless', 'pep', 'pepped', 'pepping', 'peppy', 'peps', 'perfect', 'perfection', 'perfectly', 'permissible', 'perseverance', 'persevere', 'personages', 'personalized', 'phenomenal', 'phenomenally', 'picturesque', 'piety', 'pinnacle', 'playful', 'playfully', 'pleasant', 'pleasantly', 'pleased', 'pleases', 'pleasing', 'pleasingly', 'pleasurable', 'pleasurably', 'pleasure', 'plentiful', 'pluses', 'plush', 'plusses', 'poetic', 'poeticize', 'poignant', 'poise', 'poised', 'polished', 'polite', 'politeness', 'popular', 'portable', 'posh', 'positive', 'positively', 'positives', 'powerful', 'powerfully', 'praise', 'praiseworthy', 'praising', 'pre-eminent', 'precious', 'precise', 'precisely', 'preeminent', 'prefer', 'preferable', 'preferably', 'prefered', 'preferes', 'preferring', 'prefers', 'premier', 'prestige', 'prestigious', 'prettily', 'pretty', 'priceless', 'pride', 'principled', 'privilege', 'privileged', 'prize', 'proactive', 'problem-free', 'problem-solver', 'prodigious', 'prodigiously', 'prodigy', 'productive', 'productively', 'proficient', 'proficiently', 'profound', 'profoundly', 'profuse', 'profusion', 'progress', 'progressive', 'prolific', 'prominence', 'prominent', 'promise', 'promised', 'promises', 'promising', 'promoter', 'prompt', 'promptly', 'proper', 'properly', 'propitious', 'propitiously', 'pros', 'prosper', 'prosperity', 'prosperous', 'prospros', 'protect', 'protection', 'protective', 'proud', 'proven', 'proves', 'providence', 'proving', 'prowess', 'prudence', 'prudent', 'prudently', 'punctual', 'pure', 'purify', 'purposeful', 'quaint', 'qualified', 'qualify', 'quicker', 'quiet', 'quieter', 'radiance', 'radiant', 'rapid', 'rapport', 'rapt', 'rapture', 'raptureous', 'raptureously', 'rapturous', 'rapturously', 'rational', 'razor-sharp', 'reachable', 'readable', 'readily', 'ready', 'reaffirm', 'reaffirmation', 'realistic', 'realizable', 'reasonable', 'reasonably', 'reasoned', 'reassurance', 'reassure', 'receptive', 'reclaim', 'recomend', 'recommend', 'recommendation', 'recommendations', 'recommended', 'reconcile', 'reconciliation', 'record-setting', 'recover', 'recovery', 'rectification', 'rectify', 'rectifying', 'redeem', 'redeeming', 'redemption', 'refine', 'refined', 'refinement', 'reform', 'reformed', 'reforming', 'reforms', 'refresh', 'refreshed', 'refreshing', 'refund', 'refunded', 'regal', 'regally', 'regard', 'rejoice', 'rejoicing', 'rejoicingly', 'rejuvenate', 'rejuvenated', 'rejuvenating', 'relaxed', 'relent', 'reliable', 'reliably', 'relief', 'relish', 'remarkable', 'remarkably', 'remedy', 'remission', 'remunerate', 'renaissance', 'renewed', 'renown', 'renowned', 'replaceable', 'reputable', 'reputation', 'resilient', 'resolute', 'resound', 'resounding', 'resourceful', 'resourcefulness', 'respect', 'respectable', 'respectful', 'respectfully', 'respite', 'resplendent', 'responsibly', 'responsive', 'restful', 'restored', 'restructure', 'restructured', 'restructuring', 'retractable', 'revel', 'revelation', 'revere', 'reverence', 'reverent', 'reverently', 'revitalize', 'revival', 'revive', 'revives', 'revolutionary', 'revolutionize', 'revolutionized', 'revolutionizes', 'reward', 'rewarding', 'rewardingly', 'rich', 'richer', 'richly', 'richness', 'right', 'righten', 'righteous', 'righteously', 'righteousness', 'rightful', 'rightfully', 'rightly', 'rightness', 'risk-free', 'robust', 'rock-star', 'rock-stars', 'rockstar', 'rockstars', 'romantic', 'romantically', 'romanticize', 'roomier', 'roomy', 'rosy', 'safe', 'safely', 'sagacity', 'sagely', 'saint', 'saintliness', 'saintly', 'salutary', 'salute', 'sane', 'satisfactorily', 'satisfactory', 'satisfied', 'satisfies', 'satisfy', 'satisfying', 'satisified', 'saver', 'savings', 'savior', 'savvy', 'scenic', 'seamless', 'seasoned', 'secure', 'securely', 'selective', 'self-determination', 'self-respect', 'self-satisfaction', 'self-sufficiency', 'self-sufficient', 'sensation', 'sensational', 'sensationally', 'sensations', 'sensible', 'sensibly', 'sensitive', 'serene', 'serenity', 'sexy', 'sharp', 'sharper', 'sharpest', 'shimmering', 'shimmeringly', 'shine', 'shiny', 'significant', 'silent', 'simpler', 'simplest', 'simplified', 'simplifies', 'simplify', 'simplifying', 'sincere', 'sincerely', 'sincerity', 'skill', 'skilled', 'skillful', 'skillfully', 'slammin', 'sleek', 'slick', 'smart', 'smarter', 'smartest', 'smartly', 'smile', 'smiles', 'smiling', 'smilingly', 'smitten', 'smooth', 'smoother', 'smoothes', 'smoothest', 'smoothly', 'snappy', 'snazzy', 'sociable', 'soft', 'softer', 'solace', 'solicitous', 'solicitously', 'solid', 'solidarity', 'soothe', 'soothingly', 'sophisticated', 'soulful', 'soundly', 'soundness', 'spacious', 'sparkle', 'sparkling', 'spectacular', 'spectacularly', 'speedily', 'speedy', 'spellbind', 'spellbinding', 'spellbindingly', 'spellbound', 'spirited', 'spiritual', 'splendid', 'splendidly', 'splendor', 'spontaneous', 'sporty', 'spotless', 'sprightly', 'stability', 'stabilize', 'stable', 'stainless', 'standout', 'state-of-the-art', 'stately', 'statuesque', 'staunch', 'staunchly', 'staunchness', 'steadfast', 'steadfastly', 'steadfastness', 'steadiest', 'steadiness', 'steady', 'stellar', 'stellarly', 'stimulate', 'stimulates', 'stimulating', 'stimulative', 'stirringly', 'straighten', 'straightforward', 'streamlined', 'striking', 'strikingly', 'striving', 'strong', 'stronger', 'strongest', 'stunned', 'stunning', 'stunningly', 'stupendous', 'stupendously', 'sturdier', 'sturdy', 'stylish', 'stylishly', 'stylized', 'suave', 'suavely', 'sublime', 'subsidize', 'subsidized', 'subsidizes', 'subsidizing', 'substantive', 'succeed', 'succeeded', 'succeeding', 'succeeds', 'succes', 'success', 'successes', 'successful', 'successfully', 'suffice', 'sufficed', 'suffices', 'sufficient', 'sufficiently', 'suitable', 'sumptuous', 'sumptuously', 'sumptuousness', 'super', 'superb', 'superbly', 'superior', 'superiority', 'supple', 'support', 'supported', 'supporter', 'supporting', 'supportive', 'supports', 'supremacy', 'supreme', 'supremely', 'supurb', 'supurbly', 'surmount', 'surpass', 'surreal', 'survival', 'survivor', 'sustainability', 'sustainable', 'swank', 'swankier', 'swankiest', 'swanky', 'sweeping', 'sweet', 'sweeten', 'sweetheart', 'sweetly', 'sweetness', 'swift', 'swiftness', 'talent', 'talented', 'talents', 'tantalize', 'tantalizing', 'tantalizingly', 'tempt', 'tempting', 'temptingly', 'tenacious', 'tenaciously', 'tenacity', 'tender', 'tenderly', 'terrific', 'terrifically', 'thank', 'thankful', 'thinner', 'thoughtful', 'thoughtfully', 'thoughtfulness', 'thrift', 'thrifty', 'thrill', 'thrilled', 'thrilling', 'thrillingly', 'thrills', 'thrive', 'thriving', 'thumb-up', 'thumbs-up', 'tickle', 'tidy', 'time-honored', 'timely', 'tingle', 'titillate', 'titillating', 'titillatingly', 'togetherness', 'tolerable', 'toll-free', 'top', 'top-notch', 'top-quality', 'topnotch', 'tops', 'tough', 'tougher', 'toughest', 'traction', 'tranquil', 'tranquility', 'transparent', 'treasure', 'tremendously', 'trendy', 'triumph', 'triumphal', 'triumphant', 'triumphantly', 'trivially', 'trophy', 'trouble-free', 'trump', 'trumpet', 'trust', 'trusted', 'trusting', 'trustingly', 'trustworthiness', 'trustworthy', 'trusty', 'truthful', 'truthfully', 'truthfulness', 'twinkly', 'ultra-crisp', 'unabashed', 'unabashedly', 'unaffected', 'unassailable', 'unbeatable', 'unbiased', 'unbound', 'uncomplicated', 'unconditional', 'undamaged', 'undaunted', 'understandable', 'undisputable', 'undisputably', 'undisputed', 'unencumbered', 'unequivocal', 'unequivocally', 'unfazed', 'unfettered', 'unforgettable', 'unity', 'unlimited', 'unmatched', 'unparalleled', 'unquestionable', 'unquestionably', 'unreal', 'unrestricted', 'unrivaled', 'unselfish', 'unwavering', 'upbeat', 'upgradable', 'upgradeable', 'upgraded', 'upheld', 'uphold', 'uplift', 'uplifting', 'upliftingly', 'upliftment', 'upscale', 'usable', 'useable', 'useful', 'user-friendly', 'user-replaceable', 'valiant', 'valiantly', 'valor', 'valuable', 'variety', 'venerate', 'verifiable', 'veritable', 'versatile', 'versatility', 'vibrant', 'vibrantly', 'victorious', 'victory', 'viewable', 'vigilance', 'vigilant', 'virtue', 'virtuous', 'virtuously', 'visionary', 'vivacious', 'vivid', 'vouch', 'vouchsafe', 'warm', 'warmer', 'warmhearted', 'warmly', 'warmth', 'wealthy', 'welcome', 'well', 'well-backlit', 'well-balanced', 'well-behaved', 'well-being', 'well-bred', 'well-connected', 'well-educated', 'well-established', 'well-informed', 'well-intentioned', 'well-known', 'well-made', 'well-managed', 'well-mannered', 'well-positioned', 'well-received', 'well-regarded', 'well-rounded', 'well-run', 'well-wishers', 'wellbeing', 'whoa', 'wholeheartedly', 'wholesome', 'whooa', 'whoooa', 'wieldy', 'willing', 'willingly', 'willingness', 'win', 'windfall', 'winnable', 'winner', 'winners', 'winning', 'wins', 'wisdom', 'wise', 'wisely', 'witty', 'won', 'wonder', 'wonderful', 'wonderfully', 'wonderous', 'wonderously', 'wonders', 'wondrous', 'woo', 'work', 'workable', 'worked', 'works', 'world-famous', 'worth', 'worth-while', 'worthiness', 'worthwhile', 'worthy', 'wow', 'wowed', 'wowing', 'wows', 'yay', 'youthful', 'zeal', 'zenith', 'zest', 'zippy', nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
            "['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably', 'abominate', 'abomination', 'abort', 'aborted', 'aborts', 'abrade', 'abrasive', 'abrupt', 'abruptly', 'abscond', 'absence', 'absent-minded', 'absentee', 'absurd', 'absurdity', 'absurdly', 'absurdness', 'abuse', 'abused', 'abuses', 'abusive', 'abysmal', 'abysmally', 'abyss', 'accidental', 'accost', 'accursed', 'accusation', 'accusations', 'accuse', 'accuses', 'accusing', 'accusingly', 'acerbate', 'acerbic', 'acerbically', 'ache', 'ached', 'aches', 'achey', 'aching', 'acrid', 'acridly', 'acridness', 'acrimonious', 'acrimoniously', 'acrimony', 'adamant', 'adamantly', 'addict', 'addicted', 'addicting', 'addicts', 'admonish', 'admonisher', 'admonishingly', 'admonishment', 'admonition', 'adulterate', 'adulterated', 'adulteration', 'adulterier', 'adversarial', 'adversary', 'adverse', 'adversity', 'afflict', 'affliction', 'afflictive', 'affront', 'afraid', 'aggravate', 'aggravating', 'aggravation', 'aggression', 'aggressive', 'aggressiveness', 'aggressor', 'aggrieve', 'aggrieved', 'aggrivation', 'aghast', 'agonies', 'agonize', 'agonizing', 'agonizingly', 'agony', 'aground', 'ail', 'ailing', 'ailment', 'aimless', 'alarm', 'alarmed', 'alarming', 'alarmingly', 'alienate', 'alienated', 'alienation', 'allegation', 'allegations', 'allege', 'allergic', 'allergies', 'allergy', 'aloof', 'altercation', 'ambiguity', 'ambiguous', 'ambivalence', 'ambivalent', 'ambush', 'amiss', 'amputate', 'anarchism', 'anarchist', 'anarchistic', 'anarchy', 'anemic', 'anger', 'angrily', 'angriness', 'angry', 'anguish', 'animosity', 'annihilate', 'annihilation', 'annoy', 'annoyance', 'annoyances', 'annoyed', 'annoying', 'annoyingly', 'annoys', 'anomalous', 'anomaly', 'antagonism', 'antagonist', 'antagonistic', 'antagonize', 'anti-', 'anti-american', 'anti-israeli', 'anti-occupation', 'anti-proliferation', 'anti-semites', 'anti-social', 'anti-us', 'anti-white', 'antipathy', 'antiquated', 'antithetical', 'anxieties', 'anxiety', 'anxious', 'anxiously', 'anxiousness', 'apathetic', 'apathetically', 'apathy', 'apocalypse', 'apocalyptic', 'apologist', 'apologists', 'appal', 'appall', 'appalled', 'appalling', 'appallingly', 'apprehension', 'apprehensions', 'apprehensive', 'apprehensively', 'arbitrary', 'arcane', 'archaic', 'arduous', 'arduously', 'argumentative', 'arrogance', 'arrogant', 'arrogantly', 'ashamed', 'asinine', 'asininely', 'asinininity', 'askance', 'asperse', 'aspersion', 'aspersions', 'assail', 'assassin', 'assassinate', 'assault', 'assult', 'astray', 'asunder', 'atrocious', 'atrocities', 'atrocity', 'atrophy', 'attack', 'attacks', 'audacious', 'audaciously', 'audaciousness', 'audacity', 'audiciously', 'austere', 'authoritarian', 'autocrat', 'autocratic', 'avalanche', 'avarice', 'avaricious', 'avariciously', 'avenge', 'averse', 'aversion', 'aweful', 'awful', 'awfully', 'awfulness', 'awkward', 'awkwardness', 'ax', 'babble', 'back-logged', 'back-wood', 'back-woods', 'backache', 'backaches', 'backaching', 'backbite', 'backbiting', 'backward', 'backwardness', 'backwood', 'backwoods', 'bad', 'badly', 'baffle', 'baffled', 'bafflement', 'baffling', 'bait', 'balk', 'banal', 'banalize', 'bane', 'banish', 'banishment', 'bankrupt', 'barbarian', 'barbaric', 'barbarically', 'barbarity', 'barbarous', 'barbarously', 'barren', 'baseless', 'bash', 'bashed', 'bashful', 'bashing', 'bastard', 'bastards', 'battered', 'battering', 'batty', 'bearish', 'beastly', 'bedlam', 'bedlamite', 'befoul', 'beg', 'beggar', 'beggarly', 'begging', 'beguile', 'belabor', 'belated', 'beleaguer', 'belie', 'belittle', 'belittled', 'belittling', 'bellicose', 'belligerence', 'belligerent', 'belligerently', 'bemoan', 'bemoaning', 'bemused', 'bent', 'berate', 'bereave', 'bereavement', 'bereft', 'berserk', 'beseech', 'beset', 'besiege', 'besmirch', 'bestial', 'betray', 'betrayal', 'betrayals', 'betrayer', 'betraying', 'betrays', 'bewail', 'beware', 'bewilder', 'bewildered', 'bewildering', 'bewilderingly', 'bewilderment', 'bewitch', 'bias', 'biased', 'biases', 'bicker', 'bickering', 'bid-rigging', 'bigotries', 'bigotry', 'bitch', 'bitchy', 'biting', 'bitingly', 'bitter', 'bitterly', 'bitterness', 'bizarre', 'blab', 'blabber', 'blackmail', 'blah', 'blame', 'blameworthy', 'bland', 'blandish', 'blaspheme', 'blasphemous', 'blasphemy', 'blasted', 'blatant', 'blatantly', 'blather', 'bleak', 'bleakly', 'bleakness', 'bleed', 'bleeding', 'bleeds', 'blemish', 'blind', 'blinding', 'blindingly', 'blindside', 'blister', 'blistering', 'bloated', 'blockage', 'blockhead', 'bloodshed', 'bloodthirsty', 'bloody', 'blotchy', 'blow', 'blunder', 'blundering', 'blunders', 'blunt', 'blur', 'bluring', 'blurred', 'blurring', 'blurry', 'blurs', 'blurt', 'boastful', 'boggle', 'bogus', 'boil', 'boiling', 'boisterous', 'bomb', 'bombard', 'bombardment', 'bombastic', 'bondage', 'bonkers', 'bore', 'bored', 'boredom', 'bores', 'boring', 'botch', 'bother', 'bothered', 'bothering', 'bothers', 'bothersome', 'bowdlerize', 'boycott', 'braggart', 'bragger', 'brainless', 'brainwash', 'brash', 'brashly', 'brashness', 'brat', 'bravado', 'brazen', 'brazenly', 'brazenness', 'breach', 'break', 'break-up', 'break-ups', 'breakdown', 'breaking', 'breaks', 'breakup', 'breakups', 'bribery', 'brimstone', 'bristle', 'brittle', 'broke', 'broken', 'broken-hearted', 'brood', 'browbeat', 'bruise', 'bruised', 'bruises', 'bruising', 'brusque', 'brutal', 'brutalising', 'brutalities', 'brutality', 'brutalize', 'brutalizing', 'brutally', 'brute', 'brutish', 'bs', 'buckle', 'bug', 'bugging', 'buggy', 'bugs', 'bulkier', 'bulkiness', 'bulky', 'bulkyness', 'bull****', 'bull----', 'bullies', 'bullshit', 'bullshyt', 'bully', 'bullying', 'bullyingly', 'bum', 'bump', 'bumped', 'bumping', 'bumpping', 'bumps', 'bumpy', 'bungle', 'bungler', 'bungling', 'bunk', 'burden', 'burdensome', 'burdensomely', 'burn', 'burned', 'burning', 'burns', 'bust', 'busts', 'busybody', 'butcher', 'butchery', 'buzzing', 'byzantine', 'cackle', 'calamities', 'calamitous', 'calamitously', 'calamity', 'callous', 'calumniate', 'calumniation', 'calumnies', 'calumnious', 'calumniously', 'calumny', 'cancer', 'cancerous', 'cannibal', 'cannibalize', 'capitulate', 'capricious', 'capriciously', 'capriciousness', 'capsize', 'careless', 'carelessness', 'caricature', 'carnage', 'carp', 'cartoonish', 'cash-strapped', 'castigate', 'castrated', 'casualty', 'cataclysm', 'cataclysmal', 'cataclysmic', 'cataclysmically', 'catastrophe', 'catastrophes', 'catastrophic', 'catastrophically', 'catastrophies', 'caustic', 'caustically', 'cautionary', 'cave', 'censure', 'chafe', 'chaff', 'chagrin', 'challenging', 'chaos', 'chaotic', 'chasten', 'chastise', 'chastisement', 'chatter', 'chatterbox', 'cheap', 'cheapen', 'cheaply', 'cheat', 'cheated', 'cheater', 'cheating', 'cheats', 'checkered', 'cheerless', 'cheesy', 'chide', 'childish', 'chill', 'chilly', 'chintzy', 'choke', 'choleric', 'choppy', 'chore', 'chronic', 'chunky', 'clamor', 'clamorous', 'clash', 'cliche', 'cliched', 'clique', 'clog', 'clogged', 'clogs', 'cloud', 'clouding', 'cloudy', 'clueless', 'clumsy', 'clunky', 'coarse', 'cocky', 'coerce', 'coercion', 'coercive', 'cold', 'coldly', 'collapse', 'collude', 'collusion', 'combative', 'combust', 'comical', 'commiserate', 'commonplace', 'commotion', 'commotions', 'complacent', 'complain', 'complained', 'complaining', 'complains', 'complaint', 'complaints', 'complex', 'complicated', 'complication', 'complicit', 'compulsion', 'compulsive', 'concede', 'conceded', 'conceit', 'conceited', 'concen', 'concens', 'concern', 'concerned', 'concerns', 'concession', 'concessions', 'condemn', 'condemnable', 'condemnation', 'condemned', 'condemns', 'condescend', 'condescending', 'condescendingly', 'condescension', 'confess', 'confession', 'confessions', 'confined', 'conflict', 'conflicted', 'conflicting', 'conflicts', 'confound', 'confounded', 'confounding', 'confront', 'confrontation', 'confrontational', 'confuse', 'confused', 'confuses', 'confusing', 'confusion', 'confusions', 'congested', 'congestion', 'cons', 'conscons', 'conservative', 'conspicuous', 'conspicuously', 'conspiracies', 'conspiracy', 'conspirator', 'conspiratorial', 'conspire', 'consternation', 'contagious', 'contaminate', 'contaminated', 'contaminates', 'contaminating', 'contamination', 'contempt', 'contemptible', 'contemptuous', 'contemptuously', 'contend', 'contention', 'contentious', 'contort', 'contortions', 'contradict', 'contradiction', 'contradictory', 'contrariness', 'contravene', 'contrive', 'contrived', 'controversial', 'controversy', 'convoluted', 'corrode', 'corrosion', 'corrosions', 'corrosive', 'corrupt', 'corrupted', 'corrupting', 'corruption', 'corrupts', 'corruptted', 'costlier', 'costly', 'counter-productive', 'counterproductive', 'coupists', 'covetous', 'coward', 'cowardly', 'crabby', 'crack', 'cracked', 'cracks', 'craftily', 'craftly', 'crafty', 'cramp', 'cramped', 'cramping', 'cranky', 'crap', 'crappy', 'craps', 'crash', 'crashed', 'crashes', 'crashing', 'crass', 'craven', 'cravenly', 'craze', 'crazily', 'craziness', 'crazy', 'creak', 'creaking', 'creaks', 'credulous', 'creep', 'creeping', 'creeps', 'creepy', 'crept', 'crime', 'criminal', 'cringe', 'cringed', 'cringes', 'cripple', 'crippled', 'cripples', 'crippling', 'crisis', 'critic', 'critical', 'criticism', 'criticisms', 'criticize', 'criticized', 'criticizing', 'critics', 'cronyism', 'crook', 'crooked', 'crooks', 'crowded', 'crowdedness', 'crude', 'cruel', 'crueler', 'cruelest', 'cruelly', 'cruelness', 'cruelties', 'cruelty', 'crumble', 'crumbling', 'crummy', 'crumple', 'crumpled', 'crumples', 'crush', 'crushed', 'crushing', 'cry', 'culpable', 'culprit', 'cumbersome', 'cunt', 'cunts', 'cuplrit', 'curse', 'cursed', 'curses', 'curt', 'cuss', 'cussed', 'cutthroat', 'cynical', 'cynicism', 'd*mn', 'damage', 'damaged', 'damages', 'damaging', 'damn', 'damnable', 'damnably', 'damnation', 'damned', 'damning', 'damper', 'danger', 'dangerous', 'dangerousness', 'dark', 'darken', 'darkened', 'darker', 'darkness', 'dastard', 'dastardly', 'daunt', 'daunting', 'dauntingly', 'dawdle', 'daze', 'dazed', 'dead', 'deadbeat', 'deadlock', 'deadly', 'deadweight', 'deaf', 'dearth', 'death', 'debacle', 'debase', 'debasement', 'debaser', 'debatable', 'debauch', 'debaucher', 'debauchery', 'debilitate', 'debilitating', 'debility', 'debt', 'debts', 'decadence', 'decadent', 'decay', 'decayed', 'deceit', 'deceitful', 'deceitfully', 'deceitfulness', 'deceive', 'deceiver', 'deceivers', 'deceiving', 'deception', 'deceptive', 'deceptively', 'declaim', 'decline', 'declines', 'declining', 'decrement', 'decrepit', 'decrepitude', 'decry', 'defamation', 'defamations', 'defamatory', 'defame', 'defect', 'defective', 'defects', 'defensive', 'defiance', 'defiant', 'defiantly', 'deficiencies', 'deficiency', 'deficient', 'defile', 'defiler', 'deform', 'deformed', 'defrauding', 'defunct', 'defy', 'degenerate', 'degenerately', 'degeneration', 'degradation', 'degrade', 'degrading', 'degradingly', 'dehumanization', 'dehumanize', 'deign', 'deject', 'dejected', 'dejectedly', 'dejection', 'delay', 'delayed', 'delaying', 'delays', 'delinquency', 'delinquent', 'delirious', 'delirium', 'delude', 'deluded', 'deluge', 'delusion', 'delusional', 'delusions', 'demean', 'demeaning', 'demise', 'demolish', 'demolisher', 'demon', 'demonic', 'demonize', 'demonized', 'demonizes', 'demonizing', 'demoralize', 'demoralizing', 'demoralizingly', 'denial', 'denied', 'denies', 'denigrate', 'denounce', 'dense', 'dent', 'dented', 'dents', 'denunciate', 'denunciation', 'denunciations', 'deny', 'denying', 'deplete', 'deplorable', 'deplorably', 'deplore', 'deploring', 'deploringly', 'deprave', 'depraved', 'depravedly', 'deprecate', 'depress', 'depressed', 'depressing', 'depressingly', 'depression', 'depressions', 'deprive', 'deprived', 'deride', 'derision', 'derisive', 'derisively', 'derisiveness', 'derogatory', 'desecrate', 'desert', 'desertion', 'desiccate', 'desiccated', 'desititute', 'desolate', 'desolately', 'desolation', 'despair', 'despairing', 'despairingly', 'desperate', 'desperately', 'desperation', 'despicable', 'despicably', 'despise', 'despised', 'despoil', 'despoiler', 'despondence', 'despondency', 'despondent', 'despondently', 'despot', 'despotic', 'despotism', 'destabilisation', 'destains', 'destitute', 'destitution', 'destroy', 'destroyer', 'destruction', 'destructive', 'desultory', 'deter', 'deteriorate', 'deteriorating', 'deterioration', 'deterrent', 'detest', 'detestable', 'detestably', 'detested', 'detesting', 'detests', 'detract', 'detracted', 'detracting', 'detraction', 'detracts', 'detriment', 'detrimental', 'devastate', 'devastated', 'devastates', 'devastating', 'devastatingly', 'devastation', 'deviate', 'deviation', 'devil', 'devilish', 'devilishly', 'devilment', 'devilry', 'devious', 'deviously', 'deviousness', 'devoid', 'diabolic', 'diabolical', 'diabolically', 'diametrically', 'diappointed', 'diatribe', 'diatribes', 'dick', 'dictator', 'dictatorial', 'die', 'die-hard', 'died', 'dies', 'difficult', 'difficulties', 'difficulty', 'diffidence', 'dilapidated', 'dilemma', 'dilly-dally', 'dim', 'dimmer', 'din', 'ding', 'dings', 'dinky', 'dire', 'direly', 'direness', 'dirt', 'dirtbag', 'dirtbags', 'dirts', 'dirty', 'disable', 'disabled', 'disaccord', 'disadvantage', 'disadvantaged', 'disadvantageous', 'disadvantages', 'disaffect', 'disaffected', 'disaffirm', 'disagree', 'disagreeable', 'disagreeably', 'disagreed', 'disagreeing', 'disagreement', 'disagrees', 'disallow', 'disapointed', 'disapointing', 'disapointment', 'disappoint', 'disappointed', 'disappointing', 'disappointingly', 'disappointment', 'disappointments', 'disappoints', 'disapprobation', 'disapproval', 'disapprove', 'disapproving', 'disarm', 'disarray', 'disaster', 'disasterous', 'disastrous', 'disastrously', 'disavow', 'disavowal', 'disbelief', 'disbelieve', 'disbeliever', 'disclaim', 'discombobulate', 'discomfit', 'discomfititure', 'discomfort', 'discompose', 'disconcert', 'disconcerted', 'disconcerting', 'disconcertingly', 'disconsolate', 'disconsolately', 'disconsolation', 'discontent', 'discontented', 'discontentedly', 'discontinued', 'discontinuity', 'discontinuous', 'discord', 'discordance', 'discordant', 'discountenance', 'discourage', 'discouragement', 'discouraging', 'discouragingly', 'discourteous', 'discourteously', 'discoutinous', 'discredit', 'discrepant', 'discriminate', 'discrimination', 'discriminatory', 'disdain', 'disdained', 'disdainful', 'disdainfully', 'disfavor', 'disgrace', 'disgraced', 'disgraceful', 'disgracefully', 'disgruntle', 'disgruntled', 'disgust', 'disgusted', 'disgustedly', 'disgustful', 'disgustfully', 'disgusting', 'disgustingly', 'dishearten', 'disheartening', 'dishearteningly', 'dishonest', 'dishonestly', 'dishonesty', 'dishonor', 'dishonorable', 'dishonorablely', 'disillusion', 'disillusioned', 'disillusionment', 'disillusions', 'disinclination', 'disinclined', 'disingenuous', 'disingenuously', 'disintegrate', 'disintegrated', 'disintegrates', 'disintegration', 'disinterest', 'disinterested', 'dislike', 'disliked', 'dislikes', 'disliking', 'dislocated', 'disloyal', 'disloyalty', 'dismal', 'dismally', 'dismalness', 'dismay', 'dismayed', 'dismaying', 'dismayingly', 'dismissive', 'dismissively', 'disobedience', 'disobedient', 'disobey', 'disoobedient', 'disorder', 'disordered', 'disorderly', 'disorganized', 'disorient', 'disoriented', 'disown', 'disparage', 'disparaging', 'disparagingly', 'dispensable', 'dispirit', 'dispirited', 'dispiritedly', 'dispiriting', 'displace', 'displaced', 'displease', 'displeased', 'displeasing', 'displeasure', 'disproportionate', 'disprove', 'disputable', 'dispute', 'disputed', 'disquiet', 'disquieting', 'disquietingly', 'disquietude', 'disregard', 'disregardful', 'disreputable', 'disrepute', 'disrespect', 'disrespectable', 'disrespectablity', 'disrespectful', 'disrespectfully', 'disrespectfulness', 'disrespecting', 'disrupt', 'disruption', 'disruptive', 'diss', 'dissapointed', 'dissappointed', 'dissappointing', 'dissatisfaction', 'dissatisfactory', 'dissatisfied', 'dissatisfies', 'dissatisfy', 'dissatisfying', 'dissed', 'dissemble', 'dissembler', 'dissension', 'dissent', 'dissenter', 'dissention', 'disservice', 'disses', 'dissidence', 'dissident', 'dissidents', 'dissing', 'dissocial', 'dissolute', 'dissolution', 'dissonance', 'dissonant', 'dissonantly', 'dissuade', 'dissuasive', 'distains', 'distaste', 'distasteful', 'distastefully', 'distort', 'distorted', 'distortion', 'distorts', 'distract', 'distracting', 'distraction', 'distraught', 'distraughtly', 'distraughtness', 'distress', 'distressed', 'distressing', 'distressingly', 'distrust', 'distrustful', 'distrusting', 'disturb', 'disturbance', 'disturbed', 'disturbing', 'disturbingly', 'disunity', 'disvalue', 'divergent', 'divisive', 'divisively', 'divisiveness', 'dizzing', 'dizzingly', 'dizzy', 'doddering', 'dodgey', 'dogged', 'doggedly', 'dogmatic', 'doldrums', 'domineer', 'domineering', 'donside', 'doom', 'doomed', 'doomsday', 'dope', 'doubt', 'doubtful', 'doubtfully', 'doubts', 'douchbag', 'douchebag', 'douchebags', 'downbeat', 'downcast', 'downer', 'downfall', 'downfallen', 'downgrade', 'downhearted', 'downheartedly', 'downhill', 'downside', 'downsides', 'downturn', 'downturns', 'drab', 'draconian', 'draconic', 'drag', 'dragged', 'dragging', 'dragoon', 'drags', 'drain', 'drained', 'draining', 'drains', 'drastic', 'drastically', 'drawback', 'drawbacks', 'dread', 'dreadful', 'dreadfully', 'dreadfulness', 'dreary', 'dripped', 'dripping', 'drippy', 'drips', 'drones', 'droop', 'droops', 'drop-out', 'drop-outs', 'dropout', 'dropouts', 'drought', 'drowning', 'drunk', 'drunkard', 'drunken', 'dubious', 'dubiously', 'dubitable', 'dud', 'dull', 'dullard', 'dumb', 'dumbfound', 'dump', 'dumped', 'dumping', 'dumps', 'dunce', 'dungeon', 'dungeons', 'dupe', 'dust', 'dusty', 'dwindling', 'dying', 'earsplitting', 'eccentric', 'eccentricity', 'effigy', 'effrontery', 'egocentric', 'egomania', 'egotism', 'egotistical', 'egotistically', 'egregious', 'egregiously', 'election-rigger', 'elimination', 'emaciated', 'emasculate', 'embarrass', 'embarrassing', 'embarrassingly', 'embarrassment', 'embattled', 'embroil', 'embroiled', 'embroilment', 'emergency', 'emphatic', 'emphatically', 'emptiness', 'encroach', 'encroachment', 'endanger', 'enemies', 'enemy', 'enervate', 'enfeeble', 'enflame', 'engulf', 'enjoin', 'enmity', 'enrage', 'enraged', 'enraging', 'enslave', 'entangle', 'entanglement', 'entrap', 'entrapment', 'envious', 'enviously', 'enviousness', 'epidemic', 'equivocal', 'erase', 'erode', 'erodes', 'erosion', 'err', 'errant', 'erratic', 'erratically', 'erroneous', 'erroneously', 'error', 'errors', 'eruptions', 'escapade', 'eschew', 'estranged', 'evade', 'evasion', 'evasive', 'evil', 'evildoer', 'evils', 'eviscerate', 'exacerbate', 'exagerate', 'exagerated', 'exagerates', 'exaggerate', 'exaggeration', 'exasperate', 'exasperated', 'exasperating', 'exasperatingly', 'exasperation', 'excessive', 'excessively', 'exclusion', 'excoriate', 'excruciating', 'excruciatingly', 'excuse', 'excuses', 'execrate', 'exhaust', 'exhausted', 'exhaustion', 'exhausts', 'exhorbitant', 'exhort', 'exile', 'exorbitant', 'exorbitantance', 'exorbitantly', 'expel', 'expensive', 'expire', 'expired', 'explode', 'exploit', 'exploitation', 'explosive', 'expropriate', 'expropriation', 'expulse', 'expunge', 'exterminate', 'extermination', 'extinguish', 'extort', 'extortion', 'extraneous', 'extravagance', 'extravagant', 'extravagantly', 'extremism', 'extremist', 'extremists', 'eyesore', 'f**k', 'fabricate', 'fabrication', 'facetious', 'facetiously', 'fail', 'failed', 'failing', 'fails', 'failure', 'failures', 'faint', 'fainthearted', 'faithless', 'fake', 'fall', 'fallacies', 'fallacious', 'fallaciously', 'fallaciousness', 'fallacy', 'fallen', 'falling', 'fallout', 'falls', 'false', 'falsehood', 'falsely', 'falsify', 'falter', 'faltered', 'famine', 'famished', 'fanatic', 'fanatical', 'fanatically', 'fanaticism', 'fanatics', 'fanciful', 'far-fetched', 'farce', 'farcical', 'farcical-yet-provocative', 'farcically', 'farfetched', 'fascism', 'fascist', 'fastidious', 'fastidiously', 'fastuous', 'fat', 'fat-cat', 'fat-cats', 'fatal', 'fatalistic', 'fatalistically', 'fatally', 'fatcat', 'fatcats', 'fateful', 'fatefully', 'fathomless', 'fatigue', 'fatigued', 'fatique', 'fatty', 'fatuity', 'fatuous', 'fatuously', 'fault', 'faults', 'faulty', 'fawningly', 'faze', 'fear', 'fearful', 'fearfully', 'fears', 'fearsome', 'feckless', 'feeble', 'feeblely', 'feebleminded', 'feign', 'feint', 'fell', 'felon', 'felonious', 'ferociously', 'ferocity', 'fetid', 'fever', 'feverish', 'fevers', 'fiasco', 'fib', 'fibber', 'fickle', 'fiction', 'fictional', 'fictitious', 'fidget', 'fidgety', 'fiend', 'fiendish', 'fierce', 'figurehead', 'filth', 'filthy', 'finagle', 'finicky', 'fissures', 'fist', 'flabbergast', 'flabbergasted', 'flagging', 'flagrant', 'flagrantly', 'flair', 'flairs', 'flak', 'flake', 'flakey', 'flakieness', 'flaking', 'flaky', 'flare', 'flares', 'flareup', 'flareups', 'flat-out', 'flaunt', 'flaw', 'flawed', 'flaws', 'flee', 'fleed', 'fleeing', 'fleer', 'flees', 'fleeting', 'flicering', 'flicker', 'flickering', 'flickers', 'flighty', 'flimflam', 'flimsy', 'flirt', 'flirty', 'floored', 'flounder', 'floundering', 'flout', 'fluster', 'foe', 'fool', 'fooled', 'foolhardy', 'foolish', 'foolishly', 'foolishness', 'forbid', 'forbidden', 'forbidding', 'forceful', 'foreboding', 'forebodingly', 'forfeit', 'forged', 'forgetful', 'forgetfully', 'forgetfulness', 'forlorn', 'forlornly', 'forsake', 'forsaken', 'forswear', 'foul', 'foully', 'foulness', 'fractious', 'fractiously', 'fracture', 'fragile', 'fragmented', 'frail', 'frantic', 'frantically', 'franticly', 'fraud', 'fraudulent', 'fraught', 'frazzle', 'frazzled', 'freak', 'freaking', 'freakish', 'freakishly', 'freaks', 'freeze', 'freezes', 'freezing', 'frenetic', 'frenetically', 'frenzied', 'frenzy', 'fret', 'fretful', 'frets', 'friction', 'frictions', 'fried', 'friggin', 'frigging', 'fright', 'frighten', 'frightening', 'frighteningly', 'frightful', 'frightfully', 'frigid', 'frost', 'frown', 'froze', 'frozen', 'fruitless', 'fruitlessly', 'frustrate', 'frustrated', 'frustrates', 'frustrating', 'frustratingly', 'frustration', 'frustrations', 'fuck', 'fucking', 'fudge', 'fugitive', 'full-blown', 'fulminate', 'fumble', 'fume', 'fumes', 'fundamentalism', 'funky', 'funnily', 'funny', 'furious', 'furiously', 'furor', 'fury', 'fuss', 'fussy', 'fustigate', 'fusty', 'futile', 'futilely', 'futility', 'fuzzy', 'gabble', 'gaff', 'gaffe', 'gainsay', 'gainsayer', 'gall', 'galling', 'gallingly', 'galls', 'gangster', 'gape', 'garbage', 'garish', 'gasp', 'gauche', 'gaudy', 'gawk', 'gawky', 'geezer', 'genocide', 'get-rich', 'ghastly', 'ghetto', 'ghosting', 'gibber', 'gibberish', 'gibe', 'giddy', 'gimmick', 'gimmicked', 'gimmicking', 'gimmicks', 'gimmicky', 'glare', 'glaringly', 'glib', 'glibly', 'glitch', 'glitches', 'gloatingly', 'gloom', 'gloomy', 'glower', 'glum', 'glut', 'gnawing', 'goad', 'goading', 'god-awful', 'goof', 'goofy', 'goon', 'gossip', 'graceless', 'gracelessly', 'graft', 'grainy', 'grapple', 'grate', 'grating', 'gravely', 'greasy', 'greed', 'greedy', 'grief', 'grievance', 'grievances', 'grieve', 'grieving', 'grievous', 'grievously', 'grim', 'grimace', 'grind', 'gripe', 'gripes', 'grisly', 'gritty', 'gross', 'grossly', 'grotesque', 'grouch', 'grouchy', 'groundless', 'grouse', 'growl', 'grudge', 'grudges', 'grudging', 'grudgingly', 'gruesome', 'gruesomely', 'gruff', 'grumble', 'grumpier', 'grumpiest', 'grumpily', 'grumpish', 'grumpy', 'guile', 'guilt', 'guiltily', 'guilty', 'gullible', 'gutless', 'gutter', 'hack', 'hacks', 'haggard', 'haggle', 'hairloss', 'halfhearted', 'halfheartedly', 'hallucinate', 'hallucination', 'hamper', 'hampered', 'handicapped', 'hang', 'hangs', 'haphazard', 'hapless', 'harangue', 'harass', 'harassed', 'harasses', 'harassment', 'harboring', 'harbors', 'hard', 'hard-hit', 'hard-line', 'hard-liner', 'hardball', 'harden', 'hardened', 'hardheaded', 'hardhearted', 'hardliner', 'hardliners', 'hardship', 'hardships', 'harm', 'harmed', 'harmful', 'harms', 'harpy', 'harridan', 'harried', 'harrow', 'harsh', 'harshly', 'hasseling', 'hassle', 'hassled', 'hassles', 'haste', 'hastily', 'hasty', 'hate', 'hated', 'hateful', 'hatefully', 'hatefulness', 'hater', 'haters', 'hates', 'hating', 'hatred', 'haughtily', 'haughty', 'haunt', 'haunting', 'havoc', 'hawkish', 'haywire', 'hazard', 'hazardous', 'haze', 'hazy', 'head-aches', 'headache', 'headaches', 'heartbreaker', 'heartbreaking', 'heartbreakingly', 'heartless', 'heathen', 'heavy-handed', 'heavyhearted', 'heck', 'heckle', 'heckled', 'heckles', 'hectic', 'hedge', 'hedonistic', 'heedless', 'hefty', 'hegemonism', 'hegemonistic', 'hegemony', 'heinous', 'hell', 'hell-bent', 'hellion', 'hells', 'helpless', 'helplessly', 'helplessness', 'heresy', 'heretic', 'heretical', 'hesitant', 'hestitant', 'hideous', 'hideously', 'hideousness', 'high-priced', 'hiliarious', 'hinder', 'hindrance', 'hiss', 'hissed', 'hissing', 'ho-hum', 'hoard', 'hoax', 'hobble', 'hogs', 'hollow', 'hoodium', 'hoodwink', 'hooligan', 'hopeless', 'hopelessly', 'hopelessness', 'horde', 'horrendous', 'horrendously', 'horrible', 'horrid', 'horrific', 'horrified', 'horrifies', 'horrify', 'horrifying', 'horrifys', 'hostage', 'hostile', 'hostilities', 'hostility', 'hotbeds', 'hothead', 'hotheaded', 'hothouse', 'hubris', 'huckster', 'hum', 'humid', 'humiliate', 'humiliating', 'humiliation', 'humming', 'hung', 'hurt', 'hurted', 'hurtful', 'hurting', 'hurts', 'hustler', 'hype', 'hypocricy', 'hypocrisy', 'hypocrite', 'hypocrites', 'hypocritical', 'hypocritically', 'hysteria', 'hysteric', 'hysterical', 'hysterically', 'hysterics', 'idiocies', 'idiocy', 'idiot', 'idiotic', 'idiotically', 'idiots', 'idle', 'ignoble', 'ignominious', 'ignominiously', 'ignominy', 'ignorance', 'ignorant', 'ignore', 'ill-advised', 'ill-conceived', 'ill-defined', 'ill-designed', 'ill-fated', 'ill-favored', 'ill-formed', 'ill-mannered', 'ill-natured', 'ill-sorted', 'ill-tempered', 'ill-treated', 'ill-treatment', 'ill-usage', 'ill-used', 'illegal', 'illegally', 'illegitimate', 'illicit', 'illiterate', 'illness', 'illogic', 'illogical', 'illogically', 'illusion', 'illusions', 'illusory', 'imaginary', 'imbalance', 'imbecile', 'imbroglio', 'immaterial', 'immature', 'imminence', 'imminently', 'immobilized', 'immoderate', 'immoderately', 'immodest', 'immoral', 'immorality', 'immorally', 'immovable', 'impair', 'impaired', 'impasse', 'impatience', 'impatient', 'impatiently', 'impeach', 'impedance', 'impede', 'impediment', 'impending', 'impenitent', 'imperfect', 'imperfection', 'imperfections', 'imperfectly', 'imperialist', 'imperil', 'imperious', 'imperiously', 'impermissible', 'impersonal', 'impertinent', 'impetuous', 'impetuously', 'impiety', 'impinge', 'impious', 'implacable', 'implausible', 'implausibly', 'implicate', 'implication', 'implode', 'impolite', 'impolitely', 'impolitic', 'importunate', 'importune', 'impose', 'imposers', 'imposing', 'imposition', 'impossible', 'impossiblity', 'impossibly', 'impotent', 'impoverish', 'impoverished', 'impractical', 'imprecate', 'imprecise', 'imprecisely', 'imprecision', 'imprison', 'imprisonment', 'improbability', 'improbable', 'improbably', 'improper', 'improperly', 'impropriety', 'imprudence', 'imprudent', 'impudence', 'impudent', 'impudently', 'impugn', 'impulsive', 'impulsively', 'impunity', 'impure', 'impurity', 'inability', 'inaccuracies', 'inaccuracy', 'inaccurate', 'inaccurately', 'inaction', 'inactive', 'inadequacy', 'inadequate', 'inadequately', 'inadverent', 'inadverently', 'inadvisable', 'inadvisably', 'inane', 'inanely', 'inappropriate', 'inappropriately', 'inapt', 'inaptitude', 'inarticulate', 'inattentive', 'inaudible', 'incapable', 'incapably', 'incautious', 'incendiary', 'incense', 'incessant', 'incessantly', 'incite', 'incitement', 'incivility', 'inclement', 'incognizant', 'incoherence', 'incoherent', 'incoherently', 'incommensurate', 'incomparable', 'incomparably', 'incompatability', 'incompatibility', 'incompatible', 'incompetence', 'incompetent', 'incompetently', 'incomplete', 'incompliant', 'incomprehensible', 'incomprehension', 'inconceivable', 'inconceivably', 'incongruous', 'incongruously', 'inconsequent', 'inconsequential', 'inconsequentially', 'inconsequently', 'inconsiderate', 'inconsiderately', 'inconsistence', 'inconsistencies', 'inconsistency', 'inconsistent', 'inconsolable', 'inconsolably', 'inconstant', 'inconvenience', 'inconveniently', 'incorrect', 'incorrectly', 'incorrigible', 'incorrigibly', 'incredulous', 'incredulously', 'inculcate', 'indecency', 'indecent', 'indecently', 'indecision', 'indecisive', 'indecisively', 'indecorum', 'indefensible', 'indelicate', 'indeterminable', 'indeterminably', 'indeterminate', 'indifference', 'indifferent', 'indigent', 'indignant', 'indignantly', 'indignation', 'indignity', 'indiscernible', 'indiscreet', 'indiscreetly', 'indiscretion', 'indiscriminate', 'indiscriminately', 'indiscriminating', 'indistinguishable', 'indoctrinate', 'indoctrination', 'indolent', 'indulge', 'ineffective', 'ineffectively', 'ineffectiveness', 'ineffectual', 'ineffectually', 'ineffectualness', 'inefficacious', 'inefficacy', 'inefficiency', 'inefficient', 'inefficiently', 'inelegance', 'inelegant', 'ineligible', 'ineloquent', 'ineloquently', 'inept', 'ineptitude', 'ineptly', 'inequalities', 'inequality', 'inequitable', 'inequitably', 'inequities', 'inescapable', 'inescapably', 'inessential', 'inevitable', 'inevitably', 'inexcusable', 'inexcusably', 'inexorable', 'inexorably', 'inexperience', 'inexperienced', 'inexpert', 'inexpertly', 'inexpiable', 'inexplainable', 'inextricable', 'inextricably', 'infamous', 'infamously', 'infamy', 'infected', 'infection', 'infections', 'inferior', 'inferiority', 'infernal', 'infest', 'infested', 'infidel', 'infidels', 'infiltrator', 'infiltrators', 'infirm', 'inflame', 'inflammation', 'inflammatory', 'inflammed', 'inflated', 'inflationary', 'inflexible', 'inflict', 'infraction', 'infringe', 'infringement', 'infringements', 'infuriate', 'infuriated', 'infuriating', 'infuriatingly', 'inglorious', 'ingrate', 'ingratitude', 'inhibit', 'inhibition', 'inhospitable', 'inhospitality', 'inhuman', 'inhumane', 'inhumanity', 'inimical', 'inimically', 'iniquitous', 'iniquity', 'injudicious', 'injure', 'injurious', 'injury', 'injustice', 'injustices', 'innuendo', 'inoperable', 'inopportune', 'inordinate', 'inordinately', 'insane', 'insanely', 'insanity', 'insatiable', 'insecure', 'insecurity', 'insensible', 'insensitive', 'insensitively', 'insensitivity', 'insidious', 'insidiously', 'insignificance', 'insignificant', 'insignificantly', 'insincere', 'insincerely', 'insincerity', 'insinuate', 'insinuating', 'insinuation', 'insociable', 'insolence', 'insolent', 'insolently', 'insolvent', 'insouciance', 'instability', 'instable', 'instigate', 'instigator', 'instigators', 'insubordinate', 'insubstantial', 'insubstantially', 'insufferable', 'insufferably', 'insufficiency', 'insufficient', 'insufficiently', 'insular', 'insult', 'insulted', 'insulting', 'insultingly', 'insults', 'insupportable', 'insupportably', 'insurmountable', 'insurmountably', 'insurrection', 'intefere', 'inteferes', 'intense', 'interfere', 'interference', 'interferes', 'intermittent', 'interrupt', 'interruption', 'interruptions', 'intimidate', 'intimidating', 'intimidatingly', 'intimidation', 'intolerable', 'intolerablely', 'intolerance', 'intoxicate', 'intractable', 'intransigence', 'intransigent', 'intrude', 'intrusion', 'intrusive', 'inundate', 'inundated', 'invader', 'invalid', 'invalidate', 'invalidity', 'invasive', 'invective', 'inveigle', 'invidious', 'invidiously', 'invidiousness', 'invisible', 'involuntarily', 'involuntary', 'irascible', 'irate', 'irately', 'ire', 'irk', 'irked', 'irking', 'irks', 'irksome', 'irksomely', 'irksomeness', 'irksomenesses', 'ironic', 'ironical', 'ironically', 'ironies', 'irony', 'irragularity', 'irrational', 'irrationalities', 'irrationality', 'irrationally', 'irrationals', 'irreconcilable', 'irrecoverable', 'irrecoverableness', 'irrecoverablenesses', 'irrecoverably', 'irredeemable', 'irredeemably', 'irreformable', 'irregular', 'irregularity', 'irrelevance', 'irrelevant', 'irreparable', 'irreplacible', 'irrepressible', 'irresolute', 'irresolvable', 'irresponsible', 'irresponsibly', 'irretating', 'irretrievable', 'irreversible', 'irritable', 'irritably', 'irritant', 'irritate', 'irritated', 'irritating', 'irritation', 'irritations', 'isolate', 'isolated', 'isolation', 'issue', 'issues', 'itch', 'itching', 'itchy', 'jabber', 'jaded', 'jagged', 'jam', 'jarring', 'jaundiced', 'jealous', 'jealously', 'jealousness', 'jealousy', 'jeer', 'jeering', 'jeeringly', 'jeers', 'jeopardize', 'jeopardy', 'jerk', 'jerky', 'jitter', 'jitters', 'jittery', 'job-killing', 'jobless', 'joke', 'joker', 'jolt', 'judder', 'juddering', 'judders', 'jumpy', 'junk', 'junky', 'junkyard', 'jutter', 'jutters', 'kaput', 'kill', 'killed', 'killer', 'killing', 'killjoy', 'kills', 'knave', 'knife', 'knock', 'knotted', 'kook', 'kooky', 'lack', 'lackadaisical', 'lacked', 'lackey', 'lackeys', 'lacking', 'lackluster', 'lacks', 'laconic', 'lag', 'lagged', 'lagging', 'laggy', 'lags', 'laid-off', 'lambast', 'lambaste', 'lame', 'lame-duck', 'lament', 'lamentable', 'lamentably', 'languid', 'languish', 'languor', 'languorous', 'languorously', 'lanky', 'lapse', 'lapsed', 'lapses', 'lascivious', 'last-ditch', 'latency', 'laughable', 'laughably', 'laughingstock', 'lawbreaker', 'lawbreaking', 'lawless', 'lawlessness', 'layoff', 'layoff-happy', 'lazy', 'leak', 'leakage', 'leakages', 'leaking', 'leaks', 'leaky', 'lech', 'lecher', 'lecherous', 'lechery', 'leech', 'leer', 'leery', 'left-leaning', 'lemon', 'lengthy', 'less-developed', 'lesser-known', 'letch', 'lethal', 'lethargic', 'lethargy', 'lewd', 'lewdly', 'lewdness', 'liability', 'liable', 'liar', 'liars', 'licentious', 'licentiously', 'licentiousness', 'lie', 'lied', 'lier', 'lies', 'life-threatening', 'lifeless', 'limit', 'limitation', 'limitations', 'limited', 'limits', 'limp', 'listless', 'litigious', 'little-known', 'livid', 'lividly', 'loath', 'loathe', 'loathing', 'loathly', 'loathsome', 'loathsomely', 'lone', 'loneliness', 'lonely', 'loner', 'lonesome', 'long-time', 'long-winded', 'longing', 'longingly', 'loophole', 'loopholes', 'loose', 'loot', 'lorn', 'lose', 'loser', 'losers', 'loses', 'losing', 'loss', 'losses', 'lost', 'loud', 'louder', 'lousy', 'loveless', 'lovelorn', 'low-rated', 'lowly', 'ludicrous', 'ludicrously', 'lugubrious', 'lukewarm', 'lull', 'lumpy', 'lunatic', 'lunaticism', 'lurch', 'lure', 'lurid', 'lurk', 'lurking', 'lying', 'macabre', 'mad', 'madden', 'maddening', 'maddeningly', 'madder', 'madly', 'madman', 'madness', 'maladjusted', 'maladjustment', 'malady', 'malaise', 'malcontent', 'malcontented', 'maledict', 'malevolence', 'malevolent', 'malevolently', 'malice', 'malicious', 'maliciously', 'maliciousness', 'malign', 'malignant', 'malodorous', 'maltreatment', 'mangle', 'mangled', 'mangles', 'mangling', 'mania', 'maniac', 'maniacal', 'manic', 'manipulate', 'manipulation', 'manipulative', 'manipulators', 'mar', 'marginal', 'marginally', 'martyrdom', 'martyrdom-seeking', 'mashed', 'massacre', 'massacres', 'matte', 'mawkish', 'mawkishly', 'mawkishness', 'meager', 'meaningless', 'meanness', 'measly', 'meddle', 'meddlesome', 'mediocre', 'mediocrity', 'melancholy', 'melodramatic', 'melodramatically', 'meltdown', 'menace', 'menacing', 'menacingly', 'mendacious', 'mendacity', 'menial', 'merciless', 'mercilessly', 'mess', 'messed', 'messes', 'messing', 'messy', 'midget', 'miff', 'militancy', 'mindless', 'mindlessly', 'mirage', 'mire', 'misalign', 'misaligned', 'misaligns', 'misapprehend', 'misbecome', 'misbecoming', 'misbegotten', 'misbehave', 'misbehavior', 'miscalculate', 'miscalculation', 'miscellaneous', 'mischief', 'mischievous', 'mischievously', 'misconception', 'misconceptions', 'miscreant', 'miscreants', 'misdirection', 'miser', 'miserable', 'miserableness', 'miserably', 'miseries', 'miserly', 'misery', 'misfit', 'misfortune', 'misgiving', 'misgivings', 'misguidance', 'misguide', 'misguided', 'mishandle', 'mishap', 'misinform', 'misinformed', 'misinterpret', 'misjudge', 'misjudgment', 'mislead', 'misleading', 'misleadingly', 'mislike', 'mismanage', 'mispronounce', 'mispronounced', 'mispronounces', 'misread', 'misreading', 'misrepresent', 'misrepresentation', 'miss', 'missed', 'misses', 'misstatement', 'mist', 'mistake', 'mistaken', 'mistakenly', 'mistakes', 'mistified', 'mistress', 'mistrust', 'mistrustful', 'mistrustfully', 'mists', 'misunderstand', 'misunderstanding', 'misunderstandings', 'misunderstood', 'misuse', 'moan', 'mobster', 'mock', 'mocked', 'mockeries', 'mockery', 'mocking', 'mockingly', 'mocks', 'molest', 'molestation', 'monotonous', 'monotony', 'monster', 'monstrosities', 'monstrosity', 'monstrous', 'monstrously', 'moody', 'moot', 'mope', 'morbid', 'morbidly', 'mordant', 'mordantly', 'moribund', 'moron', 'moronic', 'morons', 'mortification', 'mortified', 'mortify', 'mortifying', 'motionless', 'motley', 'mourn', 'mourner', 'mournful', 'mournfully', 'muddle', 'muddy', 'mudslinger', 'mudslinging', 'mulish', 'multi-polarization', 'mundane', 'murder', 'murderer', 'murderous', 'murderously', 'murky', 'muscle-flexing', 'mushy', 'musty', 'mysterious', 'mysteriously', 'mystery', 'mystify', 'myth', 'nag', 'nagging', 'naive', 'naively', 'narrower', 'nastily', 'nastiness', 'nasty', 'naughty', 'nauseate', 'nauseates', 'nauseating', 'nauseatingly', 'naïve', 'nebulous', 'nebulously', 'needless', 'needlessly', 'needy', 'nefarious', 'nefariously', 'negate', 'negation', 'negative', 'negatives', 'negativity', 'neglect', 'neglected', 'negligence', 'negligent', 'nemesis', 'nepotism', 'nervous', 'nervously', 'nervousness', 'nettle', 'nettlesome', 'neurotic', 'neurotically', 'niggle', 'niggles', 'nightmare', 'nightmarish', 'nightmarishly', 'nitpick', 'nitpicking', 'noise', 'noises', 'noisier', 'noisy', 'non-confidence', 'nonexistent', 'nonresponsive', 'nonsense', 'nosey', 'notoriety', 'notorious', 'notoriously', 'noxious', 'nuisance', 'numb', 'obese', 'object', 'objection', 'objectionable', 'objections', 'oblique', 'obliterate', 'obliterated', 'oblivious', 'obnoxious', 'obnoxiously', 'obscene', 'obscenely', 'obscenity', 'obscure', 'obscured', 'obscures', 'obscurity', 'obsess', 'obsessive', 'obsessively', 'obsessiveness', 'obsolete', 'obstacle', 'obstinate', 'obstinately', 'obstruct', 'obstructed', 'obstructing', 'obstruction', 'obstructs', 'obtrusive', 'obtuse', 'occlude', 'occluded', 'occludes', 'occluding', 'odd', 'odder', 'oddest', 'oddities', 'oddity', 'oddly', 'odor', 'offence', 'offend', 'offender', 'offending', 'offenses', 'offensive', 'offensively', 'offensiveness', 'officious', 'ominous', 'ominously', 'omission', 'omit', 'one-sided', 'onerous', 'onerously', 'onslaught', 'opinionated', 'opponent', 'opportunistic', 'oppose', 'opposition', 'oppositions', 'oppress', 'oppression', 'oppressive', 'oppressively', 'oppressiveness', 'oppressors', 'ordeal', 'orphan', 'ostracize', 'outbreak', 'outburst', 'outbursts', 'outcast', 'outcry', 'outlaw', 'outmoded', 'outrage', 'outraged', 'outrageous', 'outrageously', 'outrageousness', 'outrages', 'outsider', 'over-acted', 'over-awe', 'over-balanced', 'over-hyped', 'over-priced', 'over-valuation', 'overact', 'overacted', 'overawe', 'overbalance', 'overbalanced', 'overbearing', 'overbearingly', 'overblown', 'overdo', 'overdone', 'overdue', 'overemphasize', 'overheat', 'overkill', 'overloaded', 'overlook', 'overpaid', 'overpayed', 'overplay', 'overpower', 'overpriced', 'overrated', 'overreach', 'overrun', 'overshadow', 'oversight', 'oversights', 'oversimplification', 'oversimplified', 'oversimplify', 'oversize', 'overstate', 'overstated', 'overstatement', 'overstatements', 'overstates', 'overtaxed', 'overthrow', 'overthrows', 'overturn', 'overweight', 'overwhelm', 'overwhelmed', 'overwhelming', 'overwhelmingly', 'overwhelms', 'overzealous', 'overzealously', 'overzelous', 'pain', 'painful', 'painfull', 'painfully', 'pains', 'pale', 'pales', 'paltry', 'pan', 'pandemonium', 'pander', 'pandering', 'panders', 'panic', 'panick', 'panicked', 'panicking', 'panicky', 'paradoxical', 'paradoxically', 'paralize', 'paralyzed', 'paranoia', 'paranoid', 'parasite', 'pariah', 'parody', 'partiality', 'partisan', 'partisans', 'passe', 'passive', 'passiveness', 'pathetic', 'pathetically', 'patronize', 'paucity', 'pauper', 'paupers', 'payback', 'peculiar', 'peculiarly', 'pedantic', 'peeled', 'peeve', 'peeved', 'peevish', 'peevishly', 'penalize', 'penalty', 'perfidious', 'perfidity', 'perfunctory', 'peril', 'perilous', 'perilously', 'perish', 'pernicious', 'perplex', 'perplexed', 'perplexing', 'perplexity', 'persecute', 'persecution', 'pertinacious', 'pertinaciously', 'pertinacity', 'perturb', 'perturbed', 'pervasive', 'perverse', 'perversely', 'perversion', 'perversity', 'pervert', 'perverted', 'perverts', 'pessimism', 'pessimistic', 'pessimistically', 'pest', 'pestilent', 'petrified', 'petrify', 'pettifog', 'petty', 'phobia', 'phobic', 'phony', 'picket', 'picketed', 'picketing', 'pickets', 'picky', 'pig', 'pigs', 'pillage', 'pillory', 'pimple', 'pinch', 'pique', 'pitiable', 'pitiful', 'pitifully', 'pitiless', 'pitilessly', 'pittance', 'pity', 'plagiarize', 'plague', 'plasticky', 'plaything', 'plea', 'pleas', 'plebeian', 'plight', 'plot', 'plotters', 'ploy', 'plunder', 'plunderer', 'pointless', 'pointlessly', 'poison', 'poisonous', 'poisonously', 'pokey', 'poky', 'polarisation', 'polemize', 'pollute', 'polluter', 'polluters', 'polution', 'pompous', 'poor', 'poorer', 'poorest', 'poorly', 'posturing', 'pout', 'poverty', 'powerless', 'prate', 'pratfall', 'prattle', 'precarious', 'precariously', 'precipitate', 'precipitous', 'predatory', 'predicament', 'prejudge', 'prejudice', 'prejudices', 'prejudicial', 'premeditated', 'preoccupy', 'preposterous', 'preposterously', 'presumptuous', 'presumptuously', 'pretence', 'pretend', 'pretense', 'pretentious', 'pretentiously', 'prevaricate', 'pricey', 'pricier', 'prick', 'prickle', 'prickles', 'prideful', 'prik', 'primitive', 'prison', 'prisoner', 'problem', 'problematic', 'problems', 'procrastinate', 'procrastinates', 'procrastination', 'profane', 'profanity', 'prohibit', 'prohibitive', 'prohibitively', 'propaganda', 'propagandize', 'proprietary', 'prosecute', 'protest', 'protested', 'protesting', 'protests', 'protracted', 'provocation', 'provocative', 'provoke', 'pry', 'pugnacious', 'pugnaciously', 'pugnacity', 'punch', 'punish', 'punishable', 'punitive', 'punk', 'puny', 'puppet', 'puppets', 'puzzled', 'puzzlement', 'puzzling', 'quack', 'qualm', 'qualms', 'quandary', 'quarrel', 'quarrellous', 'quarrellously', 'quarrels', 'quarrelsome', 'quash', 'queer', 'questionable', 'quibble', 'quibbles', 'quitter', 'rabid', 'racism', 'racist', 'racists', 'racy', 'radical', 'radicalization', 'radically', 'radicals', 'rage', 'ragged', 'raging', 'rail', 'raked', 'rampage', 'rampant', 'ramshackle', 'rancor', 'randomly', 'rankle', 'rant', 'ranted', 'ranting', 'rantingly', 'rants', 'rape', 'raped', 'raping', 'rascal', 'rascals', 'rash', 'rattle', 'rattled', 'rattles', 'ravage', 'raving', 'reactionary', 'rebellious', 'rebuff', 'rebuke', 'recalcitrant', 'recant', 'recession', 'recessionary', 'reckless', 'recklessly', 'recklessness', 'recoil', 'recourses', 'redundancy', 'redundant', 'refusal', 'refuse', 'refused', 'refuses', 'refusing', 'refutation', 'refute', 'refuted', 'refutes', 'refuting', 'regress', 'regression', 'regressive', 'regret', 'regreted', 'regretful', 'regretfully', 'regrets', 'regrettable', 'regrettably', 'regretted', 'reject', 'rejected', 'rejecting', 'rejection', 'rejects', 'relapse', 'relentless', 'relentlessly', 'relentlessness', 'reluctance', 'reluctant', 'reluctantly', 'remorse', 'remorseful', 'remorsefully', 'remorseless', 'remorselessly', 'remorselessness', 'renounce', 'renunciation', 'repel', 'repetitive', 'reprehensible', 'reprehensibly', 'reprehension', 'reprehensive', 'repress', 'repression', 'repressive', 'reprimand', 'reproach', 'reproachful', 'reprove', 'reprovingly', 'repudiate', 'repudiation', 'repugn', 'repugnance', 'repugnant', 'repugnantly', 'repulse', 'repulsed', 'repulsing', 'repulsive', 'repulsively', 'repulsiveness', 'resent', 'resentful', 'resentment', 'resignation', 'resigned', 'resistance', 'restless', 'restlessness', 'restrict', 'restricted', 'restriction', 'restrictive', 'resurgent', 'retaliate', 'retaliatory', 'retard', 'retarded', 'retardedness', 'retards', 'reticent', 'retract', 'retreat', 'retreated', 'revenge', 'revengeful', 'revengefully', 'revert', 'revile', 'reviled', 'revoke', 'revolt', 'revolting', 'revoltingly', 'revulsion', 'revulsive', 'rhapsodize', 'rhetoric', 'rhetorical', 'ricer', 'ridicule', 'ridicules', 'ridiculous', 'ridiculously', 'rife', 'rift', 'rifts', 'rigid', 'rigidity', 'rigidness', 'rile', 'riled', 'rip', 'rip-off', 'ripoff', 'ripped', 'risk', 'risks', 'risky', 'rival', 'rivalry', 'roadblocks', 'rocky', 'rogue', 'rollercoaster', 'rot', 'rotten', 'rough', 'rremediable', 'rubbish', 'rude', 'rue', 'ruffian', 'ruffle', 'ruin', 'ruined', 'ruining', 'ruinous', 'ruins', 'rumbling', 'rumor', 'rumors', 'rumours', 'rumple', 'run-down', 'runaway', 'rupture', 'rust', 'rusts', 'rusty', 'rut', 'ruthless', 'ruthlessly', 'ruthlessness', 'ruts', 'sabotage', 'sack', 'sacrificed', 'sad', 'sadden', 'sadly', 'sadness', 'sag', 'sagged', 'sagging', 'saggy', 'sags', 'salacious', 'sanctimonious', 'sap', 'sarcasm', 'sarcastic', 'sarcastically', 'sardonic', 'sardonically', 'sass', 'satirical', 'satirize', 'savage', 'savaged', 'savagery', 'savages', 'scaly', 'scam', 'scams', 'scandal', 'scandalize', 'scandalized', 'scandalous', 'scandalously', 'scandals', 'scandel', 'scandels', 'scant', 'scapegoat', 'scar', 'scarce', 'scarcely', 'scarcity', 'scare', 'scared', 'scarier', 'scariest', 'scarily', 'scarred', 'scars', 'scary', 'scathing', 'scathingly', 'sceptical', 'scoff', 'scoffingly', 'scold', 'scolded', 'scolding', 'scoldingly', 'scorching', 'scorchingly', 'scorn', 'scornful', 'scornfully', 'scoundrel', 'scourge', 'scowl', 'scramble', 'scrambled', 'scrambles', 'scrambling', 'scrap', 'scratch', 'scratched', 'scratches', 'scratchy', 'scream', 'screech', 'screw-up', 'screwed', 'screwed-up', 'screwy', 'scuff', 'scuffs', 'scum', 'scummy', 'second-class', 'second-tier', 'secretive', 'sedentary', 'seedy', 'seethe', 'seething', 'self-coup', 'self-criticism', 'self-defeating', 'self-destructive', 'self-humiliation', 'self-interest', 'self-interested', 'self-serving', 'selfinterested', 'selfish', 'selfishly', 'selfishness', 'semi-retarded', 'senile', 'sensationalize', 'senseless', 'senselessly', 'seriousness', 'sermonize', 'servitude', 'set-up', 'setback', 'setbacks', 'sever', 'severe', 'severity', 'sh*t', 'shabby', 'shadowy', 'shady', 'shake', 'shaky', 'shallow', 'sham', 'shambles', 'shame', 'shameful', 'shamefully', 'shamefulness', 'shameless', 'shamelessly', 'shamelessness', 'shark', 'sharply', 'shatter', 'shemale', 'shimmer', 'shimmy', 'shipwreck', 'shirk', 'shirker', 'shit', 'shiver', 'shock', 'shocked', 'shocking', 'shockingly', 'shoddy', 'short-lived', 'shortage', 'shortchange', 'shortcoming', 'shortcomings', 'shortness', 'shortsighted', 'shortsightedness', 'showdown', 'shrew', 'shriek', 'shrill', 'shrilly', 'shrivel', 'shroud', 'shrouded', 'shrug', 'shun', 'shunned', 'sick', 'sicken', 'sickening', 'sickeningly', 'sickly', 'sickness', 'sidetrack', 'sidetracked', 'siege', 'sillily', 'silly', 'simplistic', 'simplistically', 'sin', 'sinful', 'sinfully', 'sinister', 'sinisterly', 'sink', 'sinking', 'skeletons', 'skeptic', 'skeptical', 'skeptically', 'skepticism', 'sketchy', 'skimpy', 'skinny', 'skittish', 'skittishly', 'skulk', 'slack', 'slander', 'slanderer', 'slanderous', 'slanderously', 'slanders', 'slap', 'slashing', 'slaughter', 'slaughtered', 'slave', 'slaves', 'sleazy', 'slime', 'slog', 'slogged', 'slogging', 'slogs', 'sloooooooooooooow', 'sloooow', 'slooow', 'sloow', 'sloppily', 'sloppy', 'sloth', 'slothful', 'slow', 'slow-moving', 'slowed', 'slower', 'slowest', 'slowly', 'sloww', 'slowww', 'slowwww', 'slug', 'sluggish', 'slump', 'slumping', 'slumpping', 'slur', 'slut', 'sluts', 'sly', 'smack', 'smallish', 'smash', 'smear', 'smell', 'smelled', 'smelling', 'smells', 'smelly', 'smelt', 'smoke', 'smokescreen', 'smolder', 'smoldering', 'smother', 'smoulder', 'smouldering', 'smudge', 'smudged', 'smudges', 'smudging', 'smug', 'smugly', 'smut', 'smuttier', 'smuttiest', 'smutty', 'snag', 'snagged', 'snagging', 'snags', 'snappish', 'snappishly', 'snare', 'snarky', 'snarl', 'sneak', 'sneakily', 'sneaky', 'sneer', 'sneering', 'sneeringly', 'snob', 'snobbish', 'snobby', 'snobish', 'snobs', 'snub', 'so-cal', 'soapy', 'sob', 'sober', 'sobering', 'solemn', 'solicitude', 'somber', 'sore', 'sorely', 'soreness', 'sorrow', 'sorrowful', 'sorrowfully', 'sorry', 'sour', 'sourly', 'spade', 'spank', 'spendy', 'spew', 'spewed', 'spewing', 'spews', 'spilling', 'spinster', 'spiritless', 'spite', 'spiteful', 'spitefully', 'spitefulness', 'splatter', 'split', 'splitting', 'spoil', 'spoilage', 'spoilages', 'spoiled', 'spoilled', 'spoils', 'spook', 'spookier', 'spookiest', 'spookily', 'spooky', 'spoon-fed', 'spoon-feed', 'spoonfed', 'sporadic', 'spotty', 'spurious', 'spurn', 'sputter', 'squabble', 'squabbling', 'squander', 'squash', 'squeak', 'squeaks', 'squeaky', 'squeal', 'squealing', 'squeals', 'squirm', 'stab', 'stagnant', 'stagnate', 'stagnation', 'staid', 'stain', 'stains', 'stale', 'stalemate', 'stall', 'stalls', 'stammer', 'stampede', 'standstill', 'stark', 'starkly', 'startle', 'startling', 'startlingly', 'starvation', 'starve', 'static', 'steal', 'stealing', 'steals', 'steep', 'steeply', 'stench', 'stereotype', 'stereotypical', 'stereotypically', 'stern', 'stew', 'sticky', 'stiff', 'stiffness', 'stifle', 'stifling', 'stiflingly', 'stigma', 'stigmatize', 'sting', 'stinging', 'stingingly', 'stingy', 'stink', 'stinks', 'stodgy', 'stole', 'stolen', 'stooge', 'stooges', 'stormy', 'straggle', 'straggler', 'strain', 'strained', 'straining', 'strange', 'strangely', 'stranger', 'strangest', 'strangle', 'streaky', 'strenuous', 'stress', 'stresses', 'stressful', 'stressfully', 'stricken', 'strict', 'strictly', 'strident', 'stridently', 'strife', 'strike', 'stringent', 'stringently', 'struck', 'struggle', 'struggled', 'struggles', 'struggling', 'strut', 'stubborn', 'stubbornly', 'stubbornness', 'stuck', 'stuffy', 'stumble', 'stumbled', 'stumbles', 'stump', 'stumped', 'stumps', 'stun', 'stunt', 'stunted', 'stupid', 'stupidest', 'stupidity', 'stupidly', 'stupified', 'stupify', 'stupor', 'stutter', 'stuttered', 'stuttering', 'stutters', 'sty', 'stymied', 'sub-par', 'subdued', 'subjected', 'subjection', 'subjugate', 'subjugation', 'submissive', 'subordinate', 'subpoena', 'subpoenas', 'subservience', 'subservient', 'substandard', 'subtract', 'subversion', 'subversive', 'subversively', 'subvert', 'succumb', 'suck', 'sucked', 'sucker', 'sucks', 'sucky', 'sue', 'sued', 'sueing', 'sues', 'suffer', 'suffered', 'sufferer', 'sufferers', 'suffering', 'suffers', 'suffocate', 'sugar-coat', 'sugar-coated', 'sugarcoated', 'suicidal', 'suicide', 'sulk', 'sullen', 'sully', 'sunder', 'sunk', 'sunken', 'superficial', 'superficiality', 'superficially', 'superfluous', 'superstition', 'superstitious', 'suppress', 'suppression', 'surrender', 'susceptible', 'suspect', 'suspicion', 'suspicions', 'suspicious', 'suspiciously', 'swagger', 'swamped', 'sweaty', 'swelled', 'swelling', 'swindle', 'swipe', 'swollen', 'symptom', 'symptoms', 'syndrome', 'taboo', 'tacky', 'taint', 'tainted', 'tamper', 'tangle', 'tangled', 'tangles', 'tank', 'tanked', 'tanks', 'tantrum', 'tardy', 'tarnish', 'tarnished', 'tarnishes', 'tarnishing', 'tattered', 'taunt', 'taunting', 'tauntingly', 'taunts', 'taut', 'tawdry', 'taxing', 'tease', 'teasingly', 'tedious', 'tediously', 'temerity', 'temper', 'tempest', 'temptation', 'tenderness', 'tense', 'tension', 'tentative', 'tentatively', 'tenuous', 'tenuously', 'tepid', 'terrible', 'terribleness', 'terribly', 'terror', 'terror-genic', 'terrorism', 'terrorize', 'testily', 'testy', 'tetchily', 'tetchy', 'thankless', 'thicker', 'thirst', 'thorny', 'thoughtless', 'thoughtlessly', 'thoughtlessness', 'thrash', 'threat', 'threaten', 'threatening', 'threats', 'threesome', 'throb', 'throbbed', 'throbbing', 'throbs', 'throttle', 'thug', 'thumb-down', 'thumbs-down', 'thwart', 'time-consuming', 'timid', 'timidity', 'timidly', 'timidness', 'tin-y', 'tingled', 'tingling', 'tired', 'tiresome', 'tiring', 'tiringly', 'toil', 'toll', 'top-heavy', 'topple', 'torment', 'tormented', 'torrent', 'tortuous', 'torture', 'tortured', 'tortures', 'torturing', 'torturous', 'torturously', 'totalitarian', 'touchy', 'toughness', 'tout', 'touted', 'touts', 'toxic', 'traduce', 'tragedy', 'tragic', 'tragically', 'traitor', 'traitorous', 'traitorously', 'tramp', 'trample', 'transgress', 'transgression', 'trap', 'traped', 'trapped', 'trash', 'trashed', 'trashy', 'trauma', 'traumatic', 'traumatically', 'traumatize', 'traumatized', 'travesties', 'travesty', 'treacherous', 'treacherously', 'treachery', 'treason', 'treasonous', 'trick', 'tricked', 'trickery', 'tricky', 'trivial', 'trivialize', 'trouble', 'troubled', 'troublemaker', 'troubles', 'troublesome', 'troublesomely', 'troubling', 'troublingly', 'truant', 'tumble', 'tumbled', 'tumbles', 'tumultuous', 'turbulent', 'turmoil', 'twist', 'twisted', 'twists', 'two-faced', 'two-faces', 'tyrannical', 'tyrannically', 'tyranny', 'tyrant', 'ugh', 'uglier', 'ugliest', 'ugliness', 'ugly', 'ulterior', 'ultimatum', 'ultimatums', 'ultra-hardline', 'un-viewable', 'unable', 'unacceptable', 'unacceptablely', 'unacceptably', 'unaccessible', 'unaccustomed', 'unachievable', 'unaffordable', 'unappealing', 'unattractive', 'unauthentic', 'unavailable', 'unavoidably', 'unbearable', 'unbearablely', 'unbelievable', 'unbelievably', 'uncaring', 'uncertain', 'uncivil', 'uncivilized', 'unclean', 'unclear', 'uncollectible', 'uncomfortable', 'uncomfortably', 'uncomfy', 'uncompetitive', 'uncompromising', 'uncompromisingly', 'unconfirmed', 'unconstitutional', 'uncontrolled', 'unconvincing', 'unconvincingly', 'uncooperative', 'uncouth', 'uncreative', 'undecided', 'undefined', 'undependability', 'undependable', 'undercut', 'undercuts', 'undercutting', 'underdog', 'underestimate', 'underlings', 'undermine', 'undermined', 'undermines', 'undermining', 'underpaid', 'underpowered', 'undersized', 'undesirable', 'undetermined', 'undid', 'undignified', 'undissolved', 'undocumented', 'undone', 'undue', 'unease', 'uneasily', 'uneasiness', 'uneasy', 'uneconomical', 'unemployed', 'unequal', 'unethical', 'uneven', 'uneventful', 'unexpected', 'unexpectedly', 'unexplained', 'unfairly', 'unfaithful', 'unfaithfully', 'unfamiliar', 'unfavorable', 'unfeeling', 'unfinished', 'unfit', 'unforeseen', 'unforgiving', 'unfortunate', 'unfortunately', 'unfounded', 'unfriendly', 'unfulfilled', 'unfunded', 'ungovernable', 'ungrateful', 'unhappily', 'unhappiness', 'unhappy', 'unhealthy', 'unhelpful', 'unilateralism', 'unimaginable', 'unimaginably', 'unimportant', 'uninformed', 'uninsured', 'unintelligible', 'unintelligile', 'unipolar', 'unjust', 'unjustifiable', 'unjustifiably', 'unjustified', 'unjustly', 'unkind', 'unkindly', 'unknown', 'unlamentable', 'unlamentably', 'unlawful', 'unlawfully', 'unlawfulness', 'unleash', 'unlicensed', 'unlikely', 'unlucky', 'unmoved', 'unnatural', 'unnaturally', 'unnecessary', 'unneeded', 'unnerve', 'unnerved', 'unnerving', 'unnervingly', 'unnoticed', 'unobserved', 'unorthodox', 'unorthodoxy', 'unpleasant', 'unpleasantries', 'unpopular', 'unpredictable', 'unprepared', 'unproductive', 'unprofitable', 'unprove', 'unproved', 'unproven', 'unproves', 'unproving', 'unqualified', 'unravel', 'unraveled', 'unreachable', 'unreadable', 'unrealistic', 'unreasonable', 'unreasonably', 'unrelenting', 'unrelentingly', 'unreliability', 'unreliable', 'unresolved', 'unresponsive', 'unrest', 'unruly', 'unsafe', 'unsatisfactory', 'unsavory', 'unscrupulous', 'unscrupulously', 'unsecure', 'unseemly', 'unsettle', 'unsettled', 'unsettling', 'unsettlingly', 'unskilled', 'unsophisticated', 'unsound', 'unspeakable', 'unspeakablely', 'unspecified', 'unstable', 'unsteadily', 'unsteadiness', 'unsteady', 'unsuccessful', 'unsuccessfully', 'unsupported', 'unsupportive', 'unsure', 'unsuspecting', 'unsustainable', 'untenable', 'untested', 'unthinkable', 'unthinkably', 'untimely', 'untouched', 'untrue', 'untrustworthy', 'untruthful', 'unusable', 'unusably', 'unuseable', 'unuseably', 'unusual', 'unusually', 'unviewable', 'unwanted', 'unwarranted', 'unwatchable', 'unwelcome', 'unwell', 'unwieldy', 'unwilling', 'unwillingly', 'unwillingness', 'unwise', 'unwisely', 'unworkable', 'unworthy', 'unyielding', 'upbraid', 'upheaval', 'uprising', 'uproar', 'uproarious', 'uproariously', 'uproarous', 'uproarously', 'uproot', 'upset', 'upseting', 'upsets', 'upsetting', 'upsettingly', 'urgent', 'useless', 'usurp', 'usurper', 'utterly', 'vagrant', 'vague', 'vagueness', 'vain', 'vainly', 'vanity', 'vehement', 'vehemently', 'vengeance', 'vengeful', 'vengefully', 'vengefulness', 'venom', 'venomous', 'venomously', 'vent', 'vestiges', 'vex', 'vexation', 'vexing', 'vexingly', 'vibrate', 'vibrated', 'vibrates', 'vibrating', 'vibration', 'vice', 'vicious', 'viciously', 'viciousness', 'victimize', 'vile', 'vileness', 'vilify', 'villainous', 'villainously', 'villains', 'villian', 'villianous', 'villianously', 'villify', 'vindictive', 'vindictively', 'vindictiveness', 'violate', 'violation', 'violator', 'violators', 'violent', 'violently', 'viper', 'virulence', 'virulent', 'virulently', 'virus', 'vociferous', 'vociferously', 'volatile', 'volatility', 'vomit', 'vomited', 'vomiting', 'vomits', 'vulgar', 'vulnerable', 'wack', 'wail', 'wallow', 'wane', 'waning', 'wanton', 'war-like', 'warily', 'wariness', 'warlike', 'warned', 'warning', 'warp', 'warped', 'wary', 'washed-out', 'waste', 'wasted', 'wasteful', 'wastefulness', 'wasting', 'water-down', 'watered-down', 'wayward', 'weak', 'weaken', 'weakening', 'weaker', 'weakness', 'weaknesses', 'weariness', 'wearisome', 'weary', 'wedge', 'weed', 'weep', 'weird', 'weirdly', 'wheedle', 'whimper', 'whine', 'whining', 'whiny', 'whips', 'whore', 'whores', 'wicked', 'wickedly', 'wickedness', 'wild', 'wildly', 'wiles', 'wilt', 'wily', 'wimpy', 'wince', 'wobble', 'wobbled', 'wobbles', 'woe', 'woebegone', 'woeful', 'woefully', 'womanizer', 'womanizing', 'worn', 'worried', 'worriedly', 'worrier', 'worries', 'worrisome', 'worry', 'worrying', 'worryingly', 'worse', 'worsen', 'worsening', 'worst', 'worthless', 'worthlessly', 'worthlessness', 'wound', 'wounds', 'wrangle', 'wrath', 'wreak', 'wreaked', 'wreaks', 'wreck', 'wrest', 'wrestle', 'wretch', 'wretched', 'wretchedly', 'wretchedness', 'wrinkle', 'wrinkled', 'wrinkles', 'wrip', 'wripped', 'wripping', 'writhe', 'wrong', 'wrongful', 'wrongly', 'wrought', 'yawn', 'zap', 'zapped', 'zaps', 'zealot', 'zealous', 'zealously', 'zombie']\n"
          ]
        }
      ],
      "source": [
        "#Load Dictionary\n",
        "liu_url_neg = 'https://drive.google.com/file/d/1oJjlPwAp5VMt4kSXiJ3a4lM11JPw2pph/view?usp=sharing'\n",
        "liu_path_neg = 'https://drive.google.com/uc?export=download&id='+liu_url_neg.split('/')[-2]\n",
        "\n",
        "liu_url_pos = 'https://drive.google.com/file/d/10PRSFz1NvtUILZ07yu8LpDY-ju-kv3D6/view?usp=sharing'\n",
        "liu_path_pos = 'https://drive.google.com/uc?export=download&id='+liu_url_pos.split('/')[-2]\n",
        "\n",
        "liu_df_temp = pd.read_csv(liu_path_neg, header=None)\n",
        "liu_df_temp['1'] = pd.read_csv(liu_path_pos, header=None)\n",
        "\n",
        "liu_dict_neg = liu_df_temp[0].to_list()\n",
        "liu_dict_pos = liu_df_temp['1'].to_list()\n",
        "\n",
        "print(liu_dict_pos)\n",
        "print(liu_dict_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "ijAZfWMznPGi",
        "outputId": "650a3a42-8d8b-49da-d32f-c177901ab736"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0     1\n",
              "count      4783  2006\n",
              "unique     4783  2006\n",
              "top     2-faced    a+\n",
              "freq          1     1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-60ed013a-b82e-4b03-82f1-c7b463a3dc13\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4783</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>4783</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>2-faced</td>\n",
              "      <td>a+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60ed013a-b82e-4b03-82f1-c7b463a3dc13')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-60ed013a-b82e-4b03-82f1-c7b463a3dc13 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-60ed013a-b82e-4b03-82f1-c7b463a3dc13');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "liu_df_temp.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eecHWfFNnRac",
        "outputId": "4eaf9c08-d6c7-4d11-db68-14e7102eadf4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4783"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "len(liu_dict_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWgmUHSKolbq",
        "outputId": "66decba0-c28e-4a69-e485-b83e2290e531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6789\n"
          ]
        }
      ],
      "source": [
        "print(4783+2006)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6TB7OtkVWWN"
      },
      "source": [
        "####Create Bing Liu Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJLiAeX0UbjE"
      },
      "outputs": [],
      "source": [
        "def liu_label(sentence):\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    neg_count = 0\n",
        "    pos_count = 0\n",
        "    for neg in liu_dict_neg:\n",
        "        if (neg in tokens):\n",
        "            neg_count = neg_count +1\n",
        "    for pos in liu_dict_pos:\n",
        "        if (pos in tokens):\n",
        "            pos_count = pos_count +1\n",
        "    if pos_count > neg_count :\n",
        "        return 'positive'\n",
        "    elif neg_count > pos_count :\n",
        "        return 'negative'\n",
        "    else :\n",
        "        return 'neutral'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rUtJ2Jodg0c"
      },
      "source": [
        "####Calculate Sentiment Score and Create Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJWjtN4Ddmbz"
      },
      "outputs": [],
      "source": [
        "data['liu_label'] = data.text.apply(liu_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FA1p7WKgd5JL",
        "outputId": "62674a9e-6d6c-4a0c-f1be-e2866c867614"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text     label  \\\n",
              "0                                        What  said.   neutral   \n",
              "1   plus you've added commercials to the experien...  positive   \n",
              "2   I didn't today... Must mean I need to take an...   neutral   \n",
              "3   it's really aggressive to blast obnoxious \"en...  negative   \n",
              "4           and it's a really big bad thing about it  negative   \n",
              "\n",
              "   airline_sentiment_confidence  vader_score vader_label  afinn_score  \\\n",
              "0                        1.0000       0.0000     neutral          0.0   \n",
              "1                        0.3486       0.0000     neutral          0.0   \n",
              "2                        0.6837       0.0000     neutral          0.0   \n",
              "3                        1.0000      -0.2716    negative         -5.0   \n",
              "4                        1.0000      -0.5829    negative         -2.0   \n",
              "\n",
              "  afinn_label  swn_score swn_label  liu_score liu_label  \n",
              "0     neutral      0.000   neutral          0   neutral  \n",
              "1     neutral      0.000   neutral          1  positive  \n",
              "2     neutral      0.500  positive          0   neutral  \n",
              "3    negative      0.125  positive          2  positive  \n",
              "4    negative      0.125  positive          1  positive  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-32e7344c-7397-41c8-ad12-b8181b609535\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>vader_score</th>\n",
              "      <th>vader_label</th>\n",
              "      <th>afinn_score</th>\n",
              "      <th>afinn_label</th>\n",
              "      <th>swn_score</th>\n",
              "      <th>swn_label</th>\n",
              "      <th>liu_score</th>\n",
              "      <th>liu_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What  said.</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>plus you've added commercials to the experien...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I didn't today... Must mean I need to take an...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.500</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it's really aggressive to blast obnoxious \"en...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>-0.2716</td>\n",
              "      <td>negative</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.125</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>and it's a really big bad thing about it</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>-0.5829</td>\n",
              "      <td>negative</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.125</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32e7344c-7397-41c8-ad12-b8181b609535')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-32e7344c-7397-41c8-ad12-b8181b609535 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-32e7344c-7397-41c8-ad12-b8181b609535');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_uOzmQDY1hW"
      },
      "source": [
        "###Show Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1rz17av0Y0l9",
        "outputId": "04e97d20-7262-4d55-91f1-95e1c13b31b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0                                        What  said.   \n",
              "1   plus you've added commercials to the experien...   \n",
              "2   I didn't today... Must mean I need to take an...   \n",
              "3   it's really aggressive to blast obnoxious \"en...   \n",
              "4           and it's a really big bad thing about it   \n",
              "\n",
              "   airline_sentiment_confidence  vader_score  afinn_score  swn_score  \\\n",
              "0                        1.0000       0.0000          0.0      0.000   \n",
              "1                        0.3486       0.0000          0.0      0.000   \n",
              "2                        0.6837       0.0000          0.0      0.500   \n",
              "3                        1.0000      -0.2716         -5.0      0.125   \n",
              "4                        1.0000      -0.5829         -2.0      0.125   \n",
              "\n",
              "   liu_score  \n",
              "0          0  \n",
              "1          1  \n",
              "2          0  \n",
              "3          2  \n",
              "4          1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f0d15bb2-5538-4b2c-ba54-2c21e9d4b2f1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>vader_score</th>\n",
              "      <th>afinn_score</th>\n",
              "      <th>swn_score</th>\n",
              "      <th>liu_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What  said.</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>plus you've added commercials to the experien...</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I didn't today... Must mean I need to take an...</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it's really aggressive to blast obnoxious \"en...</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>-0.2716</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>and it's a really big bad thing about it</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>-0.5829</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f0d15bb2-5538-4b2c-ba54-2c21e9d4b2f1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f0d15bb2-5538-4b2c-ba54-2c21e9d4b2f1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f0d15bb2-5538-4b2c-ba54-2c21e9d4b2f1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "data[['text', 'airline_sentiment_confidence', 'vader_score', 'afinn_score', 'swn_score', 'liu_score']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JZi_35y-Zmk-",
        "outputId": "8a4c0c0c-376e-4ee0-9a95-3e8307e770aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text     label vader_label  \\\n",
              "0                                        What  said.   neutral     neutral   \n",
              "1   plus you've added commercials to the experien...  positive     neutral   \n",
              "2   I didn't today... Must mean I need to take an...   neutral     neutral   \n",
              "3   it's really aggressive to blast obnoxious \"en...  negative    negative   \n",
              "4           and it's a really big bad thing about it  negative    negative   \n",
              "\n",
              "  afinn_label swn_label liu_label  \n",
              "0     neutral   neutral   neutral  \n",
              "1     neutral   neutral  positive  \n",
              "2     neutral  positive   neutral  \n",
              "3    negative  positive  positive  \n",
              "4    negative  positive  positive  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95d300bb-81f5-42dc-aafd-0251d8172a0a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>vader_label</th>\n",
              "      <th>afinn_label</th>\n",
              "      <th>swn_label</th>\n",
              "      <th>liu_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What  said.</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>plus you've added commercials to the experien...</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I didn't today... Must mean I need to take an...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it's really aggressive to blast obnoxious \"en...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>and it's a really big bad thing about it</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95d300bb-81f5-42dc-aafd-0251d8172a0a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-95d300bb-81f5-42dc-aafd-0251d8172a0a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-95d300bb-81f5-42dc-aafd-0251d8172a0a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "data[['text', 'label', 'vader_label', 'afinn_label', 'swn_label', 'liu_label']].head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kalimat = 'guys need serious training customer service many better options put way guys handle ur mistakes'\n",
        "\n",
        "list_token_kalimat_tes = nltk.word_tokenize(kalimat)\n",
        "\n",
        "for kata in list_token_kalimat_tes:\n",
        "    print(liu_label(kata))\n",
        "\n",
        "print('skor akhir : ',liu_label(kalimat))\n",
        "\n",
        "# print(vader_score('omg'))\n",
        "# print(vader_score('finally'))\n",
        "# print(vader_label(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyP0GxJbAPDv",
        "outputId": "3cebfade-5670-4cbd-eb42-ac7918aa6846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "positive\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "negative\n",
            "skor akhir :  neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.loc[(data['label']!=data['vader_label']) & (data['label']!=data['afinn_label']) & (data['label']!=data['swn_label'])& (data['label']!=data['liu_label'])]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "JMWxZVSe59ih",
        "outputId": "d0b688bb-8125-4406-9b15-dddb96132a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text     label  \\\n",
              "10     know suicide second leading cause death among ...   neutral   \n",
              "17     flew nyc sfo last week couldnt fully sit seat ...  negative   \n",
              "18                                                flying  positive   \n",
              "24     guys messed seating reserved seating friends g...  negative   \n",
              "27                    miss dont worry well together soon   neutral   \n",
              "...                                                  ...       ...   \n",
              "14620  wait 2+ hrs cs call back flt cld protection am...  negative   \n",
              "14624  call chairman call emerald today call former c...  negative   \n",
              "14629  change flight phone system keeps telling repre...  negative   \n",
              "14632  george doesnt look good please follow link sta...   neutral   \n",
              "14634                                   right cue delays  negative   \n",
              "\n",
              "       airline_sentiment_confidence  vader_score vader_label  afinn_score  \\\n",
              "10                           0.6769      -0.8555    negative         -2.0   \n",
              "17                           1.0000       0.5754    positive          2.0   \n",
              "18                           1.0000       0.0000     neutral          0.0   \n",
              "24                           1.0000       0.6486    positive          0.0   \n",
              "27                           0.6854       0.2037    positive         -5.0   \n",
              "...                             ...          ...         ...          ...   \n",
              "14620                        1.0000       0.0000     neutral          0.0   \n",
              "14624                        1.0000       0.0000     neutral          0.0   \n",
              "14629                        1.0000       0.0000     neutral          0.0   \n",
              "14632                        0.6760      -0.5216    negative          4.0   \n",
              "14634                        0.6684       0.0000     neutral          0.0   \n",
              "\n",
              "      afinn_label  swn_score swn_label  liu_score liu_label  \n",
              "10       negative      0.125  positive          1  positive  \n",
              "17       positive      0.500  positive          0   neutral  \n",
              "18        neutral      0.000   neutral          0   neutral  \n",
              "24        neutral      0.500  positive          0   neutral  \n",
              "27       negative     -0.125  negative          2  positive  \n",
              "...           ...        ...       ...        ...       ...  \n",
              "14620     neutral      0.000   neutral          1  positive  \n",
              "14624     neutral      0.000   neutral          0   neutral  \n",
              "14629     neutral      0.375  positive          0   neutral  \n",
              "14632    positive      0.750  positive         -2  negative  \n",
              "14634     neutral      0.250  positive          0   neutral  \n",
              "\n",
              "[2603 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f97b952e-2f0c-419c-95a6-e4483c656fa1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>vader_score</th>\n",
              "      <th>vader_label</th>\n",
              "      <th>afinn_score</th>\n",
              "      <th>afinn_label</th>\n",
              "      <th>swn_score</th>\n",
              "      <th>swn_label</th>\n",
              "      <th>liu_score</th>\n",
              "      <th>liu_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>know suicide second leading cause death among ...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6769</td>\n",
              "      <td>-0.8555</td>\n",
              "      <td>negative</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.125</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>flew nyc sfo last week couldnt fully sit seat ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.5754</td>\n",
              "      <td>positive</td>\n",
              "      <td>2.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.500</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>flying</td>\n",
              "      <td>positive</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>guys messed seating reserved seating friends g...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.6486</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.500</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>miss dont worry well together soon</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6854</td>\n",
              "      <td>0.2037</td>\n",
              "      <td>positive</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>-0.125</td>\n",
              "      <td>negative</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14620</th>\n",
              "      <td>wait 2+ hrs cs call back flt cld protection am...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14624</th>\n",
              "      <td>call chairman call emerald today call former c...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14629</th>\n",
              "      <td>change flight phone system keeps telling repre...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.375</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14632</th>\n",
              "      <td>george doesnt look good please follow link sta...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6760</td>\n",
              "      <td>-0.5216</td>\n",
              "      <td>negative</td>\n",
              "      <td>4.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.750</td>\n",
              "      <td>positive</td>\n",
              "      <td>-2</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14634</th>\n",
              "      <td>right cue delays</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.6684</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.250</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2603 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f97b952e-2f0c-419c-95a6-e4483c656fa1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f97b952e-2f0c-419c-95a6-e4483c656fa1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f97b952e-2f0c-419c-95a6-e4483c656fa1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6fA3Y9sFbpj"
      },
      "source": [
        "### Lexicon Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlErEJCvDPZE",
        "outputId": "92f6afee-e07d-41be-dabf-ee7735307481"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text     label  \\\n",
              "0                                        What  said.   neutral   \n",
              "1   plus you've added commercials to the experien...  positive   \n",
              "2   I didn't today... Must mean I need to take an...   neutral   \n",
              "3   it's really aggressive to blast obnoxious \"en...  negative   \n",
              "4           and it's a really big bad thing about it  negative   \n",
              "\n",
              "   airline_sentiment_confidence  vader_score vader_label  afinn_score  \\\n",
              "0                        1.0000       0.0000     neutral          0.0   \n",
              "1                        0.3486       0.0000     neutral          0.0   \n",
              "2                        0.6837       0.0000     neutral          0.0   \n",
              "3                        1.0000      -0.2716    negative         -5.0   \n",
              "4                        1.0000      -0.5829    negative         -2.0   \n",
              "\n",
              "  afinn_label  swn_score swn_label  liu_score liu_label  \n",
              "0     neutral      0.000   neutral          0   neutral  \n",
              "1     neutral      0.000   neutral          1  positive  \n",
              "2     neutral      0.500  positive          0   neutral  \n",
              "3    negative      0.125  positive          2  positive  \n",
              "4    negative      0.125  positive          1  positive  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-400bc609-4268-4520-ac34-8d34855906f4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>vader_score</th>\n",
              "      <th>vader_label</th>\n",
              "      <th>afinn_score</th>\n",
              "      <th>afinn_label</th>\n",
              "      <th>swn_score</th>\n",
              "      <th>swn_label</th>\n",
              "      <th>liu_score</th>\n",
              "      <th>liu_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What  said.</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>plus you've added commercials to the experien...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I didn't today... Must mean I need to take an...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.500</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it's really aggressive to blast obnoxious \"en...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>-0.2716</td>\n",
              "      <td>negative</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.125</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>and it's a really big bad thing about it</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>-0.5829</td>\n",
              "      <td>negative</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.125</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-400bc609-4268-4520-ac34-8d34855906f4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-400bc609-4268-4520-ac34-8d34855906f4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-400bc609-4268-4520-ac34-8d34855906f4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et29bm4fJN0X",
        "outputId": "acc13c5a-70c5-489e-f26a-971959831acf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      label  manual  vader  afinn   swn   liu\n",
              "0  negative    9178   5175   4973  5210  3355\n",
              "1   neutral    3099   3302   4074  4263  7211\n",
              "2  positive    2363   6163   5593  5167  4074"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bf3fd814-d73f-4c70-9d64-4713aba95d74\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>manual</th>\n",
              "      <th>vader</th>\n",
              "      <th>afinn</th>\n",
              "      <th>swn</th>\n",
              "      <th>liu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>9178</td>\n",
              "      <td>5175</td>\n",
              "      <td>4973</td>\n",
              "      <td>5210</td>\n",
              "      <td>3355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neutral</td>\n",
              "      <td>3099</td>\n",
              "      <td>3302</td>\n",
              "      <td>4074</td>\n",
              "      <td>4263</td>\n",
              "      <td>7211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>positive</td>\n",
              "      <td>2363</td>\n",
              "      <td>6163</td>\n",
              "      <td>5593</td>\n",
              "      <td>5167</td>\n",
              "      <td>4074</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf3fd814-d73f-4c70-9d64-4713aba95d74')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bf3fd814-d73f-4c70-9d64-4713aba95d74 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bf3fd814-d73f-4c70-9d64-4713aba95d74');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "data_count = data.groupby(['label']).size().to_frame()\n",
        "data_count['vader'] = data.groupby(['vader_label']).size().to_frame()\n",
        "# data_count['textblob'] = data.groupby(['textblob_label']).size().to_frame()\n",
        "data_count['afinn'] = data.groupby(['afinn_label']).size().to_frame()\n",
        "data_count['swn'] = data.groupby(['swn_label']).size().to_frame()\n",
        "data_count['liu'] = data.groupby(['liu_label']).size().to_frame()\n",
        "\n",
        "data_count.head().reset_index().rename(columns={0: 'manual'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhIaMoTBMEru",
        "outputId": "1fb9a8d0-2d7b-4f72-8013-dbbf8d9cf7b2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# import seaborn as sns\n",
        "\n",
        "# cm = confusion_matrix(data['label'], data['vader_label'])\n",
        "# f = sns.heatmap(cm, annot=True)\n",
        "\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(data['label'], data['vader_label'])\n",
        "\n",
        "# print(cf_matrix)\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues',fmt='g')\n",
        "\n",
        "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
        "ax.set_xlabel('\\nPredicted VADER Label')\n",
        "ax.set_ylabel('Manual Labeling');\n",
        "\n",
        "# ## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['Negative','Neutral', 'Positive'])\n",
        "ax.yaxis.set_ticklabels(['Negative','Neutral', 'Positive'])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9ddrW4uFlsv",
        "outputId": "d0c6de5d-ff7e-4bc5-b4a6-f277432e20dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VADER Accuracy :  0.5508196721311476\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.90      0.51      0.65      9178\n",
            "     neutral       0.39      0.42      0.41      3099\n",
            "    positive       0.34      0.89      0.49      2363\n",
            "\n",
            "    accuracy                           0.55     14640\n",
            "   macro avg       0.55      0.61      0.52     14640\n",
            "weighted avg       0.70      0.55      0.57     14640\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vader_accuracy = accuracy_score(data['vader_label'],data['label'])\n",
        "print('VADER Accuracy : ', vader_accuracy)\n",
        "print(classification_report(data['label'],data['vader_label']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwJmzbcoGceK"
      },
      "outputs": [],
      "source": [
        "# textblob_accuracy = accuracy_score(data['label'],data['textblob_label'])\n",
        "# print('TextBlob Accuracy : ', textblob_accuracy)\n",
        "# print(classification_report(data['label'],data['textblob_label']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IZ3vl28IoM-",
        "outputId": "dc510752-9b3b-4918-d465-2a101d34c432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AFINN111 Accuracy :  0.5433060109289618\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.90      0.49      0.63      9178\n",
            "     neutral       0.37      0.49      0.42      3099\n",
            "    positive       0.36      0.84      0.50      2363\n",
            "\n",
            "    accuracy                           0.54     14640\n",
            "   macro avg       0.54      0.60      0.52     14640\n",
            "weighted avg       0.70      0.54      0.56     14640\n",
            "\n"
          ]
        }
      ],
      "source": [
        "afinn_accuracy = accuracy_score(data['label'],data['afinn_label'])\n",
        "print('AFINN111 Accuracy : ', afinn_accuracy)\n",
        "print(classification_report(data['label'],data['afinn_label']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxp9sGshOMO0",
        "outputId": "c60ebc18-8be2-419a-f0b3-bb7d33b4c739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SWN Accuracy :  0.47452185792349727\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.79      0.45      0.57      9178\n",
            "     neutral       0.33      0.45      0.38      3099\n",
            "    positive       0.27      0.60      0.38      2363\n",
            "\n",
            "    accuracy                           0.47     14640\n",
            "   macro avg       0.47      0.50      0.44     14640\n",
            "weighted avg       0.61      0.47      0.50     14640\n",
            "\n"
          ]
        }
      ],
      "source": [
        "swn_accuracy = accuracy_score(data['label'],data['swn_label'])\n",
        "print('SWN Accuracy : ', swn_accuracy)\n",
        "print(classification_report(data['label'],data['swn_label']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk7okGpKeB-D",
        "outputId": "16b36950-fcaf-41f2-ef68-3f9b7841af08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liu Accuracy :  0.25594262295081965\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.43      0.16      0.23      9178\n",
            "     neutral       0.30      0.70      0.42      3099\n",
            "    positive       0.03      0.06      0.04      2363\n",
            "\n",
            "    accuracy                           0.26     14640\n",
            "   macro avg       0.26      0.30      0.23     14640\n",
            "weighted avg       0.34      0.26      0.24     14640\n",
            "\n"
          ]
        }
      ],
      "source": [
        "liu_accuracy = accuracy_score(data['label'],data['liu_label'])\n",
        "print('Liu Accuracy : ', liu_accuracy)\n",
        "print(classification_report(data['label'],data['liu_label']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtiDjtk78IOp",
        "outputId": "1ac73e43-527c-43dc-e24c-17eb12ea3556"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "bar_text = ['VADER', 'AFINN-111', 'SentiWordNet', 'Bing Liu']\n",
        "bar_value = [vader_accuracy, afinn_accuracy, swn_accuracy, liu_accuracy]\n",
        "plt.bar(bar_text, bar_value)\n",
        "plt.title('Hasil Lexical Labelling')\n",
        "plt.xlabel('Lexical Resource')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.axis([None, None, 0, 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMW5J9oX3nU9"
      },
      "source": [
        "###Export CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v4u6q8dGx4D"
      },
      "outputs": [],
      "source": [
        "data.to_csv('labelled-fixed.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaYQHxeSxkSS"
      },
      "source": [
        "##4. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('labelled-fixed.csv')"
      ],
      "metadata": {
        "id": "X8iN0cRImurw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM3yTjAHxkSS",
        "outputId": "1bb63855-d42d-4126-91ab-1cbecd6291c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text     label  \\\n",
              "0                                               said   neutral   \n",
              "1       plus youve added commercials eperience tacky  positive   \n",
              "2       didnt today must mean need take another trip   neutral   \n",
              "3  really aggressive blast obnoious entertainment...  negative   \n",
              "4                               really big bad thing  negative   \n",
              "\n",
              "   airline_sentiment_confidence  vader_score vader_label  afinn_score  \\\n",
              "0                        1.0000       0.0000     neutral          0.0   \n",
              "1                        0.3486       0.0000     neutral          0.0   \n",
              "2                        0.6837       0.0000     neutral          0.0   \n",
              "3                        1.0000      -0.2716    negative         -5.0   \n",
              "4                        1.0000      -0.5829    negative         -2.0   \n",
              "\n",
              "  afinn_label  swn_score swn_label  liu_score liu_label  \n",
              "0     neutral      0.000   neutral          0   neutral  \n",
              "1     neutral      0.000   neutral          1  positive  \n",
              "2     neutral      0.500  positive          0   neutral  \n",
              "3    negative      0.125  positive          2  positive  \n",
              "4    negative      0.125  positive          1  positive  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a18b324f-c7b4-4808-8510-c7350496d166\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>vader_score</th>\n",
              "      <th>vader_label</th>\n",
              "      <th>afinn_score</th>\n",
              "      <th>afinn_label</th>\n",
              "      <th>swn_score</th>\n",
              "      <th>swn_label</th>\n",
              "      <th>liu_score</th>\n",
              "      <th>liu_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>said</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>plus youve added commercials eperience tacky</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>didnt today must mean need take another trip</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.500</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>really aggressive blast obnoious entertainment...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>-0.2716</td>\n",
              "      <td>negative</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.125</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>really big bad thing</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>-0.5829</td>\n",
              "      <td>negative</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.125</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a18b324f-c7b4-4808-8510-c7350496d166')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a18b324f-c7b4-4808-8510-c7350496d166 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a18b324f-c7b4-4808-8510-c7350496d166');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "#2nd method preprocessing\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "    text = text.replace('x', '')\n",
        "#    text = re.sub(r'\\W+', '', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "    return text\n",
        "\n",
        "data['text'] = data['text'].apply(clean_text)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.loc[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpT66APHGZD6",
        "outputId": "567803b6-92fa-4852-f827-bc18aa0f128e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text                            really aggressive blast obnoious entertainment...\n",
              "label                                                                    negative\n",
              "airline_sentiment_confidence                                                  1.0\n",
              "vader_score                                                               -0.2716\n",
              "vader_label                                                              negative\n",
              "afinn_score                                                                  -5.0\n",
              "afinn_label                                                              negative\n",
              "swn_score                                                                   0.125\n",
              "swn_label                                                                positive\n",
              "liu_score                                                                       2\n",
              "liu_label                                                                positive\n",
              "Name: 3, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgC_MlKxxkSS"
      },
      "outputs": [],
      "source": [
        "data.to_csv('labelled-preprocessed.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FDtCVdxxkSR"
      },
      "source": [
        "##5. Embedding & Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFh5pnF4xkSS"
      },
      "source": [
        "###Select and Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ORjVkB8xkST"
      },
      "outputs": [],
      "source": [
        "data_clean = data[['text','label','vader_label','afinn_label','swn_label','liu_label']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7ITWao-xkST",
        "outputId": "397e6e70-d689-4d67-8d6d-151dd69f094b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text     label vader_label  \\\n",
              "0                                               said   neutral     neutral   \n",
              "1       plus youve added commercials eperience tacky  positive     neutral   \n",
              "2       didnt today must mean need take another trip   neutral     neutral   \n",
              "3  really aggressive blast obnoious entertainment...  negative    negative   \n",
              "4                               really big bad thing  negative    negative   \n",
              "\n",
              "  afinn_label swn_label liu_label  \n",
              "0     neutral   neutral   neutral  \n",
              "1     neutral   neutral  positive  \n",
              "2     neutral  positive   neutral  \n",
              "3    negative  positive  positive  \n",
              "4    negative  positive  positive  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ccba066f-d7b7-4e6e-88d9-0bd8fe068770\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>vader_label</th>\n",
              "      <th>afinn_label</th>\n",
              "      <th>swn_label</th>\n",
              "      <th>liu_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>said</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>plus youve added commercials eperience tacky</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>didnt today must mean need take another trip</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>really aggressive blast obnoious entertainment...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>really big bad thing</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ccba066f-d7b7-4e6e-88d9-0bd8fe068770')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ccba066f-d7b7-4e6e-88d9-0bd8fe068770 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ccba066f-d7b7-4e6e-88d9-0bd8fe068770');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "data_clean.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9lDCwO2xkST"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXl-UIjExkST"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(data_clean, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f-1LTRI-OyN",
        "outputId": "0f7b21f1-4769-4273-a0f3-f7b23bece3d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11712, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW1euR-5xkSU",
        "outputId": "3b2bb2a6-5d2e-401f-b33e-b79aa71dae4c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2928, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "test_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv('train_df.csv')\n",
        "test_df.to_csv('test_df.csv')"
      ],
      "metadata": {
        "id": "3ZHuvwMxi6Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8pMPa38xkSV"
      },
      "source": [
        "###Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoR9wQqzxkSV"
      },
      "outputs": [],
      "source": [
        "# Number of labels: positive, negative, neutral\n",
        "num_classes = 3\n",
        "\n",
        "# Number of dimensions for word embedding\n",
        "embed_num_dims = 300\n",
        "\n",
        "#category\n",
        "class_names = ['negative', 'neutral', 'positive']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIE_58VNxkSV"
      },
      "outputs": [],
      "source": [
        "#text split\n",
        "X_train = train_df.text\n",
        "X_test = test_df.text\n",
        "\n",
        "#label split\n",
        "y_train_manual = train_df.label\n",
        "y_test_manual = test_df.label\n",
        "\n",
        "y_train_vader = train_df.vader_label\n",
        "y_test_vader = test_df.vader_label\n",
        "\n",
        "# y_train_textblob = train_df.textblob_label\n",
        "# y_test_textblob = test_df.textblob_label\n",
        "\n",
        "y_train_afinn = train_df.afinn_label\n",
        "y_test_afinn = test_df.afinn_label\n",
        "\n",
        "y_train_swn = train_df.swn_label\n",
        "y_test_swn = test_df.swn_label\n",
        "\n",
        "y_train_liu = train_df.liu_label\n",
        "y_test_liu = test_df.liu_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xvMm3VVxkSW"
      },
      "outputs": [],
      "source": [
        "encoding = {\n",
        "    'negative': 0,\n",
        "    'neutral': 1,\n",
        "    'positive': 2\n",
        "}\n",
        "\n",
        "# Integer labels\n",
        "y_train_manual = [encoding[x] for x in train_df.label]\n",
        "y_test_manual = [encoding[x] for x in test_df.label]\n",
        "\n",
        "y_train_vader = [encoding[x] for x in train_df.vader_label]\n",
        "y_test_vader = [encoding[x] for x in test_df.vader_label]\n",
        "\n",
        "# y_train_textblob = [encoding[x] for x in train_df.textblob_label]\n",
        "# y_test_textblob = [encoding[x] for x in test_df.textblob_label]\n",
        "\n",
        "y_train_afinn = [encoding[x] for x in train_df.afinn_label]\n",
        "y_test_afinn = [encoding[x] for x in test_df.afinn_label]\n",
        "\n",
        "y_train_swn = [encoding[x] for x in train_df.swn_label]\n",
        "y_test_swn = [encoding[x] for x in test_df.swn_label]\n",
        "\n",
        "y_train_liu = [encoding[x] for x in train_df.liu_label]\n",
        "y_test_liu = [encoding[x] for x in test_df.liu_label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaJSn8eBxkSW",
        "outputId": "e446a5c0-1149-4324-8d9e-0221608a2ef4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       ...,\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_manual = to_categorical(y_train_manual)\n",
        "y_test_manual = to_categorical(y_test_manual)\n",
        "\n",
        "y_train_vader = to_categorical(y_train_vader)\n",
        "y_test_vader = to_categorical(y_test_vader)\n",
        "\n",
        "# y_train_textblob = to_categorical(y_train_textblob)\n",
        "# y_test_textblob = to_categorical(y_test_textblob)\n",
        "\n",
        "y_train_afinn = to_categorical(y_train_afinn)\n",
        "y_test_afinn = to_categorical(y_test_afinn)\n",
        "\n",
        "y_train_swn = to_categorical(y_train_swn)\n",
        "y_test_swn = to_categorical(y_test_swn)\n",
        "\n",
        "y_train_liu = to_categorical(y_train_liu)\n",
        "y_test_liu = to_categorical(y_test_liu)\n",
        "\n",
        "y_train_manual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHRdrP7nxkSX"
      },
      "source": [
        "###Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpGVnIqaxkSX"
      },
      "outputs": [],
      "source": [
        "texts = data_clean.text.tolist()\n",
        "\n",
        "texts_train = X_train.tolist()\n",
        "texts_test = X_test.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3XG8WGQxkSX",
        "outputId": "cc3eca64-1e6c-4b06-df6d-c77e6bfdd9dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words: 15340\n"
          ]
        }
      ],
      "source": [
        "#tokenization + fitting\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data.text)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data.text)\n",
        "sequence_train = tokenizer.texts_to_sequences(texts_train)\n",
        "sequence_test = tokenizer.texts_to_sequences(texts_test)\n",
        "\n",
        "index_of_words = tokenizer.word_index\n",
        "\n",
        "# vacab size is number of unique words + reserved 0 index for padding\n",
        "vocab_size = len(tokenizer.word_counts)+1\n",
        "\n",
        "print('Number of unique words: {}'.format(vocab_size))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data = tokenizer.fit_on_texts('really aggressive blast obnoious entertainment guests faces amp little recourse')\n",
        "list_angka = []\n",
        "\n",
        "from nltk.tokenize import word_tokenize \n",
        " \n",
        "kalimat = \"complete lack faith companies really shame think telling challenging.\"\n",
        "teks = nltk.tokenize.word_tokenize(kalimat)\n",
        "\n",
        "for kata in teks:\n",
        "  # print(kata)\n",
        "  if kata in index_of_words:\n",
        "    list_angka.append(index_of_words[kata])\n",
        "\n",
        "print(kalimat)\n",
        "print(list_angka)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxMwqCrQGy6w",
        "outputId": "e395d3aa-4972-4865-9bf2-a0d60d1e7b18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "complete lack faith companies really shame think telling challenging.\n",
            "[743, 459, 3032, 2911, 63, 1270, 155, 446, 4290]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if list_angka in sequence_test:\n",
        "  print(sequence_test.index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5mCxP8WKr6j",
        "outputId": "f70cb6bb-6812-4bbf-a11c-cfee49a99ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<built-in method index of list object at 0x7fe135f203c0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_test.index(list_angka)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BCfQR5lLTP6",
        "outputId": "9d6f255a-af83-4689-b4f0-04c275bc0611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_test[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk-CrCf6LIj7",
        "outputId": "579bbe44-b5cc-4431-de31-47427d699bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[743, 459, 3032, 2911, 63, 1270, 155, 446, 4290]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91FqEiyzxkSY"
      },
      "source": [
        "###Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAwCW1P2xkSY",
        "outputId": "44f4534d-89ac-4135-dd31-34f3bfea6f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean nmber of tokens: 9.55717213114754\n",
            "std of tokens: 4.137379147577368\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'fraction of reviews')"
            ]
          },
          "metadata": {},
          "execution_count": 59
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_tokens = [len(tokens) for tokens in sequences]\n",
        "num_tokens = np.array(num_tokens)\n",
        "sn.distplot( num_tokens )\n",
        "plt.grid(True)\n",
        "\n",
        "mean_num_tokens = num_tokens.mean()\n",
        "std_num_tokens = num_tokens.std()\n",
        "print(\"mean nmber of tokens: {}\".format(mean_num_tokens))\n",
        "print(\"std of tokens: {}\".format(std_num_tokens))\n",
        "plt.xlabel('number of tokens')\n",
        "plt.ylabel('fraction of reviews')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMKrTywYxkSY",
        "outputId": "05bfebdc-793e-48d5-981e-6923c26538a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max length taken: 19\n",
            "untrimmed: 0.9930327868852459\n"
          ]
        }
      ],
      "source": [
        "# Max input length (max number of words) \n",
        "# max_seq_len = 500\n",
        "\n",
        "max_seq_len = int(mean_num_tokens + 2.5 * std_num_tokens)\n",
        "print(\"max length taken: {}\".format(max_seq_len))\n",
        "print(\"untrimmed: {}\".format(np.sum(num_tokens < max_seq_len) / len(num_tokens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN8oIaPlxkSY",
        "outputId": "a667c489-93be-4e44-f7a3-54b3131cc67e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,  226,  185,  991],\n",
              "       [   0,    0,    0, ...,  266,   42,   42],\n",
              "       [   0,    0,    0, ..., 1118,  149, 7984],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,  491,  123,  315],\n",
              "       [   0,    0,    0, ...,   93, 1122, 4493],\n",
              "       [   0,    0,    0, ...,  105,  126,  171]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "#padding\n",
        "\n",
        "X_train_pad = pad_sequences(sequence_train, maxlen = max_seq_len )\n",
        "X_test_pad = pad_sequences(sequence_test, maxlen = max_seq_len )\n",
        "\n",
        "X_train_pad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequence_test[0])\n",
        "print(X_test_pad[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXcnID3jstFR",
        "outputId": "d2f9e44f-7ef8-4302-d38b-42104e865bd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[743, 459, 3032, 2911, 63, 1270, 155, 446, 4290]\n",
            "[   0    0    0    0    0    0    0    0    0    0  743  459 3032 2911\n",
            "   63 1270  155  446 4290]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pad[10160]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlxh2xt6KWpj",
        "outputId": "fd23d11a-da25-4a30-ff3b-382400a9a7ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,   63, 3496,\n",
              "       4389, 4390,  874, 2943, 3497,   16,  391, 2564], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkF_M7bGxkSZ",
        "outputId": "f9cb8a2d-e2ce-4ccc-a20e-2671facafa80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2928, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "X_test_pad.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_pad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R_PxNq-kIfy",
        "outputId": "9f411690-7828-492c-f908-60c52112b1c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    0,     0,     0, ...,   155,   446,  4290],\n",
              "       [    0,     0,     0, ...,     3,  7007,     5],\n",
              "       [    0,     0,     0, ...,   140,  1265,   167],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,     4,     3, 13978],\n",
              "       [    0,     0,     0, ...,   496,   882,   495],\n",
              "       [    0,     0,     0, ...,  2365,  6675,   191]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pad_df = pd.DataFrame(X_train_pad)\n",
        "X_train_pad_df.to_csv('X_train_pad.csv', index=False, header=False)\n",
        "\n",
        "X_test_pad_df = pd.DataFrame(X_test_pad)\n",
        "X_test_pad_df.to_csv('X_test_pad.csv', index=False, header=False)"
      ],
      "metadata": {
        "id": "yD3cP15qm3bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcU8y_zyxkSZ"
      },
      "source": [
        "###Pretrained Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rzo4e3QnxkSZ"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "    return embedding_matrix\n",
        "\n",
        "fname = 'embeddings/wiki-news-300d-1M.vec'\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "    print('Downloading word vectors...')\n",
        "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip',\n",
        "                              'wiki-news-300d-1M.vec.zip')\n",
        "    print('Unzipping...')\n",
        "    with zipfile.ZipFile('wiki-news-300d-1M.vec.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('embeddings')\n",
        "    print('done.')\n",
        "    \n",
        "    os.remove('wiki-news-300d-1M.vec.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5PQNQqlxkSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d185a5-f5ba-4a76-f762-e810484b9c54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15340, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "embedd_matrix = create_embedding_matrix(fname, index_of_words, embed_num_dims)\n",
        "embedd_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COISh0DwxkSa",
        "outputId": "8f302955-e424-4a14-d314-7d335003c2e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words found in wiki vocab: 10092\n",
            "New words found: 5247\n"
          ]
        }
      ],
      "source": [
        "# Inspect unseen words\n",
        "new_words = 0\n",
        "\n",
        "for word in index_of_words:\n",
        "    entry = embedd_matrix[index_of_words[word]]\n",
        "    if all(v == 0 for v in entry):\n",
        "        new_words = new_words + 1\n",
        "\n",
        "print('Words found in wiki vocab: ' + str(len(index_of_words) - new_words))\n",
        "print('New words found: ' + str(new_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3DMijrpxkSb"
      },
      "outputs": [],
      "source": [
        "# Embedding layer before the actaul LSTM \n",
        "embedd_layer = Embedding(vocab_size,\n",
        "                         embed_num_dims,\n",
        "                         input_length = max_seq_len,\n",
        "                         weights = [embedd_matrix],\n",
        "                         trainable=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUGKoC21xyku"
      },
      "source": [
        "###Modelling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 200"
      ],
      "metadata": {
        "id": "qRs0iTHRszyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or5-fgemxyku",
        "outputId": "54f23047-ec5c-4f86-8196-0c1fef9a5945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 19, 300)           4602000   \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 19, 8)             9888      \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 19, 8)             544       \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 19, 8)             544       \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 8)                0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                144       \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,613,171\n",
            "Trainable params: 11,171\n",
            "Non-trainable params: 4,602,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#model\n",
        "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import *\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "mc_manual = ModelCheckpoint('best_manual.h5', monitor='val_accuracy', mode='max', save_best_only=True,verbose=1) \n",
        "mc_vader = ModelCheckpoint('best_vader.h5', monitor='val_accuracy', mode='max', save_best_only=True,verbose=1) \n",
        "mc_afinn = ModelCheckpoint('best_afinn.h5', monitor='val_accuracy', mode='max', save_best_only=True,verbose=1) \n",
        "mc_swn = ModelCheckpoint('best_swn.h5', monitor='val_accuracy', mode='max', save_best_only=True,verbose=1) \n",
        "mc_liu = ModelCheckpoint('best_liu.h5', monitor='val_accuracy', mode='max', save_best_only=True,verbose=1) \n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedd_layer)\n",
        "model.add(LSTM(8, return_sequences=True, dropout=0.5))\n",
        "model.add(LSTM(8, return_sequences=True, dropout=0.5))\n",
        "model.add(LSTM(8, return_sequences=True, dropout=0.5))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6jMDcS3rzLI"
      },
      "outputs": [],
      "source": [
        "# #model\n",
        "# from tensorflow.keras.layers import GlobalMaxPooling1D\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# from keras.callbacks import *\n",
        "\n",
        "# batch_size = 32\n",
        "# epochs = 50\n",
        "\n",
        "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "# mc=ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', save_best_only=True,verbose=1) \n",
        "\n",
        "# model=Sequential()\n",
        "\n",
        "# #lstm layer\n",
        "# model.add(embedd_layer)\n",
        "\n",
        "# model.add(LSTM(8,return_sequences=True,dropout=0.3))\n",
        "\n",
        "# #Global Maxpooling\n",
        "# model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# #Dense Layer\n",
        "# model.add(Dense(16, activation='relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "# model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0s8ch3MJxykv"
      },
      "outputs": [],
      "source": [
        "model_manual = model\n",
        "model_vader = model\n",
        "model_afinn = model\n",
        "model_swn = model\n",
        "model_liu = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgoXiipMxykv",
        "outputId": "5fa83406-eeaf-40e6-9b14-606c6444b97a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8964 - accuracy: 0.6233\n",
            "Epoch 1: val_accuracy improved from -inf to 0.65273, saving model to best_manual.h5\n",
            "330/330 [==============================] - 20s 35ms/step - loss: 0.8965 - accuracy: 0.6233 - val_loss: 0.7605 - val_accuracy: 0.6527\n",
            "Epoch 2/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7726 - accuracy: 0.6651\n",
            "Epoch 2: val_accuracy improved from 0.65273 to 0.68942, saving model to best_manual.h5\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.7724 - accuracy: 0.6651 - val_loss: 0.7037 - val_accuracy: 0.6894\n",
            "Epoch 3/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7255 - accuracy: 0.7009\n",
            "Epoch 3: val_accuracy improved from 0.68942 to 0.74061, saving model to best_manual.h5\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.7255 - accuracy: 0.7009 - val_loss: 0.6652 - val_accuracy: 0.7406\n",
            "Epoch 4/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7088 - accuracy: 0.7076\n",
            "Epoch 4: val_accuracy improved from 0.74061 to 0.75000, saving model to best_manual.h5\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.7087 - accuracy: 0.7078 - val_loss: 0.6535 - val_accuracy: 0.7500\n",
            "Epoch 5/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6936 - accuracy: 0.7171\n",
            "Epoch 5: val_accuracy improved from 0.75000 to 0.75341, saving model to best_manual.h5\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6936 - accuracy: 0.7171 - val_loss: 0.6499 - val_accuracy: 0.7534\n",
            "Epoch 6/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6805 - accuracy: 0.7185\n",
            "Epoch 6: val_accuracy did not improve from 0.75341\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6804 - accuracy: 0.7186 - val_loss: 0.6265 - val_accuracy: 0.7534\n",
            "Epoch 7/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6743 - accuracy: 0.7181\n",
            "Epoch 7: val_accuracy improved from 0.75341 to 0.76280, saving model to best_manual.h5\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6744 - accuracy: 0.7179 - val_loss: 0.6176 - val_accuracy: 0.7628\n",
            "Epoch 8/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6674 - accuracy: 0.7208\n",
            "Epoch 8: val_accuracy did not improve from 0.76280\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6674 - accuracy: 0.7208 - val_loss: 0.6176 - val_accuracy: 0.7560\n",
            "Epoch 9/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6612 - accuracy: 0.7281\n",
            "Epoch 9: val_accuracy did not improve from 0.76280\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6612 - accuracy: 0.7281 - val_loss: 0.6122 - val_accuracy: 0.7585\n",
            "Epoch 10/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6597 - accuracy: 0.7267\n",
            "Epoch 10: val_accuracy did not improve from 0.76280\n",
            "330/330 [==============================] - 10s 30ms/step - loss: 0.6596 - accuracy: 0.7267 - val_loss: 0.6236 - val_accuracy: 0.7449\n",
            "Epoch 11/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6531 - accuracy: 0.7316\n",
            "Epoch 11: val_accuracy improved from 0.76280 to 0.76621, saving model to best_manual.h5\n",
            "330/330 [==============================] - 10s 30ms/step - loss: 0.6531 - accuracy: 0.7316 - val_loss: 0.6100 - val_accuracy: 0.7662\n",
            "Epoch 12/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6397 - accuracy: 0.7394\n",
            "Epoch 12: val_accuracy improved from 0.76621 to 0.76962, saving model to best_manual.h5\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6397 - accuracy: 0.7395 - val_loss: 0.5965 - val_accuracy: 0.7696\n",
            "Epoch 13/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6568 - accuracy: 0.7275\n",
            "Epoch 13: val_accuracy did not improve from 0.76962\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6570 - accuracy: 0.7272 - val_loss: 0.6026 - val_accuracy: 0.7602\n",
            "Epoch 14/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6359 - accuracy: 0.7364\n",
            "Epoch 14: val_accuracy did not improve from 0.76962\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6362 - accuracy: 0.7362 - val_loss: 0.6144 - val_accuracy: 0.7551\n",
            "Epoch 15/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6373 - accuracy: 0.7371\n",
            "Epoch 15: val_accuracy did not improve from 0.76962\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6373 - accuracy: 0.7370 - val_loss: 0.5896 - val_accuracy: 0.7662\n",
            "Epoch 16/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6330 - accuracy: 0.7382\n",
            "Epoch 16: val_accuracy did not improve from 0.76962\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6330 - accuracy: 0.7382 - val_loss: 0.5873 - val_accuracy: 0.7585\n",
            "Epoch 17/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6276 - accuracy: 0.7399\n",
            "Epoch 17: val_accuracy did not improve from 0.76962\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6277 - accuracy: 0.7399 - val_loss: 0.5870 - val_accuracy: 0.7611\n",
            "Epoch 18/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6397 - accuracy: 0.7357\n",
            "Epoch 18: val_accuracy did not improve from 0.76962\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6397 - accuracy: 0.7357 - val_loss: 0.5882 - val_accuracy: 0.7611\n",
            "Epoch 19/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6343 - accuracy: 0.7382\n",
            "Epoch 19: val_accuracy improved from 0.76962 to 0.77048, saving model to best_manual.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6344 - accuracy: 0.7381 - val_loss: 0.5838 - val_accuracy: 0.7705\n",
            "Epoch 20/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6277 - accuracy: 0.7479\n",
            "Epoch 20: val_accuracy did not improve from 0.77048\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6279 - accuracy: 0.7478 - val_loss: 0.5915 - val_accuracy: 0.7637\n",
            "Epoch 21/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6300 - accuracy: 0.7442\n",
            "Epoch 21: val_accuracy did not improve from 0.77048\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6300 - accuracy: 0.7442 - val_loss: 0.5872 - val_accuracy: 0.7654\n",
            "Epoch 22/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6098 - accuracy: 0.7505\n",
            "Epoch 22: val_accuracy did not improve from 0.77048\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6101 - accuracy: 0.7504 - val_loss: 0.5789 - val_accuracy: 0.7671\n",
            "Epoch 23/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6150 - accuracy: 0.7451\n",
            "Epoch 23: val_accuracy did not improve from 0.77048\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6150 - accuracy: 0.7451 - val_loss: 0.5775 - val_accuracy: 0.7696\n",
            "Epoch 24/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6192 - accuracy: 0.7481\n",
            "Epoch 24: val_accuracy did not improve from 0.77048\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6192 - accuracy: 0.7481 - val_loss: 0.5861 - val_accuracy: 0.7602\n",
            "Epoch 25/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6132 - accuracy: 0.7492\n",
            "Epoch 25: val_accuracy did not improve from 0.77048\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6131 - accuracy: 0.7494 - val_loss: 0.5802 - val_accuracy: 0.7637\n",
            "Epoch 26/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6043 - accuracy: 0.7530\n",
            "Epoch 26: val_accuracy did not improve from 0.77048\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6040 - accuracy: 0.7533 - val_loss: 0.5799 - val_accuracy: 0.7637\n",
            "Epoch 27/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6040 - accuracy: 0.7565\n",
            "Epoch 27: val_accuracy improved from 0.77048 to 0.77389, saving model to best_manual.h5\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6041 - accuracy: 0.7565 - val_loss: 0.5760 - val_accuracy: 0.7739\n",
            "Epoch 28/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6145 - accuracy: 0.7524\n",
            "Epoch 28: val_accuracy improved from 0.77389 to 0.77560, saving model to best_manual.h5\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6143 - accuracy: 0.7526 - val_loss: 0.5717 - val_accuracy: 0.7756\n",
            "Epoch 29/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6110 - accuracy: 0.7527\n",
            "Epoch 29: val_accuracy did not improve from 0.77560\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6110 - accuracy: 0.7527 - val_loss: 0.5724 - val_accuracy: 0.7688\n",
            "Epoch 30/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6096 - accuracy: 0.7519\n",
            "Epoch 30: val_accuracy did not improve from 0.77560\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6099 - accuracy: 0.7518 - val_loss: 0.5680 - val_accuracy: 0.7696\n",
            "Epoch 31/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6041 - accuracy: 0.7548\n",
            "Epoch 31: val_accuracy did not improve from 0.77560\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6041 - accuracy: 0.7548 - val_loss: 0.5658 - val_accuracy: 0.7747\n",
            "Epoch 32/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6060 - accuracy: 0.7558\n",
            "Epoch 32: val_accuracy did not improve from 0.77560\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6061 - accuracy: 0.7558 - val_loss: 0.5670 - val_accuracy: 0.7756\n",
            "Epoch 33/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6073 - accuracy: 0.7552\n",
            "Epoch 33: val_accuracy did not improve from 0.77560\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6073 - accuracy: 0.7552 - val_loss: 0.5923 - val_accuracy: 0.7688\n",
            "Epoch 34/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6033 - accuracy: 0.7569\n",
            "Epoch 34: val_accuracy did not improve from 0.77560\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6037 - accuracy: 0.7566 - val_loss: 0.5658 - val_accuracy: 0.7739\n",
            "Epoch 35/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5999 - accuracy: 0.7542\n",
            "Epoch 35: val_accuracy did not improve from 0.77560\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.5996 - accuracy: 0.7544 - val_loss: 0.5645 - val_accuracy: 0.7747\n",
            "Epoch 36/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6017 - accuracy: 0.7542\n",
            "Epoch 36: val_accuracy did not improve from 0.77560\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6017 - accuracy: 0.7542 - val_loss: 0.5665 - val_accuracy: 0.7688\n",
            "Epoch 37/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5997 - accuracy: 0.7546\n",
            "Epoch 37: val_accuracy improved from 0.77560 to 0.77730, saving model to best_manual.h5\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5997 - accuracy: 0.7546 - val_loss: 0.5618 - val_accuracy: 0.7773\n",
            "Epoch 38/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5975 - accuracy: 0.7572\n",
            "Epoch 38: val_accuracy did not improve from 0.77730\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5980 - accuracy: 0.7572 - val_loss: 0.5641 - val_accuracy: 0.7739\n",
            "Epoch 39/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5945 - accuracy: 0.7594\n",
            "Epoch 39: val_accuracy did not improve from 0.77730\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5945 - accuracy: 0.7594 - val_loss: 0.5667 - val_accuracy: 0.7773\n",
            "Epoch 40/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6006 - accuracy: 0.7599\n",
            "Epoch 40: val_accuracy did not improve from 0.77730\n",
            "330/330 [==============================] - 15s 46ms/step - loss: 0.6006 - accuracy: 0.7599 - val_loss: 0.5665 - val_accuracy: 0.7765\n",
            "Epoch 41/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5964 - accuracy: 0.7601\n",
            "Epoch 41: val_accuracy did not improve from 0.77730\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5964 - accuracy: 0.7601 - val_loss: 0.5727 - val_accuracy: 0.7654\n",
            "Epoch 42/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5973 - accuracy: 0.7598\n",
            "Epoch 42: val_accuracy did not improve from 0.77730\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.5972 - accuracy: 0.7598 - val_loss: 0.5642 - val_accuracy: 0.7739\n",
            "Epoch 43/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5936 - accuracy: 0.7566\n",
            "Epoch 43: val_accuracy did not improve from 0.77730\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.5933 - accuracy: 0.7568 - val_loss: 0.5756 - val_accuracy: 0.7722\n",
            "Epoch 44/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5938 - accuracy: 0.7565\n",
            "Epoch 44: val_accuracy did not improve from 0.77730\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5938 - accuracy: 0.7565 - val_loss: 0.5668 - val_accuracy: 0.7739\n",
            "Epoch 45/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5794 - accuracy: 0.7689\n",
            "Epoch 45: val_accuracy did not improve from 0.77730\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.5794 - accuracy: 0.7689 - val_loss: 0.5679 - val_accuracy: 0.7705\n",
            "Epoch 46/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5879 - accuracy: 0.7603\n",
            "Epoch 46: val_accuracy did not improve from 0.77730\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.5879 - accuracy: 0.7602 - val_loss: 0.5646 - val_accuracy: 0.7765\n",
            "Epoch 47/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5880 - accuracy: 0.7642\n",
            "Epoch 47: val_accuracy did not improve from 0.77730\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5879 - accuracy: 0.7644 - val_loss: 0.5648 - val_accuracy: 0.7713\n",
            "Epoch 48/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5920 - accuracy: 0.7600\n",
            "Epoch 48: val_accuracy did not improve from 0.77730\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.5920 - accuracy: 0.7600 - val_loss: 0.5752 - val_accuracy: 0.7696\n",
            "Epoch 49/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5946 - accuracy: 0.7614\n",
            "Epoch 49: val_accuracy improved from 0.77730 to 0.77901, saving model to best_manual.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5943 - accuracy: 0.7615 - val_loss: 0.5721 - val_accuracy: 0.7790\n",
            "Epoch 50/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5824 - accuracy: 0.7647\n",
            "Epoch 50: val_accuracy did not improve from 0.77901\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.5824 - accuracy: 0.7647 - val_loss: 0.5774 - val_accuracy: 0.7756\n",
            "Epoch 51/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5807 - accuracy: 0.7664\n",
            "Epoch 51: val_accuracy did not improve from 0.77901\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.5807 - accuracy: 0.7664 - val_loss: 0.5818 - val_accuracy: 0.7782\n",
            "Epoch 52/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5837 - accuracy: 0.7634\n",
            "Epoch 52: val_accuracy did not improve from 0.77901\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.5836 - accuracy: 0.7635 - val_loss: 0.5619 - val_accuracy: 0.7765\n",
            "Epoch 53/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5863 - accuracy: 0.7618\n",
            "Epoch 53: val_accuracy did not improve from 0.77901\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.5862 - accuracy: 0.7618 - val_loss: 0.5755 - val_accuracy: 0.7713\n",
            "Epoch 54/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5849 - accuracy: 0.7592\n",
            "Epoch 54: val_accuracy did not improve from 0.77901\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.5849 - accuracy: 0.7592 - val_loss: 0.5632 - val_accuracy: 0.7705\n",
            "Epoch 55/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5914 - accuracy: 0.7642\n",
            "Epoch 55: val_accuracy improved from 0.77901 to 0.77986, saving model to best_manual.h5\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5917 - accuracy: 0.7641 - val_loss: 0.5618 - val_accuracy: 0.7799\n",
            "Epoch 56/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5840 - accuracy: 0.7697\n",
            "Epoch 56: val_accuracy did not improve from 0.77986\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5840 - accuracy: 0.7697 - val_loss: 0.5634 - val_accuracy: 0.7730\n",
            "Epoch 57/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5805 - accuracy: 0.7623\n",
            "Epoch 57: val_accuracy did not improve from 0.77986\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5805 - accuracy: 0.7623 - val_loss: 0.5691 - val_accuracy: 0.7773\n",
            "Epoch 58/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5784 - accuracy: 0.7653\n",
            "Epoch 58: val_accuracy did not improve from 0.77986\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5782 - accuracy: 0.7655 - val_loss: 0.5608 - val_accuracy: 0.7799\n",
            "Epoch 59/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5788 - accuracy: 0.7672\n",
            "Epoch 59: val_accuracy did not improve from 0.77986\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5790 - accuracy: 0.7672 - val_loss: 0.5630 - val_accuracy: 0.7696\n",
            "Epoch 60/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5869 - accuracy: 0.7657\n",
            "Epoch 60: val_accuracy did not improve from 0.77986\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5869 - accuracy: 0.7657 - val_loss: 0.5589 - val_accuracy: 0.7790\n",
            "Epoch 61/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7678\n",
            "Epoch 61: val_accuracy did not improve from 0.77986\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5642 - accuracy: 0.7679 - val_loss: 0.5643 - val_accuracy: 0.7790\n",
            "Epoch 62/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5759 - accuracy: 0.7728\n",
            "Epoch 62: val_accuracy improved from 0.77986 to 0.78328, saving model to best_manual.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5756 - accuracy: 0.7730 - val_loss: 0.5542 - val_accuracy: 0.7833\n",
            "Epoch 63/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5747 - accuracy: 0.7652\n",
            "Epoch 63: val_accuracy did not improve from 0.78328\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5747 - accuracy: 0.7652 - val_loss: 0.5630 - val_accuracy: 0.7816\n",
            "Epoch 64/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5778 - accuracy: 0.7716\n",
            "Epoch 64: val_accuracy did not improve from 0.78328\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5778 - accuracy: 0.7715 - val_loss: 0.5628 - val_accuracy: 0.7833\n",
            "Epoch 65/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5755 - accuracy: 0.7716\n",
            "Epoch 65: val_accuracy did not improve from 0.78328\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5754 - accuracy: 0.7714 - val_loss: 0.5726 - val_accuracy: 0.7807\n",
            "Epoch 66/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5773 - accuracy: 0.7695\n",
            "Epoch 66: val_accuracy did not improve from 0.78328\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5773 - accuracy: 0.7695 - val_loss: 0.5845 - val_accuracy: 0.7688\n",
            "Epoch 67/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5648 - accuracy: 0.7736\n",
            "Epoch 67: val_accuracy did not improve from 0.78328\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5648 - accuracy: 0.7736 - val_loss: 0.5649 - val_accuracy: 0.7773\n",
            "Epoch 68/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5810 - accuracy: 0.7659\n",
            "Epoch 68: val_accuracy improved from 0.78328 to 0.78413, saving model to best_manual.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5810 - accuracy: 0.7659 - val_loss: 0.5640 - val_accuracy: 0.7841\n",
            "Epoch 69/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5758 - accuracy: 0.7676\n",
            "Epoch 69: val_accuracy did not improve from 0.78413\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5765 - accuracy: 0.7673 - val_loss: 0.5597 - val_accuracy: 0.7841\n",
            "Epoch 70/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5789 - accuracy: 0.7706\n",
            "Epoch 70: val_accuracy did not improve from 0.78413\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5789 - accuracy: 0.7706 - val_loss: 0.5692 - val_accuracy: 0.7833\n",
            "Epoch 71/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5779 - accuracy: 0.7711\n",
            "Epoch 71: val_accuracy did not improve from 0.78413\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5779 - accuracy: 0.7711 - val_loss: 0.5660 - val_accuracy: 0.7773\n",
            "Epoch 72/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5763 - accuracy: 0.7680\n",
            "Epoch 72: val_accuracy improved from 0.78413 to 0.78498, saving model to best_manual.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5763 - accuracy: 0.7680 - val_loss: 0.5621 - val_accuracy: 0.7850\n",
            "Epoch 73/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5685 - accuracy: 0.7707\n",
            "Epoch 73: val_accuracy did not improve from 0.78498\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5685 - accuracy: 0.7707 - val_loss: 0.5558 - val_accuracy: 0.7850\n",
            "Epoch 74/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5725 - accuracy: 0.7754\n",
            "Epoch 74: val_accuracy did not improve from 0.78498\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5725 - accuracy: 0.7754 - val_loss: 0.5659 - val_accuracy: 0.7824\n",
            "Epoch 75/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5685 - accuracy: 0.7750\n",
            "Epoch 75: val_accuracy did not improve from 0.78498\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5684 - accuracy: 0.7751 - val_loss: 0.5720 - val_accuracy: 0.7765\n",
            "Epoch 76/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5597 - accuracy: 0.7791\n",
            "Epoch 76: val_accuracy did not improve from 0.78498\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5597 - accuracy: 0.7791 - val_loss: 0.5716 - val_accuracy: 0.7816\n",
            "Epoch 77/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5689 - accuracy: 0.7699\n",
            "Epoch 77: val_accuracy improved from 0.78498 to 0.78925, saving model to best_manual.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5696 - accuracy: 0.7699 - val_loss: 0.5692 - val_accuracy: 0.7892\n",
            "Epoch 78/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5710 - accuracy: 0.7694\n",
            "Epoch 78: val_accuracy did not improve from 0.78925\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5710 - accuracy: 0.7694 - val_loss: 0.5573 - val_accuracy: 0.7867\n",
            "Epoch 79/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5720 - accuracy: 0.7678\n",
            "Epoch 79: val_accuracy did not improve from 0.78925\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5718 - accuracy: 0.7679 - val_loss: 0.5701 - val_accuracy: 0.7705\n",
            "Epoch 80/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5681 - accuracy: 0.7680\n",
            "Epoch 80: val_accuracy did not improve from 0.78925\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.5678 - accuracy: 0.7682 - val_loss: 0.5672 - val_accuracy: 0.7875\n",
            "Epoch 81/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5733 - accuracy: 0.7723\n",
            "Epoch 81: val_accuracy did not improve from 0.78925\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5736 - accuracy: 0.7720 - val_loss: 0.5616 - val_accuracy: 0.7833\n",
            "Epoch 82/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7739\n",
            "Epoch 82: val_accuracy did not improve from 0.78925\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5638 - accuracy: 0.7741 - val_loss: 0.5651 - val_accuracy: 0.7833\n",
            "Epoch 83/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5746 - accuracy: 0.7714\n",
            "Epoch 83: val_accuracy did not improve from 0.78925\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5746 - accuracy: 0.7713 - val_loss: 0.5649 - val_accuracy: 0.7850\n",
            "Epoch 84/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5722 - accuracy: 0.7711\n",
            "Epoch 84: val_accuracy did not improve from 0.78925\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5721 - accuracy: 0.7712 - val_loss: 0.5671 - val_accuracy: 0.7850\n",
            "Epoch 85/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5659 - accuracy: 0.7719\n",
            "Epoch 85: val_accuracy improved from 0.78925 to 0.79181, saving model to best_manual.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5663 - accuracy: 0.7717 - val_loss: 0.5548 - val_accuracy: 0.7918\n",
            "Epoch 86/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5733 - accuracy: 0.7726\n",
            "Epoch 86: val_accuracy did not improve from 0.79181\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5732 - accuracy: 0.7727 - val_loss: 0.5638 - val_accuracy: 0.7782\n",
            "Epoch 87/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5628 - accuracy: 0.7740\n",
            "Epoch 87: val_accuracy did not improve from 0.79181\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5628 - accuracy: 0.7740 - val_loss: 0.5656 - val_accuracy: 0.7816\n",
            "Epoch 88/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5687 - accuracy: 0.7750\n",
            "Epoch 88: val_accuracy did not improve from 0.79181\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5687 - accuracy: 0.7750 - val_loss: 0.5674 - val_accuracy: 0.7892\n",
            "Epoch 89/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5661 - accuracy: 0.7720\n",
            "Epoch 89: val_accuracy did not improve from 0.79181\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5661 - accuracy: 0.7720 - val_loss: 0.5804 - val_accuracy: 0.7816\n",
            "Epoch 90/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5645 - accuracy: 0.7731\n",
            "Epoch 90: val_accuracy did not improve from 0.79181\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5645 - accuracy: 0.7731 - val_loss: 0.5591 - val_accuracy: 0.7867\n",
            "Epoch 91/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5605 - accuracy: 0.7739\n",
            "Epoch 91: val_accuracy did not improve from 0.79181\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5605 - accuracy: 0.7739 - val_loss: 0.5658 - val_accuracy: 0.7901\n",
            "Epoch 92/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5594 - accuracy: 0.7728\n",
            "Epoch 92: val_accuracy did not improve from 0.79181\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5594 - accuracy: 0.7728 - val_loss: 0.5610 - val_accuracy: 0.7901\n",
            "Epoch 93/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5675 - accuracy: 0.7772\n",
            "Epoch 93: val_accuracy did not improve from 0.79181\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5678 - accuracy: 0.7771 - val_loss: 0.5665 - val_accuracy: 0.7875\n",
            "Epoch 94/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5723 - accuracy: 0.7714\n",
            "Epoch 94: val_accuracy did not improve from 0.79181\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5719 - accuracy: 0.7716 - val_loss: 0.5686 - val_accuracy: 0.7799\n",
            "Epoch 95/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5628 - accuracy: 0.7739\n",
            "Epoch 95: val_accuracy did not improve from 0.79181\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5628 - accuracy: 0.7739 - val_loss: 0.5739 - val_accuracy: 0.7875\n",
            "Epoch 96/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5587 - accuracy: 0.7727\n",
            "Epoch 96: val_accuracy did not improve from 0.79181\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5587 - accuracy: 0.7727 - val_loss: 0.5594 - val_accuracy: 0.7884\n",
            "Epoch 97/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5680 - accuracy: 0.7780\n",
            "Epoch 97: val_accuracy did not improve from 0.79181\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5678 - accuracy: 0.7782 - val_loss: 0.5647 - val_accuracy: 0.7901\n",
            "Epoch 98/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5633 - accuracy: 0.7757\n",
            "Epoch 98: val_accuracy did not improve from 0.79181\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5634 - accuracy: 0.7756 - val_loss: 0.5814 - val_accuracy: 0.7833\n",
            "Epoch 99/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5657 - accuracy: 0.7725\n",
            "Epoch 99: val_accuracy did not improve from 0.79181\n",
            "330/330 [==============================] - 13s 41ms/step - loss: 0.5655 - accuracy: 0.7726 - val_loss: 0.5576 - val_accuracy: 0.7790\n",
            "Epoch 100/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5648 - accuracy: 0.7746\n",
            "Epoch 100: val_accuracy improved from 0.79181 to 0.79352, saving model to best_manual.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5645 - accuracy: 0.7747 - val_loss: 0.5645 - val_accuracy: 0.7935\n",
            "Epoch 101/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7713\n",
            "Epoch 101: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5637 - accuracy: 0.7713 - val_loss: 0.5599 - val_accuracy: 0.7892\n",
            "Epoch 102/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5550 - accuracy: 0.7787\n",
            "Epoch 102: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5550 - accuracy: 0.7787 - val_loss: 0.5513 - val_accuracy: 0.7884\n",
            "Epoch 103/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5546 - accuracy: 0.7786\n",
            "Epoch 103: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5545 - accuracy: 0.7787 - val_loss: 0.5569 - val_accuracy: 0.7790\n",
            "Epoch 104/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5630 - accuracy: 0.7672\n",
            "Epoch 104: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5630 - accuracy: 0.7672 - val_loss: 0.5815 - val_accuracy: 0.7705\n",
            "Epoch 105/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5705 - accuracy: 0.7745\n",
            "Epoch 105: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5705 - accuracy: 0.7745 - val_loss: 0.5626 - val_accuracy: 0.7884\n",
            "Epoch 106/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5566 - accuracy: 0.7804\n",
            "Epoch 106: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5568 - accuracy: 0.7802 - val_loss: 0.5771 - val_accuracy: 0.7858\n",
            "Epoch 107/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5600 - accuracy: 0.7735\n",
            "Epoch 107: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5600 - accuracy: 0.7735 - val_loss: 0.5609 - val_accuracy: 0.7841\n",
            "Epoch 108/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5597 - accuracy: 0.7749\n",
            "Epoch 108: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5595 - accuracy: 0.7750 - val_loss: 0.5650 - val_accuracy: 0.7833\n",
            "Epoch 109/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5563 - accuracy: 0.7757\n",
            "Epoch 109: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5564 - accuracy: 0.7756 - val_loss: 0.5551 - val_accuracy: 0.7910\n",
            "Epoch 110/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5591 - accuracy: 0.7768\n",
            "Epoch 110: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5594 - accuracy: 0.7766 - val_loss: 0.5644 - val_accuracy: 0.7910\n",
            "Epoch 111/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7798\n",
            "Epoch 111: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5617 - accuracy: 0.7798 - val_loss: 0.5630 - val_accuracy: 0.7858\n",
            "Epoch 112/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5568 - accuracy: 0.7765\n",
            "Epoch 112: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5564 - accuracy: 0.7766 - val_loss: 0.5612 - val_accuracy: 0.7790\n",
            "Epoch 113/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5622 - accuracy: 0.7741\n",
            "Epoch 113: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5620 - accuracy: 0.7741 - val_loss: 0.5651 - val_accuracy: 0.7901\n",
            "Epoch 114/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5605 - accuracy: 0.7776\n",
            "Epoch 114: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5605 - accuracy: 0.7776 - val_loss: 0.5741 - val_accuracy: 0.7892\n",
            "Epoch 115/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5532 - accuracy: 0.7827\n",
            "Epoch 115: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5532 - accuracy: 0.7827 - val_loss: 0.5635 - val_accuracy: 0.7867\n",
            "Epoch 116/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7760\n",
            "Epoch 116: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5644 - accuracy: 0.7762 - val_loss: 0.5575 - val_accuracy: 0.7927\n",
            "Epoch 117/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7721\n",
            "Epoch 117: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5634 - accuracy: 0.7716 - val_loss: 0.5634 - val_accuracy: 0.7850\n",
            "Epoch 118/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5613 - accuracy: 0.7761\n",
            "Epoch 118: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5617 - accuracy: 0.7760 - val_loss: 0.5686 - val_accuracy: 0.7790\n",
            "Epoch 119/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5541 - accuracy: 0.7769\n",
            "Epoch 119: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5541 - accuracy: 0.7769 - val_loss: 0.5632 - val_accuracy: 0.7833\n",
            "Epoch 120/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5588 - accuracy: 0.7737\n",
            "Epoch 120: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5586 - accuracy: 0.7738 - val_loss: 0.5705 - val_accuracy: 0.7747\n",
            "Epoch 121/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5541 - accuracy: 0.7796\n",
            "Epoch 121: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5538 - accuracy: 0.7799 - val_loss: 0.5559 - val_accuracy: 0.7833\n",
            "Epoch 122/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5682 - accuracy: 0.7740\n",
            "Epoch 122: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5682 - accuracy: 0.7740 - val_loss: 0.5654 - val_accuracy: 0.7799\n",
            "Epoch 123/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5466 - accuracy: 0.7821\n",
            "Epoch 123: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5466 - accuracy: 0.7821 - val_loss: 0.5672 - val_accuracy: 0.7858\n",
            "Epoch 124/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5530 - accuracy: 0.7831\n",
            "Epoch 124: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5528 - accuracy: 0.7831 - val_loss: 0.5649 - val_accuracy: 0.7927\n",
            "Epoch 125/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5597 - accuracy: 0.7800\n",
            "Epoch 125: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5599 - accuracy: 0.7799 - val_loss: 0.5617 - val_accuracy: 0.7858\n",
            "Epoch 126/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5465 - accuracy: 0.7844\n",
            "Epoch 126: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5463 - accuracy: 0.7844 - val_loss: 0.5629 - val_accuracy: 0.7875\n",
            "Epoch 127/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5463 - accuracy: 0.7795\n",
            "Epoch 127: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5463 - accuracy: 0.7795 - val_loss: 0.5637 - val_accuracy: 0.7918\n",
            "Epoch 128/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5516 - accuracy: 0.7815\n",
            "Epoch 128: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5513 - accuracy: 0.7818 - val_loss: 0.5625 - val_accuracy: 0.7901\n",
            "Epoch 129/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5506 - accuracy: 0.7790\n",
            "Epoch 129: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5506 - accuracy: 0.7788 - val_loss: 0.5684 - val_accuracy: 0.7824\n",
            "Epoch 130/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5536 - accuracy: 0.7804\n",
            "Epoch 130: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5534 - accuracy: 0.7805 - val_loss: 0.5687 - val_accuracy: 0.7816\n",
            "Epoch 131/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5541 - accuracy: 0.7754\n",
            "Epoch 131: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5541 - accuracy: 0.7754 - val_loss: 0.5672 - val_accuracy: 0.7833\n",
            "Epoch 132/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5524 - accuracy: 0.7787\n",
            "Epoch 132: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5524 - accuracy: 0.7787 - val_loss: 0.5694 - val_accuracy: 0.7790\n",
            "Epoch 133/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5489 - accuracy: 0.7823\n",
            "Epoch 133: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5489 - accuracy: 0.7823 - val_loss: 0.5709 - val_accuracy: 0.7892\n",
            "Epoch 134/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5474 - accuracy: 0.7827\n",
            "Epoch 134: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5474 - accuracy: 0.7827 - val_loss: 0.5624 - val_accuracy: 0.7927\n",
            "Epoch 135/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5480 - accuracy: 0.7787\n",
            "Epoch 135: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5480 - accuracy: 0.7787 - val_loss: 0.5721 - val_accuracy: 0.7884\n",
            "Epoch 136/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5554 - accuracy: 0.7752\n",
            "Epoch 136: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5554 - accuracy: 0.7752 - val_loss: 0.5816 - val_accuracy: 0.7867\n",
            "Epoch 137/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5537 - accuracy: 0.7785\n",
            "Epoch 137: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5537 - accuracy: 0.7785 - val_loss: 0.5709 - val_accuracy: 0.7841\n",
            "Epoch 138/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5464 - accuracy: 0.7790\n",
            "Epoch 138: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5465 - accuracy: 0.7788 - val_loss: 0.5723 - val_accuracy: 0.7875\n",
            "Epoch 139/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5543 - accuracy: 0.7818\n",
            "Epoch 139: val_accuracy did not improve from 0.79352\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5543 - accuracy: 0.7818 - val_loss: 0.5573 - val_accuracy: 0.7892\n",
            "Epoch 140/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5520 - accuracy: 0.7799\n",
            "Epoch 140: val_accuracy improved from 0.79352 to 0.79437, saving model to best_manual.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5522 - accuracy: 0.7798 - val_loss: 0.5647 - val_accuracy: 0.7944\n",
            "Epoch 141/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5523 - accuracy: 0.7794\n",
            "Epoch 141: val_accuracy did not improve from 0.79437\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5523 - accuracy: 0.7794 - val_loss: 0.5655 - val_accuracy: 0.7901\n",
            "Epoch 142/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5455 - accuracy: 0.7849\n",
            "Epoch 142: val_accuracy did not improve from 0.79437\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5455 - accuracy: 0.7849 - val_loss: 0.5510 - val_accuracy: 0.7875\n",
            "Epoch 143/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5432 - accuracy: 0.7832\n",
            "Epoch 143: val_accuracy did not improve from 0.79437\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5432 - accuracy: 0.7832 - val_loss: 0.5640 - val_accuracy: 0.7944\n",
            "Epoch 144/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5478 - accuracy: 0.7794\n",
            "Epoch 144: val_accuracy improved from 0.79437 to 0.79863, saving model to best_manual.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5479 - accuracy: 0.7795 - val_loss: 0.5760 - val_accuracy: 0.7986\n",
            "Epoch 145/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5478 - accuracy: 0.7807\n",
            "Epoch 145: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5477 - accuracy: 0.7807 - val_loss: 0.5791 - val_accuracy: 0.7884\n",
            "Epoch 146/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5531 - accuracy: 0.7806\n",
            "Epoch 146: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5531 - accuracy: 0.7806 - val_loss: 0.5793 - val_accuracy: 0.7833\n",
            "Epoch 147/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5465 - accuracy: 0.7806\n",
            "Epoch 147: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5465 - accuracy: 0.7806 - val_loss: 0.5680 - val_accuracy: 0.7901\n",
            "Epoch 148/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5471 - accuracy: 0.7778\n",
            "Epoch 148: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5470 - accuracy: 0.7778 - val_loss: 0.5833 - val_accuracy: 0.7867\n",
            "Epoch 149/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5486 - accuracy: 0.7793\n",
            "Epoch 149: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5486 - accuracy: 0.7793 - val_loss: 0.5674 - val_accuracy: 0.7833\n",
            "Epoch 150/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5535 - accuracy: 0.7802\n",
            "Epoch 150: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5537 - accuracy: 0.7801 - val_loss: 0.5711 - val_accuracy: 0.7884\n",
            "Epoch 151/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5488 - accuracy: 0.7831\n",
            "Epoch 151: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5485 - accuracy: 0.7833 - val_loss: 0.5652 - val_accuracy: 0.7884\n",
            "Epoch 152/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5565 - accuracy: 0.7800\n",
            "Epoch 152: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5561 - accuracy: 0.7802 - val_loss: 0.5619 - val_accuracy: 0.7884\n",
            "Epoch 153/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5467 - accuracy: 0.7799\n",
            "Epoch 153: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5467 - accuracy: 0.7799 - val_loss: 0.5618 - val_accuracy: 0.7884\n",
            "Epoch 154/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5489 - accuracy: 0.7796\n",
            "Epoch 154: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5485 - accuracy: 0.7799 - val_loss: 0.5602 - val_accuracy: 0.7910\n",
            "Epoch 155/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5405 - accuracy: 0.7851\n",
            "Epoch 155: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5403 - accuracy: 0.7852 - val_loss: 0.5608 - val_accuracy: 0.7918\n",
            "Epoch 156/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5474 - accuracy: 0.7843\n",
            "Epoch 156: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5474 - accuracy: 0.7843 - val_loss: 0.5741 - val_accuracy: 0.7927\n",
            "Epoch 157/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5447 - accuracy: 0.7814\n",
            "Epoch 157: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5447 - accuracy: 0.7814 - val_loss: 0.5718 - val_accuracy: 0.7952\n",
            "Epoch 158/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5435 - accuracy: 0.7874\n",
            "Epoch 158: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5439 - accuracy: 0.7872 - val_loss: 0.5847 - val_accuracy: 0.7875\n",
            "Epoch 159/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5453 - accuracy: 0.7861\n",
            "Epoch 159: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5458 - accuracy: 0.7860 - val_loss: 0.5733 - val_accuracy: 0.7892\n",
            "Epoch 160/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5468 - accuracy: 0.7823\n",
            "Epoch 160: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5466 - accuracy: 0.7824 - val_loss: 0.5832 - val_accuracy: 0.7910\n",
            "Epoch 161/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5455 - accuracy: 0.7797\n",
            "Epoch 161: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5455 - accuracy: 0.7797 - val_loss: 0.5693 - val_accuracy: 0.7875\n",
            "Epoch 162/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5505 - accuracy: 0.7794\n",
            "Epoch 162: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5504 - accuracy: 0.7795 - val_loss: 0.5817 - val_accuracy: 0.7892\n",
            "Epoch 163/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5515 - accuracy: 0.7793\n",
            "Epoch 163: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5515 - accuracy: 0.7794 - val_loss: 0.5662 - val_accuracy: 0.7901\n",
            "Epoch 164/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5470 - accuracy: 0.7828\n",
            "Epoch 164: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5470 - accuracy: 0.7828 - val_loss: 0.5598 - val_accuracy: 0.7875\n",
            "Epoch 165/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5444 - accuracy: 0.7770\n",
            "Epoch 165: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5442 - accuracy: 0.7772 - val_loss: 0.5727 - val_accuracy: 0.7918\n",
            "Epoch 166/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5536 - accuracy: 0.7792\n",
            "Epoch 166: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5539 - accuracy: 0.7790 - val_loss: 0.5724 - val_accuracy: 0.7918\n",
            "Epoch 167/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5379 - accuracy: 0.7880\n",
            "Epoch 167: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5376 - accuracy: 0.7881 - val_loss: 0.5870 - val_accuracy: 0.7892\n",
            "Epoch 168/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5435 - accuracy: 0.7842\n",
            "Epoch 168: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5435 - accuracy: 0.7842 - val_loss: 0.5807 - val_accuracy: 0.7892\n",
            "Epoch 169/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5455 - accuracy: 0.7861\n",
            "Epoch 169: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5457 - accuracy: 0.7861 - val_loss: 0.5813 - val_accuracy: 0.7961\n",
            "Epoch 170/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5449 - accuracy: 0.7830\n",
            "Epoch 170: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5449 - accuracy: 0.7830 - val_loss: 0.5798 - val_accuracy: 0.7935\n",
            "Epoch 171/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5506 - accuracy: 0.7797\n",
            "Epoch 171: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5505 - accuracy: 0.7798 - val_loss: 0.5866 - val_accuracy: 0.7816\n",
            "Epoch 172/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5433 - accuracy: 0.7817\n",
            "Epoch 172: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5435 - accuracy: 0.7816 - val_loss: 0.5696 - val_accuracy: 0.7850\n",
            "Epoch 173/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5424 - accuracy: 0.7805\n",
            "Epoch 173: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5424 - accuracy: 0.7805 - val_loss: 0.5664 - val_accuracy: 0.7927\n",
            "Epoch 174/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5460 - accuracy: 0.7805\n",
            "Epoch 174: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5460 - accuracy: 0.7805 - val_loss: 0.5701 - val_accuracy: 0.7952\n",
            "Epoch 175/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5525 - accuracy: 0.7826\n",
            "Epoch 175: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5525 - accuracy: 0.7826 - val_loss: 0.5621 - val_accuracy: 0.7935\n",
            "Epoch 176/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5363 - accuracy: 0.7899\n",
            "Epoch 176: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5363 - accuracy: 0.7899 - val_loss: 0.5690 - val_accuracy: 0.7910\n",
            "Epoch 177/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5489 - accuracy: 0.7835\n",
            "Epoch 177: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5491 - accuracy: 0.7834 - val_loss: 0.5758 - val_accuracy: 0.7910\n",
            "Epoch 178/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5386 - accuracy: 0.7844\n",
            "Epoch 178: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5386 - accuracy: 0.7843 - val_loss: 0.5786 - val_accuracy: 0.7892\n",
            "Epoch 179/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5469 - accuracy: 0.7888\n",
            "Epoch 179: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5469 - accuracy: 0.7886 - val_loss: 0.5670 - val_accuracy: 0.7910\n",
            "Epoch 180/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5470 - accuracy: 0.7819\n",
            "Epoch 180: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5473 - accuracy: 0.7819 - val_loss: 0.5807 - val_accuracy: 0.7867\n",
            "Epoch 181/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5328 - accuracy: 0.7891\n",
            "Epoch 181: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5327 - accuracy: 0.7892 - val_loss: 0.5684 - val_accuracy: 0.7867\n",
            "Epoch 182/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5388 - accuracy: 0.7893\n",
            "Epoch 182: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5388 - accuracy: 0.7893 - val_loss: 0.5778 - val_accuracy: 0.7892\n",
            "Epoch 183/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5445 - accuracy: 0.7869\n",
            "Epoch 183: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 20s 60ms/step - loss: 0.5445 - accuracy: 0.7869 - val_loss: 0.5708 - val_accuracy: 0.7875\n",
            "Epoch 184/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5451 - accuracy: 0.7803\n",
            "Epoch 184: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 16s 50ms/step - loss: 0.5451 - accuracy: 0.7803 - val_loss: 0.5709 - val_accuracy: 0.7884\n",
            "Epoch 185/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5446 - accuracy: 0.7837\n",
            "Epoch 185: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5441 - accuracy: 0.7840 - val_loss: 0.5741 - val_accuracy: 0.7884\n",
            "Epoch 186/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5385 - accuracy: 0.7890\n",
            "Epoch 186: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5381 - accuracy: 0.7892 - val_loss: 0.5719 - val_accuracy: 0.7799\n",
            "Epoch 187/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5345 - accuracy: 0.7863\n",
            "Epoch 187: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5341 - accuracy: 0.7864 - val_loss: 0.5760 - val_accuracy: 0.7824\n",
            "Epoch 188/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5468 - accuracy: 0.7799\n",
            "Epoch 188: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5473 - accuracy: 0.7798 - val_loss: 0.5991 - val_accuracy: 0.7807\n",
            "Epoch 189/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5332 - accuracy: 0.7870\n",
            "Epoch 189: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5330 - accuracy: 0.7871 - val_loss: 0.5678 - val_accuracy: 0.7927\n",
            "Epoch 190/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5394 - accuracy: 0.7851\n",
            "Epoch 190: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5391 - accuracy: 0.7853 - val_loss: 0.5597 - val_accuracy: 0.7867\n",
            "Epoch 191/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5501 - accuracy: 0.7812\n",
            "Epoch 191: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5502 - accuracy: 0.7812 - val_loss: 0.5629 - val_accuracy: 0.7884\n",
            "Epoch 192/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5429 - accuracy: 0.7832\n",
            "Epoch 192: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 13s 40ms/step - loss: 0.5429 - accuracy: 0.7833 - val_loss: 0.5773 - val_accuracy: 0.7927\n",
            "Epoch 193/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5421 - accuracy: 0.7809\n",
            "Epoch 193: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5421 - accuracy: 0.7809 - val_loss: 0.5816 - val_accuracy: 0.7790\n",
            "Epoch 194/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5419 - accuracy: 0.7850\n",
            "Epoch 194: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5416 - accuracy: 0.7852 - val_loss: 0.5749 - val_accuracy: 0.7910\n",
            "Epoch 195/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5440 - accuracy: 0.7820\n",
            "Epoch 195: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5440 - accuracy: 0.7820 - val_loss: 0.5773 - val_accuracy: 0.7918\n",
            "Epoch 196/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5433 - accuracy: 0.7848\n",
            "Epoch 196: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5433 - accuracy: 0.7847 - val_loss: 0.5628 - val_accuracy: 0.7867\n",
            "Epoch 197/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5456 - accuracy: 0.7820\n",
            "Epoch 197: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5458 - accuracy: 0.7818 - val_loss: 0.5785 - val_accuracy: 0.7892\n",
            "Epoch 198/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5417 - accuracy: 0.7830\n",
            "Epoch 198: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5417 - accuracy: 0.7830 - val_loss: 0.5782 - val_accuracy: 0.7773\n",
            "Epoch 199/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5430 - accuracy: 0.7855\n",
            "Epoch 199: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5430 - accuracy: 0.7855 - val_loss: 0.5727 - val_accuracy: 0.7918\n",
            "Epoch 200/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5336 - accuracy: 0.7866\n",
            "Epoch 200: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5336 - accuracy: 0.7866 - val_loss: 0.5689 - val_accuracy: 0.7910\n"
          ]
        }
      ],
      "source": [
        "hist_manual = model_manual.fit(X_train_pad, y_train_manual, \n",
        "                 batch_size=batch_size,\n",
        "                 epochs=epochs,\n",
        "                 validation_split=0.1,\n",
        "                 callbacks=[mc_manual]\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(hist_manual.history['accuracy'])\n",
        "plt.plot(hist_manual.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# \"Loss\"\n",
        "plt.plot(hist_manual.history['loss'])\n",
        "plt.plot(hist_manual.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC0BdpPRELG9",
        "outputId": "6eedb41d-5028-4951-9508-579118c7df4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cJCz1Z_xykw",
        "outputId": "6cad0762-10cf-4d4f-ab13-069d6d25ea14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92/92 [==============================] - 2s 8ms/step\n",
            "Accuracy: 80.40%\n",
            "\n",
            "F1 Score: 80.40\n"
          ]
        }
      ],
      "source": [
        "predictions_manual = model_manual.predict(X_test_pad)\n",
        "predictions_manual = np.argmax(predictions_manual, axis=1)\n",
        "predictions_manual = [class_names[pred] for pred in predictions_manual]\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(test_df.label, predictions_manual) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions_manual, average='micro') * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwyuPBdpxykw",
        "outputId": "1e999d92-3065-48d9-b44c-97be60233f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.9343 - accuracy: 0.5726\n",
            "Epoch 1: val_accuracy improved from -inf to 0.64249, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.9343 - accuracy: 0.5726 - val_loss: 0.8094 - val_accuracy: 0.6425\n",
            "Epoch 2/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8693 - accuracy: 0.6023\n",
            "Epoch 2: val_accuracy improved from 0.64249 to 0.66980, saving model to best_vader.h5\n",
            "330/330 [==============================] - 17s 51ms/step - loss: 0.8696 - accuracy: 0.6020 - val_loss: 0.7763 - val_accuracy: 0.6698\n",
            "Epoch 3/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8338 - accuracy: 0.6182\n",
            "Epoch 3: val_accuracy improved from 0.66980 to 0.70137, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8342 - accuracy: 0.6179 - val_loss: 0.7453 - val_accuracy: 0.7014\n",
            "Epoch 4/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8164 - accuracy: 0.6408\n",
            "Epoch 4: val_accuracy improved from 0.70137 to 0.71331, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8163 - accuracy: 0.6408 - val_loss: 0.7223 - val_accuracy: 0.7133\n",
            "Epoch 5/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7950 - accuracy: 0.6509\n",
            "Epoch 5: val_accuracy improved from 0.71331 to 0.72782, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7949 - accuracy: 0.6509 - val_loss: 0.7045 - val_accuracy: 0.7278\n",
            "Epoch 6/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7903 - accuracy: 0.6573\n",
            "Epoch 6: val_accuracy improved from 0.72782 to 0.73379, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.7900 - accuracy: 0.6575 - val_loss: 0.6950 - val_accuracy: 0.7338\n",
            "Epoch 7/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7764 - accuracy: 0.6633\n",
            "Epoch 7: val_accuracy did not improve from 0.73379\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.7764 - accuracy: 0.6633 - val_loss: 0.6830 - val_accuracy: 0.7329\n",
            "Epoch 8/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7659 - accuracy: 0.6818\n",
            "Epoch 8: val_accuracy improved from 0.73379 to 0.74147, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.7656 - accuracy: 0.6818 - val_loss: 0.6712 - val_accuracy: 0.7415\n",
            "Epoch 9/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7651 - accuracy: 0.6804\n",
            "Epoch 9: val_accuracy did not improve from 0.74147\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.7649 - accuracy: 0.6805 - val_loss: 0.6737 - val_accuracy: 0.7415\n",
            "Epoch 10/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7538 - accuracy: 0.6824\n",
            "Epoch 10: val_accuracy improved from 0.74147 to 0.74659, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.7538 - accuracy: 0.6824 - val_loss: 0.6608 - val_accuracy: 0.7466\n",
            "Epoch 11/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7419 - accuracy: 0.6903\n",
            "Epoch 11: val_accuracy did not improve from 0.74659\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7419 - accuracy: 0.6903 - val_loss: 0.6530 - val_accuracy: 0.7432\n",
            "Epoch 12/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7341 - accuracy: 0.6960\n",
            "Epoch 12: val_accuracy improved from 0.74659 to 0.74915, saving model to best_vader.h5\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.7339 - accuracy: 0.6960 - val_loss: 0.6416 - val_accuracy: 0.7491\n",
            "Epoch 13/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7396 - accuracy: 0.6960\n",
            "Epoch 13: val_accuracy improved from 0.74915 to 0.75171, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.7398 - accuracy: 0.6959 - val_loss: 0.6447 - val_accuracy: 0.7517\n",
            "Epoch 14/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7303 - accuracy: 0.7008\n",
            "Epoch 14: val_accuracy improved from 0.75171 to 0.75341, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.7304 - accuracy: 0.7007 - val_loss: 0.6395 - val_accuracy: 0.7534\n",
            "Epoch 15/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7273 - accuracy: 0.6996\n",
            "Epoch 15: val_accuracy improved from 0.75341 to 0.76195, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.7271 - accuracy: 0.6997 - val_loss: 0.6325 - val_accuracy: 0.7619\n",
            "Epoch 16/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7171 - accuracy: 0.7047\n",
            "Epoch 16: val_accuracy improved from 0.76195 to 0.76706, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.7175 - accuracy: 0.7046 - val_loss: 0.6225 - val_accuracy: 0.7671\n",
            "Epoch 17/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7245 - accuracy: 0.7108\n",
            "Epoch 17: val_accuracy did not improve from 0.76706\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.7243 - accuracy: 0.7110 - val_loss: 0.6318 - val_accuracy: 0.7628\n",
            "Epoch 18/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7248 - accuracy: 0.7057\n",
            "Epoch 18: val_accuracy did not improve from 0.76706\n",
            "330/330 [==============================] - 12s 37ms/step - loss: 0.7248 - accuracy: 0.7054 - val_loss: 0.6242 - val_accuracy: 0.7637\n",
            "Epoch 19/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7172 - accuracy: 0.7161\n",
            "Epoch 19: val_accuracy improved from 0.76706 to 0.76792, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.7179 - accuracy: 0.7157 - val_loss: 0.6156 - val_accuracy: 0.7679\n",
            "Epoch 20/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7126 - accuracy: 0.7166\n",
            "Epoch 20: val_accuracy did not improve from 0.76792\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.7126 - accuracy: 0.7166 - val_loss: 0.6117 - val_accuracy: 0.7662\n",
            "Epoch 21/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7084 - accuracy: 0.7227\n",
            "Epoch 21: val_accuracy improved from 0.76792 to 0.77389, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.7084 - accuracy: 0.7227 - val_loss: 0.6084 - val_accuracy: 0.7739\n",
            "Epoch 22/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6983 - accuracy: 0.7187\n",
            "Epoch 22: val_accuracy did not improve from 0.77389\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6983 - accuracy: 0.7187 - val_loss: 0.6096 - val_accuracy: 0.7671\n",
            "Epoch 23/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7001 - accuracy: 0.7215\n",
            "Epoch 23: val_accuracy improved from 0.77389 to 0.77560, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.7001 - accuracy: 0.7214 - val_loss: 0.6026 - val_accuracy: 0.7756\n",
            "Epoch 24/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6982 - accuracy: 0.7286\n",
            "Epoch 24: val_accuracy improved from 0.77560 to 0.77645, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6982 - accuracy: 0.7286 - val_loss: 0.6075 - val_accuracy: 0.7765\n",
            "Epoch 25/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6963 - accuracy: 0.7242\n",
            "Epoch 25: val_accuracy improved from 0.77645 to 0.77816, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6963 - accuracy: 0.7242 - val_loss: 0.6037 - val_accuracy: 0.7782\n",
            "Epoch 26/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7008 - accuracy: 0.7240\n",
            "Epoch 26: val_accuracy improved from 0.77816 to 0.78242, saving model to best_vader.h5\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.7009 - accuracy: 0.7239 - val_loss: 0.5967 - val_accuracy: 0.7824\n",
            "Epoch 27/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6986 - accuracy: 0.7306\n",
            "Epoch 27: val_accuracy improved from 0.78242 to 0.78498, saving model to best_vader.h5\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6990 - accuracy: 0.7305 - val_loss: 0.5925 - val_accuracy: 0.7850\n",
            "Epoch 28/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.7315\n",
            "Epoch 28: val_accuracy did not improve from 0.78498\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6937 - accuracy: 0.7315 - val_loss: 0.5944 - val_accuracy: 0.7841\n",
            "Epoch 29/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6940 - accuracy: 0.7346\n",
            "Epoch 29: val_accuracy did not improve from 0.78498\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6940 - accuracy: 0.7345 - val_loss: 0.5919 - val_accuracy: 0.7790\n",
            "Epoch 30/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6870 - accuracy: 0.7300\n",
            "Epoch 30: val_accuracy did not improve from 0.78498\n",
            "330/330 [==============================] - 14s 42ms/step - loss: 0.6870 - accuracy: 0.7300 - val_loss: 0.5882 - val_accuracy: 0.7850\n",
            "Epoch 31/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6902 - accuracy: 0.7348\n",
            "Epoch 31: val_accuracy improved from 0.78498 to 0.78584, saving model to best_vader.h5\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6899 - accuracy: 0.7350 - val_loss: 0.5816 - val_accuracy: 0.7858\n",
            "Epoch 32/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6878 - accuracy: 0.7283\n",
            "Epoch 32: val_accuracy improved from 0.78584 to 0.78925, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6882 - accuracy: 0.7282 - val_loss: 0.5813 - val_accuracy: 0.7892\n",
            "Epoch 33/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6819 - accuracy: 0.7379\n",
            "Epoch 33: val_accuracy improved from 0.78925 to 0.79181, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6824 - accuracy: 0.7377 - val_loss: 0.5769 - val_accuracy: 0.7918\n",
            "Epoch 34/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6791 - accuracy: 0.7369\n",
            "Epoch 34: val_accuracy did not improve from 0.79181\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6791 - accuracy: 0.7369 - val_loss: 0.5839 - val_accuracy: 0.7816\n",
            "Epoch 35/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6796 - accuracy: 0.7351\n",
            "Epoch 35: val_accuracy did not improve from 0.79181\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6796 - accuracy: 0.7351 - val_loss: 0.5816 - val_accuracy: 0.7850\n",
            "Epoch 36/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6771 - accuracy: 0.7397\n",
            "Epoch 36: val_accuracy improved from 0.79181 to 0.79863, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6776 - accuracy: 0.7398 - val_loss: 0.5789 - val_accuracy: 0.7986\n",
            "Epoch 37/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6747 - accuracy: 0.7367\n",
            "Epoch 37: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6748 - accuracy: 0.7368 - val_loss: 0.5853 - val_accuracy: 0.7816\n",
            "Epoch 38/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6746 - accuracy: 0.7396\n",
            "Epoch 38: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6744 - accuracy: 0.7398 - val_loss: 0.5734 - val_accuracy: 0.7978\n",
            "Epoch 39/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6697 - accuracy: 0.7422\n",
            "Epoch 39: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6697 - accuracy: 0.7422 - val_loss: 0.5725 - val_accuracy: 0.7969\n",
            "Epoch 40/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6700 - accuracy: 0.7395\n",
            "Epoch 40: val_accuracy did not improve from 0.79863\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6699 - accuracy: 0.7396 - val_loss: 0.5698 - val_accuracy: 0.7969\n",
            "Epoch 41/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6800 - accuracy: 0.7412\n",
            "Epoch 41: val_accuracy improved from 0.79863 to 0.80034, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6800 - accuracy: 0.7412 - val_loss: 0.5675 - val_accuracy: 0.8003\n",
            "Epoch 42/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6713 - accuracy: 0.7420\n",
            "Epoch 42: val_accuracy did not improve from 0.80034\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6712 - accuracy: 0.7420 - val_loss: 0.5785 - val_accuracy: 0.7944\n",
            "Epoch 43/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6800 - accuracy: 0.7339\n",
            "Epoch 43: val_accuracy improved from 0.80034 to 0.80205, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6800 - accuracy: 0.7339 - val_loss: 0.5648 - val_accuracy: 0.8020\n",
            "Epoch 44/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6759 - accuracy: 0.7347\n",
            "Epoch 44: val_accuracy did not improve from 0.80205\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6759 - accuracy: 0.7347 - val_loss: 0.5817 - val_accuracy: 0.7901\n",
            "Epoch 45/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6636 - accuracy: 0.7415\n",
            "Epoch 45: val_accuracy improved from 0.80205 to 0.80461, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6634 - accuracy: 0.7417 - val_loss: 0.5654 - val_accuracy: 0.8046\n",
            "Epoch 46/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6698 - accuracy: 0.7408\n",
            "Epoch 46: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6698 - accuracy: 0.7408 - val_loss: 0.5672 - val_accuracy: 0.8020\n",
            "Epoch 47/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6749 - accuracy: 0.7440\n",
            "Epoch 47: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6747 - accuracy: 0.7441 - val_loss: 0.5635 - val_accuracy: 0.7995\n",
            "Epoch 48/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6736 - accuracy: 0.7403\n",
            "Epoch 48: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6736 - accuracy: 0.7403 - val_loss: 0.5664 - val_accuracy: 0.7986\n",
            "Epoch 49/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6728 - accuracy: 0.7421\n",
            "Epoch 49: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6728 - accuracy: 0.7421 - val_loss: 0.5642 - val_accuracy: 0.8029\n",
            "Epoch 50/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6693 - accuracy: 0.7436\n",
            "Epoch 50: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6695 - accuracy: 0.7435 - val_loss: 0.5671 - val_accuracy: 0.7961\n",
            "Epoch 51/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6602 - accuracy: 0.7486\n",
            "Epoch 51: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6602 - accuracy: 0.7486 - val_loss: 0.5683 - val_accuracy: 0.8012\n",
            "Epoch 52/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6638 - accuracy: 0.7442\n",
            "Epoch 52: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6642 - accuracy: 0.7440 - val_loss: 0.5722 - val_accuracy: 0.8020\n",
            "Epoch 53/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6556 - accuracy: 0.7483\n",
            "Epoch 53: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6560 - accuracy: 0.7482 - val_loss: 0.5767 - val_accuracy: 0.7892\n",
            "Epoch 54/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6625 - accuracy: 0.7436\n",
            "Epoch 54: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6625 - accuracy: 0.7436 - val_loss: 0.5615 - val_accuracy: 0.8038\n",
            "Epoch 55/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6679 - accuracy: 0.7396\n",
            "Epoch 55: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6676 - accuracy: 0.7397 - val_loss: 0.5671 - val_accuracy: 0.7935\n",
            "Epoch 56/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6664 - accuracy: 0.7472\n",
            "Epoch 56: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6664 - accuracy: 0.7472 - val_loss: 0.5709 - val_accuracy: 0.7961\n",
            "Epoch 57/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6550 - accuracy: 0.7500\n",
            "Epoch 57: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6550 - accuracy: 0.7501 - val_loss: 0.5625 - val_accuracy: 0.7978\n",
            "Epoch 58/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6620 - accuracy: 0.7494\n",
            "Epoch 58: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6619 - accuracy: 0.7496 - val_loss: 0.5616 - val_accuracy: 0.8046\n",
            "Epoch 59/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6543 - accuracy: 0.7510\n",
            "Epoch 59: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6543 - accuracy: 0.7510 - val_loss: 0.5663 - val_accuracy: 0.7986\n",
            "Epoch 60/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6471 - accuracy: 0.7463\n",
            "Epoch 60: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6474 - accuracy: 0.7462 - val_loss: 0.5623 - val_accuracy: 0.7978\n",
            "Epoch 61/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6504 - accuracy: 0.7511\n",
            "Epoch 61: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6502 - accuracy: 0.7512 - val_loss: 0.5574 - val_accuracy: 0.7952\n",
            "Epoch 62/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6433 - accuracy: 0.7537\n",
            "Epoch 62: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6435 - accuracy: 0.7534 - val_loss: 0.5536 - val_accuracy: 0.8003\n",
            "Epoch 63/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6530 - accuracy: 0.7428\n",
            "Epoch 63: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6532 - accuracy: 0.7428 - val_loss: 0.5623 - val_accuracy: 0.7952\n",
            "Epoch 64/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6496 - accuracy: 0.7477\n",
            "Epoch 64: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6496 - accuracy: 0.7478 - val_loss: 0.5584 - val_accuracy: 0.7986\n",
            "Epoch 65/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6489 - accuracy: 0.7505\n",
            "Epoch 65: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6490 - accuracy: 0.7503 - val_loss: 0.5531 - val_accuracy: 0.8003\n",
            "Epoch 66/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6534 - accuracy: 0.7516\n",
            "Epoch 66: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6531 - accuracy: 0.7517 - val_loss: 0.5602 - val_accuracy: 0.7986\n",
            "Epoch 67/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6399 - accuracy: 0.7588\n",
            "Epoch 67: val_accuracy did not improve from 0.80461\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6399 - accuracy: 0.7588 - val_loss: 0.5633 - val_accuracy: 0.7978\n",
            "Epoch 68/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6430 - accuracy: 0.7534\n",
            "Epoch 68: val_accuracy improved from 0.80461 to 0.80631, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6430 - accuracy: 0.7534 - val_loss: 0.5586 - val_accuracy: 0.8063\n",
            "Epoch 69/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6461 - accuracy: 0.7531\n",
            "Epoch 69: val_accuracy did not improve from 0.80631\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6461 - accuracy: 0.7531 - val_loss: 0.5662 - val_accuracy: 0.8003\n",
            "Epoch 70/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6500 - accuracy: 0.7554\n",
            "Epoch 70: val_accuracy did not improve from 0.80631\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6503 - accuracy: 0.7551 - val_loss: 0.5598 - val_accuracy: 0.8012\n",
            "Epoch 71/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6548 - accuracy: 0.7485\n",
            "Epoch 71: val_accuracy did not improve from 0.80631\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6548 - accuracy: 0.7485 - val_loss: 0.5543 - val_accuracy: 0.8038\n",
            "Epoch 72/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6419 - accuracy: 0.7558\n",
            "Epoch 72: val_accuracy did not improve from 0.80631\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6420 - accuracy: 0.7559 - val_loss: 0.5564 - val_accuracy: 0.8063\n",
            "Epoch 73/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6497 - accuracy: 0.7477\n",
            "Epoch 73: val_accuracy did not improve from 0.80631\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6497 - accuracy: 0.7477 - val_loss: 0.5625 - val_accuracy: 0.8029\n",
            "Epoch 74/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6391 - accuracy: 0.7528\n",
            "Epoch 74: val_accuracy did not improve from 0.80631\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6391 - accuracy: 0.7528 - val_loss: 0.5633 - val_accuracy: 0.7969\n",
            "Epoch 75/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6547 - accuracy: 0.7509\n",
            "Epoch 75: val_accuracy did not improve from 0.80631\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6548 - accuracy: 0.7509 - val_loss: 0.5637 - val_accuracy: 0.8038\n",
            "Epoch 76/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6415 - accuracy: 0.7584\n",
            "Epoch 76: val_accuracy did not improve from 0.80631\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6415 - accuracy: 0.7583 - val_loss: 0.5604 - val_accuracy: 0.8038\n",
            "Epoch 77/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6457 - accuracy: 0.7562\n",
            "Epoch 77: val_accuracy did not improve from 0.80631\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6457 - accuracy: 0.7562 - val_loss: 0.5618 - val_accuracy: 0.8029\n",
            "Epoch 78/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6397 - accuracy: 0.7583\n",
            "Epoch 78: val_accuracy improved from 0.80631 to 0.80887, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6397 - accuracy: 0.7583 - val_loss: 0.5563 - val_accuracy: 0.8089\n",
            "Epoch 79/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6352 - accuracy: 0.7602\n",
            "Epoch 79: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6352 - accuracy: 0.7602 - val_loss: 0.5540 - val_accuracy: 0.8046\n",
            "Epoch 80/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6475 - accuracy: 0.7602\n",
            "Epoch 80: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6475 - accuracy: 0.7602 - val_loss: 0.5605 - val_accuracy: 0.8072\n",
            "Epoch 81/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6413 - accuracy: 0.7537\n",
            "Epoch 81: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6408 - accuracy: 0.7540 - val_loss: 0.5504 - val_accuracy: 0.8055\n",
            "Epoch 82/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6408 - accuracy: 0.7547\n",
            "Epoch 82: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6406 - accuracy: 0.7548 - val_loss: 0.5638 - val_accuracy: 0.8020\n",
            "Epoch 83/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6475 - accuracy: 0.7551\n",
            "Epoch 83: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6476 - accuracy: 0.7551 - val_loss: 0.5529 - val_accuracy: 0.8038\n",
            "Epoch 84/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6431 - accuracy: 0.7610\n",
            "Epoch 84: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6431 - accuracy: 0.7610 - val_loss: 0.5594 - val_accuracy: 0.8020\n",
            "Epoch 85/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6379 - accuracy: 0.7596\n",
            "Epoch 85: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6376 - accuracy: 0.7599 - val_loss: 0.5656 - val_accuracy: 0.7969\n",
            "Epoch 86/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6478 - accuracy: 0.7557\n",
            "Epoch 86: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6478 - accuracy: 0.7557 - val_loss: 0.5591 - val_accuracy: 0.8020\n",
            "Epoch 87/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6317 - accuracy: 0.7629\n",
            "Epoch 87: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6318 - accuracy: 0.7628 - val_loss: 0.5550 - val_accuracy: 0.8038\n",
            "Epoch 88/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6371 - accuracy: 0.7606\n",
            "Epoch 88: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6368 - accuracy: 0.7607 - val_loss: 0.5613 - val_accuracy: 0.7978\n",
            "Epoch 89/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6335 - accuracy: 0.7624\n",
            "Epoch 89: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6335 - accuracy: 0.7624 - val_loss: 0.5556 - val_accuracy: 0.8046\n",
            "Epoch 90/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6439 - accuracy: 0.7583\n",
            "Epoch 90: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6436 - accuracy: 0.7584 - val_loss: 0.5569 - val_accuracy: 0.8072\n",
            "Epoch 91/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6362 - accuracy: 0.7617\n",
            "Epoch 91: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6362 - accuracy: 0.7617 - val_loss: 0.5646 - val_accuracy: 0.8003\n",
            "Epoch 92/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6336 - accuracy: 0.7549\n",
            "Epoch 92: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6337 - accuracy: 0.7549 - val_loss: 0.5602 - val_accuracy: 0.8020\n",
            "Epoch 93/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6368 - accuracy: 0.7577\n",
            "Epoch 93: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6368 - accuracy: 0.7577 - val_loss: 0.5544 - val_accuracy: 0.8046\n",
            "Epoch 94/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6405 - accuracy: 0.7590\n",
            "Epoch 94: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6405 - accuracy: 0.7590 - val_loss: 0.5647 - val_accuracy: 0.8012\n",
            "Epoch 95/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6348 - accuracy: 0.7534\n",
            "Epoch 95: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6348 - accuracy: 0.7533 - val_loss: 0.5707 - val_accuracy: 0.7986\n",
            "Epoch 96/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6382 - accuracy: 0.7576\n",
            "Epoch 96: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6385 - accuracy: 0.7576 - val_loss: 0.5584 - val_accuracy: 0.8055\n",
            "Epoch 97/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6438 - accuracy: 0.7591\n",
            "Epoch 97: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6440 - accuracy: 0.7590 - val_loss: 0.5554 - val_accuracy: 0.8080\n",
            "Epoch 98/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6305 - accuracy: 0.7566\n",
            "Epoch 98: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6305 - accuracy: 0.7566 - val_loss: 0.5682 - val_accuracy: 0.8029\n",
            "Epoch 99/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6419 - accuracy: 0.7569\n",
            "Epoch 99: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6419 - accuracy: 0.7569 - val_loss: 0.5554 - val_accuracy: 0.8046\n",
            "Epoch 100/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6285 - accuracy: 0.7634\n",
            "Epoch 100: val_accuracy did not improve from 0.80887\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6288 - accuracy: 0.7632 - val_loss: 0.5617 - val_accuracy: 0.8038\n",
            "Epoch 101/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6357 - accuracy: 0.7613\n",
            "Epoch 101: val_accuracy improved from 0.80887 to 0.80973, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6357 - accuracy: 0.7613 - val_loss: 0.5481 - val_accuracy: 0.8097\n",
            "Epoch 102/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6213 - accuracy: 0.7628\n",
            "Epoch 102: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6213 - accuracy: 0.7628 - val_loss: 0.5608 - val_accuracy: 0.8046\n",
            "Epoch 103/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6352 - accuracy: 0.7584\n",
            "Epoch 103: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6352 - accuracy: 0.7584 - val_loss: 0.5602 - val_accuracy: 0.8072\n",
            "Epoch 104/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6241 - accuracy: 0.7606\n",
            "Epoch 104: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6242 - accuracy: 0.7605 - val_loss: 0.5615 - val_accuracy: 0.8046\n",
            "Epoch 105/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6342 - accuracy: 0.7569\n",
            "Epoch 105: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6342 - accuracy: 0.7569 - val_loss: 0.5607 - val_accuracy: 0.7978\n",
            "Epoch 106/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6250 - accuracy: 0.7623\n",
            "Epoch 106: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6254 - accuracy: 0.7619 - val_loss: 0.5628 - val_accuracy: 0.8012\n",
            "Epoch 107/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6298 - accuracy: 0.7614\n",
            "Epoch 107: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6298 - accuracy: 0.7615 - val_loss: 0.5577 - val_accuracy: 0.8046\n",
            "Epoch 108/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6310 - accuracy: 0.7620\n",
            "Epoch 108: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6310 - accuracy: 0.7620 - val_loss: 0.5560 - val_accuracy: 0.8029\n",
            "Epoch 109/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6296 - accuracy: 0.7616\n",
            "Epoch 109: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6298 - accuracy: 0.7615 - val_loss: 0.5579 - val_accuracy: 0.8063\n",
            "Epoch 110/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6421 - accuracy: 0.7614\n",
            "Epoch 110: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6423 - accuracy: 0.7613 - val_loss: 0.5716 - val_accuracy: 0.7969\n",
            "Epoch 111/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6300 - accuracy: 0.7668\n",
            "Epoch 111: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6303 - accuracy: 0.7666 - val_loss: 0.5553 - val_accuracy: 0.8003\n",
            "Epoch 112/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6326 - accuracy: 0.7628\n",
            "Epoch 112: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6326 - accuracy: 0.7628 - val_loss: 0.5577 - val_accuracy: 0.8012\n",
            "Epoch 113/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6274 - accuracy: 0.7639\n",
            "Epoch 113: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6274 - accuracy: 0.7639 - val_loss: 0.5561 - val_accuracy: 0.8003\n",
            "Epoch 114/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6333 - accuracy: 0.7607\n",
            "Epoch 114: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6337 - accuracy: 0.7608 - val_loss: 0.5570 - val_accuracy: 0.7995\n",
            "Epoch 115/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6224 - accuracy: 0.7642\n",
            "Epoch 115: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6224 - accuracy: 0.7642 - val_loss: 0.5558 - val_accuracy: 0.8046\n",
            "Epoch 116/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6253 - accuracy: 0.7614\n",
            "Epoch 116: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6250 - accuracy: 0.7616 - val_loss: 0.5572 - val_accuracy: 0.7995\n",
            "Epoch 117/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6281 - accuracy: 0.7623\n",
            "Epoch 117: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6282 - accuracy: 0.7622 - val_loss: 0.5538 - val_accuracy: 0.8080\n",
            "Epoch 118/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6303 - accuracy: 0.7634\n",
            "Epoch 118: val_accuracy did not improve from 0.80973\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6301 - accuracy: 0.7636 - val_loss: 0.5565 - val_accuracy: 0.8029\n",
            "Epoch 119/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6268 - accuracy: 0.7589\n",
            "Epoch 119: val_accuracy improved from 0.80973 to 0.81058, saving model to best_vader.h5\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.6268 - accuracy: 0.7589 - val_loss: 0.5520 - val_accuracy: 0.8106\n",
            "Epoch 120/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6294 - accuracy: 0.7629\n",
            "Epoch 120: val_accuracy did not improve from 0.81058\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6294 - accuracy: 0.7629 - val_loss: 0.5545 - val_accuracy: 0.8020\n",
            "Epoch 121/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6265 - accuracy: 0.7657\n",
            "Epoch 121: val_accuracy did not improve from 0.81058\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6265 - accuracy: 0.7657 - val_loss: 0.5571 - val_accuracy: 0.8038\n",
            "Epoch 122/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6284 - accuracy: 0.7593\n",
            "Epoch 122: val_accuracy did not improve from 0.81058\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6280 - accuracy: 0.7596 - val_loss: 0.5617 - val_accuracy: 0.8038\n",
            "Epoch 123/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6299 - accuracy: 0.7659\n",
            "Epoch 123: val_accuracy did not improve from 0.81058\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6297 - accuracy: 0.7660 - val_loss: 0.5611 - val_accuracy: 0.8020\n",
            "Epoch 124/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6210 - accuracy: 0.7669\n",
            "Epoch 124: val_accuracy did not improve from 0.81058\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6210 - accuracy: 0.7669 - val_loss: 0.5561 - val_accuracy: 0.8080\n",
            "Epoch 125/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6195 - accuracy: 0.7630\n",
            "Epoch 125: val_accuracy did not improve from 0.81058\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6195 - accuracy: 0.7630 - val_loss: 0.5622 - val_accuracy: 0.8106\n",
            "Epoch 126/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6269 - accuracy: 0.7617\n",
            "Epoch 126: val_accuracy did not improve from 0.81058\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6271 - accuracy: 0.7617 - val_loss: 0.5576 - val_accuracy: 0.8097\n",
            "Epoch 127/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6192 - accuracy: 0.7687\n",
            "Epoch 127: val_accuracy did not improve from 0.81058\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6193 - accuracy: 0.7685 - val_loss: 0.5580 - val_accuracy: 0.8089\n",
            "Epoch 128/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6299 - accuracy: 0.7649\n",
            "Epoch 128: val_accuracy did not improve from 0.81058\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.6299 - accuracy: 0.7649 - val_loss: 0.5497 - val_accuracy: 0.8097\n",
            "Epoch 129/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6144 - accuracy: 0.7725\n",
            "Epoch 129: val_accuracy did not improve from 0.81058\n",
            "330/330 [==============================] - 13s 38ms/step - loss: 0.6144 - accuracy: 0.7725 - val_loss: 0.5470 - val_accuracy: 0.8097\n",
            "Epoch 130/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6246 - accuracy: 0.7620\n",
            "Epoch 130: val_accuracy improved from 0.81058 to 0.81485, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6248 - accuracy: 0.7619 - val_loss: 0.5481 - val_accuracy: 0.8148\n",
            "Epoch 131/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6288 - accuracy: 0.7646\n",
            "Epoch 131: val_accuracy did not improve from 0.81485\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6289 - accuracy: 0.7646 - val_loss: 0.5527 - val_accuracy: 0.8131\n",
            "Epoch 132/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6229 - accuracy: 0.7643\n",
            "Epoch 132: val_accuracy did not improve from 0.81485\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6227 - accuracy: 0.7645 - val_loss: 0.5544 - val_accuracy: 0.8089\n",
            "Epoch 133/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6188 - accuracy: 0.7690\n",
            "Epoch 133: val_accuracy did not improve from 0.81485\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6188 - accuracy: 0.7690 - val_loss: 0.5551 - val_accuracy: 0.8114\n",
            "Epoch 134/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6263 - accuracy: 0.7651\n",
            "Epoch 134: val_accuracy did not improve from 0.81485\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6263 - accuracy: 0.7651 - val_loss: 0.5518 - val_accuracy: 0.8106\n",
            "Epoch 135/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6316 - accuracy: 0.7615\n",
            "Epoch 135: val_accuracy did not improve from 0.81485\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6316 - accuracy: 0.7615 - val_loss: 0.5517 - val_accuracy: 0.8123\n",
            "Epoch 136/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6193 - accuracy: 0.7709\n",
            "Epoch 136: val_accuracy did not improve from 0.81485\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6191 - accuracy: 0.7710 - val_loss: 0.5522 - val_accuracy: 0.8106\n",
            "Epoch 137/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6175 - accuracy: 0.7698\n",
            "Epoch 137: val_accuracy did not improve from 0.81485\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6178 - accuracy: 0.7695 - val_loss: 0.5564 - val_accuracy: 0.8097\n",
            "Epoch 138/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6213 - accuracy: 0.7676\n",
            "Epoch 138: val_accuracy did not improve from 0.81485\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6213 - accuracy: 0.7676 - val_loss: 0.5604 - val_accuracy: 0.8106\n",
            "Epoch 139/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6196 - accuracy: 0.7640\n",
            "Epoch 139: val_accuracy did not improve from 0.81485\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6196 - accuracy: 0.7640 - val_loss: 0.5508 - val_accuracy: 0.8131\n",
            "Epoch 140/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6233 - accuracy: 0.7635\n",
            "Epoch 140: val_accuracy did not improve from 0.81485\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6233 - accuracy: 0.7635 - val_loss: 0.5509 - val_accuracy: 0.8080\n",
            "Epoch 141/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6165 - accuracy: 0.7652\n",
            "Epoch 141: val_accuracy did not improve from 0.81485\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6166 - accuracy: 0.7650 - val_loss: 0.5482 - val_accuracy: 0.8089\n",
            "Epoch 142/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6209 - accuracy: 0.7674\n",
            "Epoch 142: val_accuracy improved from 0.81485 to 0.81570, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6209 - accuracy: 0.7674 - val_loss: 0.5423 - val_accuracy: 0.8157\n",
            "Epoch 143/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6156 - accuracy: 0.7682\n",
            "Epoch 143: val_accuracy did not improve from 0.81570\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6163 - accuracy: 0.7679 - val_loss: 0.5531 - val_accuracy: 0.8080\n",
            "Epoch 144/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6270 - accuracy: 0.7646\n",
            "Epoch 144: val_accuracy did not improve from 0.81570\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6269 - accuracy: 0.7645 - val_loss: 0.5536 - val_accuracy: 0.8114\n",
            "Epoch 145/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6222 - accuracy: 0.7680\n",
            "Epoch 145: val_accuracy did not improve from 0.81570\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6222 - accuracy: 0.7680 - val_loss: 0.5408 - val_accuracy: 0.8140\n",
            "Epoch 146/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6232 - accuracy: 0.7639\n",
            "Epoch 146: val_accuracy did not improve from 0.81570\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6232 - accuracy: 0.7639 - val_loss: 0.5474 - val_accuracy: 0.8123\n",
            "Epoch 147/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6200 - accuracy: 0.7714\n",
            "Epoch 147: val_accuracy did not improve from 0.81570\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6200 - accuracy: 0.7714 - val_loss: 0.5494 - val_accuracy: 0.8072\n",
            "Epoch 148/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6224 - accuracy: 0.7651\n",
            "Epoch 148: val_accuracy did not improve from 0.81570\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6224 - accuracy: 0.7651 - val_loss: 0.5535 - val_accuracy: 0.8072\n",
            "Epoch 149/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6249 - accuracy: 0.7623\n",
            "Epoch 149: val_accuracy did not improve from 0.81570\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6251 - accuracy: 0.7621 - val_loss: 0.5594 - val_accuracy: 0.8020\n",
            "Epoch 150/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6086 - accuracy: 0.7735\n",
            "Epoch 150: val_accuracy did not improve from 0.81570\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6086 - accuracy: 0.7735 - val_loss: 0.5494 - val_accuracy: 0.8131\n",
            "Epoch 151/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6242 - accuracy: 0.7664\n",
            "Epoch 151: val_accuracy did not improve from 0.81570\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6242 - accuracy: 0.7664 - val_loss: 0.5535 - val_accuracy: 0.8089\n",
            "Epoch 152/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6288 - accuracy: 0.7645\n",
            "Epoch 152: val_accuracy did not improve from 0.81570\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6288 - accuracy: 0.7645 - val_loss: 0.5491 - val_accuracy: 0.8072\n",
            "Epoch 153/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6096 - accuracy: 0.7705\n",
            "Epoch 153: val_accuracy improved from 0.81570 to 0.81741, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6096 - accuracy: 0.7705 - val_loss: 0.5452 - val_accuracy: 0.8174\n",
            "Epoch 154/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6176 - accuracy: 0.7683\n",
            "Epoch 154: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6176 - accuracy: 0.7683 - val_loss: 0.5522 - val_accuracy: 0.8157\n",
            "Epoch 155/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6205 - accuracy: 0.7701\n",
            "Epoch 155: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6205 - accuracy: 0.7701 - val_loss: 0.5427 - val_accuracy: 0.8123\n",
            "Epoch 156/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6240 - accuracy: 0.7650\n",
            "Epoch 156: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6240 - accuracy: 0.7650 - val_loss: 0.5447 - val_accuracy: 0.8114\n",
            "Epoch 157/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6125 - accuracy: 0.7675\n",
            "Epoch 157: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6123 - accuracy: 0.7676 - val_loss: 0.5547 - val_accuracy: 0.8089\n",
            "Epoch 158/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6093 - accuracy: 0.7727\n",
            "Epoch 158: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6092 - accuracy: 0.7728 - val_loss: 0.5426 - val_accuracy: 0.8174\n",
            "Epoch 159/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6221 - accuracy: 0.7681\n",
            "Epoch 159: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6221 - accuracy: 0.7681 - val_loss: 0.5478 - val_accuracy: 0.8148\n",
            "Epoch 160/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6138 - accuracy: 0.7710\n",
            "Epoch 160: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6138 - accuracy: 0.7710 - val_loss: 0.5544 - val_accuracy: 0.8089\n",
            "Epoch 161/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6225 - accuracy: 0.7666\n",
            "Epoch 161: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6226 - accuracy: 0.7665 - val_loss: 0.5448 - val_accuracy: 0.8114\n",
            "Epoch 162/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6136 - accuracy: 0.7688\n",
            "Epoch 162: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6136 - accuracy: 0.7688 - val_loss: 0.5495 - val_accuracy: 0.8114\n",
            "Epoch 163/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6141 - accuracy: 0.7720\n",
            "Epoch 163: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6141 - accuracy: 0.7720 - val_loss: 0.5547 - val_accuracy: 0.8106\n",
            "Epoch 164/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6029 - accuracy: 0.7755\n",
            "Epoch 164: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6028 - accuracy: 0.7754 - val_loss: 0.5511 - val_accuracy: 0.8097\n",
            "Epoch 165/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6240 - accuracy: 0.7659\n",
            "Epoch 165: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6240 - accuracy: 0.7659 - val_loss: 0.5471 - val_accuracy: 0.8123\n",
            "Epoch 166/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6174 - accuracy: 0.7669\n",
            "Epoch 166: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6174 - accuracy: 0.7668 - val_loss: 0.5576 - val_accuracy: 0.8106\n",
            "Epoch 167/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6164 - accuracy: 0.7669\n",
            "Epoch 167: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6165 - accuracy: 0.7668 - val_loss: 0.5432 - val_accuracy: 0.8106\n",
            "Epoch 168/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6110 - accuracy: 0.7691\n",
            "Epoch 168: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6106 - accuracy: 0.7693 - val_loss: 0.5458 - val_accuracy: 0.8148\n",
            "Epoch 169/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6256 - accuracy: 0.7669\n",
            "Epoch 169: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6256 - accuracy: 0.7669 - val_loss: 0.5501 - val_accuracy: 0.8106\n",
            "Epoch 170/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6118 - accuracy: 0.7731\n",
            "Epoch 170: val_accuracy did not improve from 0.81741\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6118 - accuracy: 0.7731 - val_loss: 0.5457 - val_accuracy: 0.8114\n",
            "Epoch 171/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6130 - accuracy: 0.7704\n",
            "Epoch 171: val_accuracy improved from 0.81741 to 0.82253, saving model to best_vader.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6130 - accuracy: 0.7704 - val_loss: 0.5492 - val_accuracy: 0.8225\n",
            "Epoch 172/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6285 - accuracy: 0.7701\n",
            "Epoch 172: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6285 - accuracy: 0.7701 - val_loss: 0.5440 - val_accuracy: 0.8131\n",
            "Epoch 173/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6142 - accuracy: 0.7756\n",
            "Epoch 173: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6140 - accuracy: 0.7756 - val_loss: 0.5505 - val_accuracy: 0.8106\n",
            "Epoch 174/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6227 - accuracy: 0.7644\n",
            "Epoch 174: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6229 - accuracy: 0.7643 - val_loss: 0.5432 - val_accuracy: 0.8174\n",
            "Epoch 175/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6166 - accuracy: 0.7739\n",
            "Epoch 175: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6165 - accuracy: 0.7739 - val_loss: 0.5423 - val_accuracy: 0.8131\n",
            "Epoch 176/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6157 - accuracy: 0.7687\n",
            "Epoch 176: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6156 - accuracy: 0.7688 - val_loss: 0.5481 - val_accuracy: 0.8131\n",
            "Epoch 177/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6070 - accuracy: 0.7766\n",
            "Epoch 177: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6067 - accuracy: 0.7768 - val_loss: 0.5452 - val_accuracy: 0.8157\n",
            "Epoch 178/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6143 - accuracy: 0.7676\n",
            "Epoch 178: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6143 - accuracy: 0.7676 - val_loss: 0.5456 - val_accuracy: 0.8123\n",
            "Epoch 179/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6133 - accuracy: 0.7689\n",
            "Epoch 179: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6133 - accuracy: 0.7689 - val_loss: 0.5446 - val_accuracy: 0.8157\n",
            "Epoch 180/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6093 - accuracy: 0.7737\n",
            "Epoch 180: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6092 - accuracy: 0.7735 - val_loss: 0.5411 - val_accuracy: 0.8166\n",
            "Epoch 181/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6087 - accuracy: 0.7722\n",
            "Epoch 181: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6088 - accuracy: 0.7721 - val_loss: 0.5348 - val_accuracy: 0.8217\n",
            "Epoch 182/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6159 - accuracy: 0.7709\n",
            "Epoch 182: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6159 - accuracy: 0.7709 - val_loss: 0.5423 - val_accuracy: 0.8131\n",
            "Epoch 183/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6215 - accuracy: 0.7622\n",
            "Epoch 183: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6215 - accuracy: 0.7622 - val_loss: 0.5397 - val_accuracy: 0.8183\n",
            "Epoch 184/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6158 - accuracy: 0.7694\n",
            "Epoch 184: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6158 - accuracy: 0.7694 - val_loss: 0.5468 - val_accuracy: 0.8097\n",
            "Epoch 185/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6140 - accuracy: 0.7738\n",
            "Epoch 185: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.6140 - accuracy: 0.7739 - val_loss: 0.5402 - val_accuracy: 0.8114\n",
            "Epoch 186/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6220 - accuracy: 0.7691\n",
            "Epoch 186: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6220 - accuracy: 0.7691 - val_loss: 0.5453 - val_accuracy: 0.8131\n",
            "Epoch 187/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6132 - accuracy: 0.7727\n",
            "Epoch 187: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6144 - accuracy: 0.7727 - val_loss: 0.5450 - val_accuracy: 0.8131\n",
            "Epoch 188/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6089 - accuracy: 0.7749\n",
            "Epoch 188: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6089 - accuracy: 0.7749 - val_loss: 0.5384 - val_accuracy: 0.8157\n",
            "Epoch 189/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6056 - accuracy: 0.7739\n",
            "Epoch 189: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6052 - accuracy: 0.7740 - val_loss: 0.5479 - val_accuracy: 0.8114\n",
            "Epoch 190/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6126 - accuracy: 0.7750\n",
            "Epoch 190: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6126 - accuracy: 0.7750 - val_loss: 0.5421 - val_accuracy: 0.8080\n",
            "Epoch 191/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6108 - accuracy: 0.7766\n",
            "Epoch 191: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6108 - accuracy: 0.7767 - val_loss: 0.5450 - val_accuracy: 0.8157\n",
            "Epoch 192/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6099 - accuracy: 0.7748\n",
            "Epoch 192: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6097 - accuracy: 0.7749 - val_loss: 0.5416 - val_accuracy: 0.8157\n",
            "Epoch 193/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6158 - accuracy: 0.7674\n",
            "Epoch 193: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6158 - accuracy: 0.7674 - val_loss: 0.5479 - val_accuracy: 0.8131\n",
            "Epoch 194/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6111 - accuracy: 0.7750\n",
            "Epoch 194: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6115 - accuracy: 0.7748 - val_loss: 0.5382 - val_accuracy: 0.8148\n",
            "Epoch 195/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6064 - accuracy: 0.7796\n",
            "Epoch 195: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6064 - accuracy: 0.7796 - val_loss: 0.5365 - val_accuracy: 0.8131\n",
            "Epoch 196/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6110 - accuracy: 0.7760\n",
            "Epoch 196: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6110 - accuracy: 0.7760 - val_loss: 0.5513 - val_accuracy: 0.8080\n",
            "Epoch 197/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6095 - accuracy: 0.7732\n",
            "Epoch 197: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6093 - accuracy: 0.7732 - val_loss: 0.5453 - val_accuracy: 0.8097\n",
            "Epoch 198/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6122 - accuracy: 0.7733\n",
            "Epoch 198: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6122 - accuracy: 0.7733 - val_loss: 0.5377 - val_accuracy: 0.8123\n",
            "Epoch 199/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6045 - accuracy: 0.7760\n",
            "Epoch 199: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6045 - accuracy: 0.7761 - val_loss: 0.5422 - val_accuracy: 0.8140\n",
            "Epoch 200/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6138 - accuracy: 0.7733\n",
            "Epoch 200: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6140 - accuracy: 0.7732 - val_loss: 0.5465 - val_accuracy: 0.8114\n"
          ]
        }
      ],
      "source": [
        "hist_vader = model_vader.fit(X_train_pad, y_train_vader, \n",
        "                 batch_size=batch_size,\n",
        "                 epochs=epochs,\n",
        "                 validation_split=0.1,\n",
        "                 callbacks=[mc_vader])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJDujAZLxykw",
        "outputId": "3bff4e62-cd9c-4569-9cd9-4d64a67cd906"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(hist_vader.history['accuracy'])\n",
        "plt.plot(hist_vader.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# \"Loss\"\n",
        "plt.plot(hist_vader.history['loss'])\n",
        "plt.plot(hist_vader.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcKlNQt0xykw",
        "outputId": "949e892e-be1f-4c03-d90d-0d3a47a81008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92/92 [==============================] - 1s 7ms/step\n",
            "Accuracy: 54.27%\n",
            "\n",
            "F1 Score: 54.27\n"
          ]
        }
      ],
      "source": [
        "predictions_vader = model_vader.predict(X_test_pad)\n",
        "predictions_vader = np.argmax(predictions_vader, axis=1)\n",
        "predictions_vader = [class_names[pred] for pred in predictions_vader]\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(test_df.label, predictions_vader) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions_vader, average='micro') * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGcDaGg8xykw",
        "outputId": "2f84261b-b9b3-4445-f4b1-a1308fe06150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7183 - accuracy: 0.7280\n",
            "Epoch 1: val_accuracy improved from -inf to 0.80119, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.7183 - accuracy: 0.7280 - val_loss: 0.5555 - val_accuracy: 0.8012\n",
            "Epoch 2/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6891 - accuracy: 0.7367\n",
            "Epoch 2: val_accuracy did not improve from 0.80119\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6890 - accuracy: 0.7367 - val_loss: 0.5491 - val_accuracy: 0.8003\n",
            "Epoch 3/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6812 - accuracy: 0.7374\n",
            "Epoch 3: val_accuracy did not improve from 0.80119\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6812 - accuracy: 0.7374 - val_loss: 0.5399 - val_accuracy: 0.8003\n",
            "Epoch 4/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6756 - accuracy: 0.7396\n",
            "Epoch 4: val_accuracy did not improve from 0.80119\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6758 - accuracy: 0.7395 - val_loss: 0.5285 - val_accuracy: 0.7986\n",
            "Epoch 5/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6610 - accuracy: 0.7459\n",
            "Epoch 5: val_accuracy did not improve from 0.80119\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6611 - accuracy: 0.7457 - val_loss: 0.5248 - val_accuracy: 0.8012\n",
            "Epoch 6/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6626 - accuracy: 0.7456\n",
            "Epoch 6: val_accuracy improved from 0.80119 to 0.80205, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6626 - accuracy: 0.7455 - val_loss: 0.5200 - val_accuracy: 0.8020\n",
            "Epoch 7/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6513 - accuracy: 0.7553\n",
            "Epoch 7: val_accuracy improved from 0.80205 to 0.80375, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6514 - accuracy: 0.7554 - val_loss: 0.5202 - val_accuracy: 0.8038\n",
            "Epoch 8/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6571 - accuracy: 0.7540\n",
            "Epoch 8: val_accuracy improved from 0.80375 to 0.81143, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6572 - accuracy: 0.7540 - val_loss: 0.5161 - val_accuracy: 0.8114\n",
            "Epoch 9/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6507 - accuracy: 0.7569\n",
            "Epoch 9: val_accuracy did not improve from 0.81143\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6506 - accuracy: 0.7569 - val_loss: 0.5131 - val_accuracy: 0.8114\n",
            "Epoch 10/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6501 - accuracy: 0.7553\n",
            "Epoch 10: val_accuracy did not improve from 0.81143\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6503 - accuracy: 0.7552 - val_loss: 0.5079 - val_accuracy: 0.8106\n",
            "Epoch 11/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6503 - accuracy: 0.7565\n",
            "Epoch 11: val_accuracy improved from 0.81143 to 0.81229, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6506 - accuracy: 0.7562 - val_loss: 0.5076 - val_accuracy: 0.8123\n",
            "Epoch 12/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6435 - accuracy: 0.7595\n",
            "Epoch 12: val_accuracy improved from 0.81229 to 0.81570, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6434 - accuracy: 0.7596 - val_loss: 0.5017 - val_accuracy: 0.8157\n",
            "Epoch 13/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6442 - accuracy: 0.7588\n",
            "Epoch 13: val_accuracy improved from 0.81570 to 0.81911, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6442 - accuracy: 0.7587 - val_loss: 0.5044 - val_accuracy: 0.8191\n",
            "Epoch 14/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6324 - accuracy: 0.7654\n",
            "Epoch 14: val_accuracy did not improve from 0.81911\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6319 - accuracy: 0.7657 - val_loss: 0.5006 - val_accuracy: 0.8183\n",
            "Epoch 15/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6275 - accuracy: 0.7661\n",
            "Epoch 15: val_accuracy did not improve from 0.81911\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6275 - accuracy: 0.7661 - val_loss: 0.5006 - val_accuracy: 0.8080\n",
            "Epoch 16/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6341 - accuracy: 0.7563\n",
            "Epoch 16: val_accuracy did not improve from 0.81911\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6341 - accuracy: 0.7563 - val_loss: 0.5045 - val_accuracy: 0.8106\n",
            "Epoch 17/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6343 - accuracy: 0.7605\n",
            "Epoch 17: val_accuracy did not improve from 0.81911\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6343 - accuracy: 0.7605 - val_loss: 0.4996 - val_accuracy: 0.8183\n",
            "Epoch 18/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6326 - accuracy: 0.7665\n",
            "Epoch 18: val_accuracy did not improve from 0.81911\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6326 - accuracy: 0.7665 - val_loss: 0.4952 - val_accuracy: 0.8174\n",
            "Epoch 19/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6244 - accuracy: 0.7686\n",
            "Epoch 19: val_accuracy did not improve from 0.81911\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6244 - accuracy: 0.7686 - val_loss: 0.4968 - val_accuracy: 0.8157\n",
            "Epoch 20/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6340 - accuracy: 0.7605\n",
            "Epoch 20: val_accuracy improved from 0.81911 to 0.81997, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6340 - accuracy: 0.7606 - val_loss: 0.4975 - val_accuracy: 0.8200\n",
            "Epoch 21/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6311 - accuracy: 0.7612\n",
            "Epoch 21: val_accuracy did not improve from 0.81997\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6311 - accuracy: 0.7612 - val_loss: 0.4991 - val_accuracy: 0.8174\n",
            "Epoch 22/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6159 - accuracy: 0.7691\n",
            "Epoch 22: val_accuracy improved from 0.81997 to 0.82253, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6159 - accuracy: 0.7692 - val_loss: 0.4889 - val_accuracy: 0.8225\n",
            "Epoch 23/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6248 - accuracy: 0.7666\n",
            "Epoch 23: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6245 - accuracy: 0.7667 - val_loss: 0.4908 - val_accuracy: 0.8174\n",
            "Epoch 24/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6232 - accuracy: 0.7642\n",
            "Epoch 24: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6232 - accuracy: 0.7641 - val_loss: 0.4880 - val_accuracy: 0.8174\n",
            "Epoch 25/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6192 - accuracy: 0.7660\n",
            "Epoch 25: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6189 - accuracy: 0.7660 - val_loss: 0.4899 - val_accuracy: 0.8208\n",
            "Epoch 26/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6179 - accuracy: 0.7669\n",
            "Epoch 26: val_accuracy did not improve from 0.82253\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6175 - accuracy: 0.7671 - val_loss: 0.4835 - val_accuracy: 0.8225\n",
            "Epoch 27/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6231 - accuracy: 0.7618\n",
            "Epoch 27: val_accuracy improved from 0.82253 to 0.82423, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6229 - accuracy: 0.7619 - val_loss: 0.4886 - val_accuracy: 0.8242\n",
            "Epoch 28/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6232 - accuracy: 0.7629\n",
            "Epoch 28: val_accuracy improved from 0.82423 to 0.82850, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6233 - accuracy: 0.7628 - val_loss: 0.4861 - val_accuracy: 0.8285\n",
            "Epoch 29/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6150 - accuracy: 0.7691\n",
            "Epoch 29: val_accuracy improved from 0.82850 to 0.83020, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6154 - accuracy: 0.7689 - val_loss: 0.4824 - val_accuracy: 0.8302\n",
            "Epoch 30/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6102 - accuracy: 0.7717\n",
            "Epoch 30: val_accuracy did not improve from 0.83020\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6101 - accuracy: 0.7716 - val_loss: 0.4801 - val_accuracy: 0.8276\n",
            "Epoch 31/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6202 - accuracy: 0.7661\n",
            "Epoch 31: val_accuracy did not improve from 0.83020\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6202 - accuracy: 0.7661 - val_loss: 0.4853 - val_accuracy: 0.8285\n",
            "Epoch 32/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6163 - accuracy: 0.7683\n",
            "Epoch 32: val_accuracy did not improve from 0.83020\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6164 - accuracy: 0.7684 - val_loss: 0.4820 - val_accuracy: 0.8276\n",
            "Epoch 33/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6163 - accuracy: 0.7663\n",
            "Epoch 33: val_accuracy improved from 0.83020 to 0.83362, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6163 - accuracy: 0.7663 - val_loss: 0.4770 - val_accuracy: 0.8336\n",
            "Epoch 34/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6175 - accuracy: 0.7684\n",
            "Epoch 34: val_accuracy did not improve from 0.83362\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6175 - accuracy: 0.7684 - val_loss: 0.4798 - val_accuracy: 0.8285\n",
            "Epoch 35/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6072 - accuracy: 0.7753\n",
            "Epoch 35: val_accuracy did not improve from 0.83362\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6072 - accuracy: 0.7753 - val_loss: 0.4810 - val_accuracy: 0.8311\n",
            "Epoch 36/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6150 - accuracy: 0.7714\n",
            "Epoch 36: val_accuracy did not improve from 0.83362\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6161 - accuracy: 0.7711 - val_loss: 0.4849 - val_accuracy: 0.8311\n",
            "Epoch 37/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6019 - accuracy: 0.7712\n",
            "Epoch 37: val_accuracy did not improve from 0.83362\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6019 - accuracy: 0.7712 - val_loss: 0.4779 - val_accuracy: 0.8319\n",
            "Epoch 38/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6141 - accuracy: 0.7748\n",
            "Epoch 38: val_accuracy did not improve from 0.83362\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6141 - accuracy: 0.7748 - val_loss: 0.4813 - val_accuracy: 0.8302\n",
            "Epoch 39/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6190 - accuracy: 0.7658\n",
            "Epoch 39: val_accuracy did not improve from 0.83362\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6192 - accuracy: 0.7656 - val_loss: 0.4790 - val_accuracy: 0.8311\n",
            "Epoch 40/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5982 - accuracy: 0.7713\n",
            "Epoch 40: val_accuracy did not improve from 0.83362\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5980 - accuracy: 0.7714 - val_loss: 0.4755 - val_accuracy: 0.8328\n",
            "Epoch 41/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6117 - accuracy: 0.7673\n",
            "Epoch 41: val_accuracy did not improve from 0.83362\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6118 - accuracy: 0.7675 - val_loss: 0.4773 - val_accuracy: 0.8302\n",
            "Epoch 42/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6086 - accuracy: 0.7699\n",
            "Epoch 42: val_accuracy improved from 0.83362 to 0.83618, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6087 - accuracy: 0.7699 - val_loss: 0.4770 - val_accuracy: 0.8362\n",
            "Epoch 43/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6114 - accuracy: 0.7699\n",
            "Epoch 43: val_accuracy did not improve from 0.83618\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6114 - accuracy: 0.7699 - val_loss: 0.4740 - val_accuracy: 0.8311\n",
            "Epoch 44/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5991 - accuracy: 0.7784\n",
            "Epoch 44: val_accuracy did not improve from 0.83618\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5991 - accuracy: 0.7784 - val_loss: 0.4712 - val_accuracy: 0.8362\n",
            "Epoch 45/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6028 - accuracy: 0.7729\n",
            "Epoch 45: val_accuracy did not improve from 0.83618\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6026 - accuracy: 0.7730 - val_loss: 0.4755 - val_accuracy: 0.8336\n",
            "Epoch 46/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6019 - accuracy: 0.7751\n",
            "Epoch 46: val_accuracy did not improve from 0.83618\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6019 - accuracy: 0.7751 - val_loss: 0.4719 - val_accuracy: 0.8353\n",
            "Epoch 47/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6011 - accuracy: 0.7765\n",
            "Epoch 47: val_accuracy did not improve from 0.83618\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6020 - accuracy: 0.7764 - val_loss: 0.4733 - val_accuracy: 0.8353\n",
            "Epoch 48/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6037 - accuracy: 0.7743\n",
            "Epoch 48: val_accuracy did not improve from 0.83618\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6037 - accuracy: 0.7742 - val_loss: 0.4784 - val_accuracy: 0.8328\n",
            "Epoch 49/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6076 - accuracy: 0.7796\n",
            "Epoch 49: val_accuracy did not improve from 0.83618\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6076 - accuracy: 0.7797 - val_loss: 0.4714 - val_accuracy: 0.8353\n",
            "Epoch 50/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6027 - accuracy: 0.7750\n",
            "Epoch 50: val_accuracy did not improve from 0.83618\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6029 - accuracy: 0.7749 - val_loss: 0.4777 - val_accuracy: 0.8319\n",
            "Epoch 51/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6031 - accuracy: 0.7718\n",
            "Epoch 51: val_accuracy did not improve from 0.83618\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6030 - accuracy: 0.7717 - val_loss: 0.4761 - val_accuracy: 0.8328\n",
            "Epoch 52/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5939 - accuracy: 0.7825\n",
            "Epoch 52: val_accuracy did not improve from 0.83618\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5941 - accuracy: 0.7824 - val_loss: 0.4732 - val_accuracy: 0.8362\n",
            "Epoch 53/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6055 - accuracy: 0.7714\n",
            "Epoch 53: val_accuracy improved from 0.83618 to 0.83703, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6055 - accuracy: 0.7714 - val_loss: 0.4756 - val_accuracy: 0.8370\n",
            "Epoch 54/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5980 - accuracy: 0.7757\n",
            "Epoch 54: val_accuracy did not improve from 0.83703\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.5980 - accuracy: 0.7758 - val_loss: 0.4682 - val_accuracy: 0.8353\n",
            "Epoch 55/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6008 - accuracy: 0.7703\n",
            "Epoch 55: val_accuracy did not improve from 0.83703\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6007 - accuracy: 0.7703 - val_loss: 0.4697 - val_accuracy: 0.8319\n",
            "Epoch 56/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5961 - accuracy: 0.7770\n",
            "Epoch 56: val_accuracy did not improve from 0.83703\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5961 - accuracy: 0.7770 - val_loss: 0.4686 - val_accuracy: 0.8370\n",
            "Epoch 57/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6005 - accuracy: 0.7737\n",
            "Epoch 57: val_accuracy did not improve from 0.83703\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6003 - accuracy: 0.7737 - val_loss: 0.4649 - val_accuracy: 0.8370\n",
            "Epoch 58/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6029 - accuracy: 0.7749\n",
            "Epoch 58: val_accuracy did not improve from 0.83703\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6030 - accuracy: 0.7750 - val_loss: 0.4721 - val_accuracy: 0.8362\n",
            "Epoch 59/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5972 - accuracy: 0.7808\n",
            "Epoch 59: val_accuracy did not improve from 0.83703\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.5979 - accuracy: 0.7805 - val_loss: 0.4657 - val_accuracy: 0.8370\n",
            "Epoch 60/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6034 - accuracy: 0.7788\n",
            "Epoch 60: val_accuracy did not improve from 0.83703\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.6034 - accuracy: 0.7788 - val_loss: 0.4704 - val_accuracy: 0.8353\n",
            "Epoch 61/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5922 - accuracy: 0.7830\n",
            "Epoch 61: val_accuracy did not improve from 0.83703\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.5926 - accuracy: 0.7830 - val_loss: 0.4691 - val_accuracy: 0.8353\n",
            "Epoch 62/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5921 - accuracy: 0.7762\n",
            "Epoch 62: val_accuracy improved from 0.83703 to 0.83874, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5922 - accuracy: 0.7761 - val_loss: 0.4645 - val_accuracy: 0.8387\n",
            "Epoch 63/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6030 - accuracy: 0.7802\n",
            "Epoch 63: val_accuracy did not improve from 0.83874\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6032 - accuracy: 0.7799 - val_loss: 0.4670 - val_accuracy: 0.8353\n",
            "Epoch 64/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6022 - accuracy: 0.7783\n",
            "Epoch 64: val_accuracy did not improve from 0.83874\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6020 - accuracy: 0.7785 - val_loss: 0.4679 - val_accuracy: 0.8336\n",
            "Epoch 65/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5900 - accuracy: 0.7802\n",
            "Epoch 65: val_accuracy did not improve from 0.83874\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5900 - accuracy: 0.7802 - val_loss: 0.4677 - val_accuracy: 0.8353\n",
            "Epoch 66/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5891 - accuracy: 0.7775\n",
            "Epoch 66: val_accuracy did not improve from 0.83874\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5890 - accuracy: 0.7774 - val_loss: 0.4712 - val_accuracy: 0.8362\n",
            "Epoch 67/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5891 - accuracy: 0.7784\n",
            "Epoch 67: val_accuracy did not improve from 0.83874\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5889 - accuracy: 0.7785 - val_loss: 0.4685 - val_accuracy: 0.8387\n",
            "Epoch 68/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5951 - accuracy: 0.7732\n",
            "Epoch 68: val_accuracy did not improve from 0.83874\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5948 - accuracy: 0.7734 - val_loss: 0.4673 - val_accuracy: 0.8379\n",
            "Epoch 69/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6004 - accuracy: 0.7720\n",
            "Epoch 69: val_accuracy did not improve from 0.83874\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6006 - accuracy: 0.7719 - val_loss: 0.4709 - val_accuracy: 0.8345\n",
            "Epoch 70/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5932 - accuracy: 0.7770\n",
            "Epoch 70: val_accuracy did not improve from 0.83874\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5932 - accuracy: 0.7769 - val_loss: 0.4656 - val_accuracy: 0.8319\n",
            "Epoch 71/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5877 - accuracy: 0.7808\n",
            "Epoch 71: val_accuracy did not improve from 0.83874\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5876 - accuracy: 0.7808 - val_loss: 0.4655 - val_accuracy: 0.8345\n",
            "Epoch 72/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5871 - accuracy: 0.7836\n",
            "Epoch 72: val_accuracy did not improve from 0.83874\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5871 - accuracy: 0.7836 - val_loss: 0.4678 - val_accuracy: 0.8362\n",
            "Epoch 73/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5886 - accuracy: 0.7794\n",
            "Epoch 73: val_accuracy did not improve from 0.83874\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5894 - accuracy: 0.7791 - val_loss: 0.4708 - val_accuracy: 0.8353\n",
            "Epoch 74/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5933 - accuracy: 0.7794\n",
            "Epoch 74: val_accuracy did not improve from 0.83874\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5932 - accuracy: 0.7796 - val_loss: 0.4679 - val_accuracy: 0.8370\n",
            "Epoch 75/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6020 - accuracy: 0.7727\n",
            "Epoch 75: val_accuracy did not improve from 0.83874\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6020 - accuracy: 0.7727 - val_loss: 0.4657 - val_accuracy: 0.8336\n",
            "Epoch 76/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5798 - accuracy: 0.7828\n",
            "Epoch 76: val_accuracy did not improve from 0.83874\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5798 - accuracy: 0.7828 - val_loss: 0.4680 - val_accuracy: 0.8362\n",
            "Epoch 77/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5783 - accuracy: 0.7818\n",
            "Epoch 77: val_accuracy did not improve from 0.83874\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5780 - accuracy: 0.7820 - val_loss: 0.4663 - val_accuracy: 0.8387\n",
            "Epoch 78/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5989 - accuracy: 0.7737\n",
            "Epoch 78: val_accuracy improved from 0.83874 to 0.84044, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5988 - accuracy: 0.7737 - val_loss: 0.4656 - val_accuracy: 0.8404\n",
            "Epoch 79/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5932 - accuracy: 0.7774\n",
            "Epoch 79: val_accuracy did not improve from 0.84044\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5932 - accuracy: 0.7774 - val_loss: 0.4712 - val_accuracy: 0.8311\n",
            "Epoch 80/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5890 - accuracy: 0.7783\n",
            "Epoch 80: val_accuracy did not improve from 0.84044\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.5891 - accuracy: 0.7783 - val_loss: 0.4640 - val_accuracy: 0.8379\n",
            "Epoch 81/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5821 - accuracy: 0.7770\n",
            "Epoch 81: val_accuracy did not improve from 0.84044\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5819 - accuracy: 0.7770 - val_loss: 0.4639 - val_accuracy: 0.8353\n",
            "Epoch 82/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5914 - accuracy: 0.7793\n",
            "Epoch 82: val_accuracy improved from 0.84044 to 0.84130, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5912 - accuracy: 0.7795 - val_loss: 0.4623 - val_accuracy: 0.8413\n",
            "Epoch 83/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5900 - accuracy: 0.7846\n",
            "Epoch 83: val_accuracy did not improve from 0.84130\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5901 - accuracy: 0.7847 - val_loss: 0.4619 - val_accuracy: 0.8336\n",
            "Epoch 84/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5830 - accuracy: 0.7854\n",
            "Epoch 84: val_accuracy improved from 0.84130 to 0.84215, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5830 - accuracy: 0.7854 - val_loss: 0.4564 - val_accuracy: 0.8422\n",
            "Epoch 85/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5747 - accuracy: 0.7826\n",
            "Epoch 85: val_accuracy did not improve from 0.84215\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5747 - accuracy: 0.7826 - val_loss: 0.4559 - val_accuracy: 0.8404\n",
            "Epoch 86/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5858 - accuracy: 0.7806\n",
            "Epoch 86: val_accuracy improved from 0.84215 to 0.84300, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5858 - accuracy: 0.7806 - val_loss: 0.4569 - val_accuracy: 0.8430\n",
            "Epoch 87/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5864 - accuracy: 0.7782\n",
            "Epoch 87: val_accuracy improved from 0.84300 to 0.84727, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5864 - accuracy: 0.7782 - val_loss: 0.4568 - val_accuracy: 0.8473\n",
            "Epoch 88/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5902 - accuracy: 0.7805\n",
            "Epoch 88: val_accuracy did not improve from 0.84727\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5902 - accuracy: 0.7805 - val_loss: 0.4623 - val_accuracy: 0.8336\n",
            "Epoch 89/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5813 - accuracy: 0.7810\n",
            "Epoch 89: val_accuracy did not improve from 0.84727\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5813 - accuracy: 0.7809 - val_loss: 0.4565 - val_accuracy: 0.8396\n",
            "Epoch 90/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5908 - accuracy: 0.7789\n",
            "Epoch 90: val_accuracy did not improve from 0.84727\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5908 - accuracy: 0.7788 - val_loss: 0.4605 - val_accuracy: 0.8430\n",
            "Epoch 91/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5858 - accuracy: 0.7872\n",
            "Epoch 91: val_accuracy did not improve from 0.84727\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5857 - accuracy: 0.7871 - val_loss: 0.4584 - val_accuracy: 0.8422\n",
            "Epoch 92/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5817 - accuracy: 0.7855\n",
            "Epoch 92: val_accuracy did not improve from 0.84727\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5813 - accuracy: 0.7858 - val_loss: 0.4638 - val_accuracy: 0.8430\n",
            "Epoch 93/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5799 - accuracy: 0.7867\n",
            "Epoch 93: val_accuracy did not improve from 0.84727\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5799 - accuracy: 0.7867 - val_loss: 0.4527 - val_accuracy: 0.8447\n",
            "Epoch 94/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5905 - accuracy: 0.7812\n",
            "Epoch 94: val_accuracy did not improve from 0.84727\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5904 - accuracy: 0.7812 - val_loss: 0.4527 - val_accuracy: 0.8404\n",
            "Epoch 95/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5782 - accuracy: 0.7827\n",
            "Epoch 95: val_accuracy did not improve from 0.84727\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5781 - accuracy: 0.7827 - val_loss: 0.4538 - val_accuracy: 0.8430\n",
            "Epoch 96/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5859 - accuracy: 0.7787\n",
            "Epoch 96: val_accuracy did not improve from 0.84727\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5859 - accuracy: 0.7787 - val_loss: 0.4558 - val_accuracy: 0.8396\n",
            "Epoch 97/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5789 - accuracy: 0.7858\n",
            "Epoch 97: val_accuracy did not improve from 0.84727\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5791 - accuracy: 0.7857 - val_loss: 0.4517 - val_accuracy: 0.8430\n",
            "Epoch 98/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5773 - accuracy: 0.7787\n",
            "Epoch 98: val_accuracy did not improve from 0.84727\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5773 - accuracy: 0.7787 - val_loss: 0.4538 - val_accuracy: 0.8422\n",
            "Epoch 99/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5869 - accuracy: 0.7809\n",
            "Epoch 99: val_accuracy did not improve from 0.84727\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5869 - accuracy: 0.7809 - val_loss: 0.4488 - val_accuracy: 0.8456\n",
            "Epoch 100/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5733 - accuracy: 0.7891\n",
            "Epoch 100: val_accuracy did not improve from 0.84727\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5733 - accuracy: 0.7891 - val_loss: 0.4519 - val_accuracy: 0.8422\n",
            "Epoch 101/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5851 - accuracy: 0.7808\n",
            "Epoch 101: val_accuracy did not improve from 0.84727\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5851 - accuracy: 0.7808 - val_loss: 0.4515 - val_accuracy: 0.8473\n",
            "Epoch 102/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5719 - accuracy: 0.7863\n",
            "Epoch 102: val_accuracy did not improve from 0.84727\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5720 - accuracy: 0.7861 - val_loss: 0.4538 - val_accuracy: 0.8447\n",
            "Epoch 103/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5785 - accuracy: 0.7846\n",
            "Epoch 103: val_accuracy improved from 0.84727 to 0.84983, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5787 - accuracy: 0.7845 - val_loss: 0.4521 - val_accuracy: 0.8498\n",
            "Epoch 104/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5807 - accuracy: 0.7825\n",
            "Epoch 104: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5805 - accuracy: 0.7825 - val_loss: 0.4555 - val_accuracy: 0.8439\n",
            "Epoch 105/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5721 - accuracy: 0.7840\n",
            "Epoch 105: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5721 - accuracy: 0.7840 - val_loss: 0.4575 - val_accuracy: 0.8370\n",
            "Epoch 106/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5762 - accuracy: 0.7852\n",
            "Epoch 106: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5761 - accuracy: 0.7854 - val_loss: 0.4519 - val_accuracy: 0.8404\n",
            "Epoch 107/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5804 - accuracy: 0.7807\n",
            "Epoch 107: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5804 - accuracy: 0.7807 - val_loss: 0.4542 - val_accuracy: 0.8413\n",
            "Epoch 108/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5802 - accuracy: 0.7831\n",
            "Epoch 108: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5801 - accuracy: 0.7832 - val_loss: 0.4481 - val_accuracy: 0.8439\n",
            "Epoch 109/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5838 - accuracy: 0.7757\n",
            "Epoch 109: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5838 - accuracy: 0.7758 - val_loss: 0.4509 - val_accuracy: 0.8422\n",
            "Epoch 110/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5794 - accuracy: 0.7832\n",
            "Epoch 110: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5794 - accuracy: 0.7832 - val_loss: 0.4492 - val_accuracy: 0.8422\n",
            "Epoch 111/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5739 - accuracy: 0.7861\n",
            "Epoch 111: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5739 - accuracy: 0.7861 - val_loss: 0.4422 - val_accuracy: 0.8447\n",
            "Epoch 112/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5758 - accuracy: 0.7892\n",
            "Epoch 112: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5760 - accuracy: 0.7891 - val_loss: 0.4477 - val_accuracy: 0.8439\n",
            "Epoch 113/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5831 - accuracy: 0.7836\n",
            "Epoch 113: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5830 - accuracy: 0.7836 - val_loss: 0.4504 - val_accuracy: 0.8447\n",
            "Epoch 114/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5786 - accuracy: 0.7842\n",
            "Epoch 114: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5786 - accuracy: 0.7842 - val_loss: 0.4462 - val_accuracy: 0.8473\n",
            "Epoch 115/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5829 - accuracy: 0.7794\n",
            "Epoch 115: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5827 - accuracy: 0.7794 - val_loss: 0.4531 - val_accuracy: 0.8387\n",
            "Epoch 116/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5800 - accuracy: 0.7806\n",
            "Epoch 116: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5800 - accuracy: 0.7806 - val_loss: 0.4532 - val_accuracy: 0.8413\n",
            "Epoch 117/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5901 - accuracy: 0.7782\n",
            "Epoch 117: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5902 - accuracy: 0.7780 - val_loss: 0.4582 - val_accuracy: 0.8430\n",
            "Epoch 118/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5719 - accuracy: 0.7872\n",
            "Epoch 118: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5718 - accuracy: 0.7873 - val_loss: 0.4554 - val_accuracy: 0.8413\n",
            "Epoch 119/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5809 - accuracy: 0.7848\n",
            "Epoch 119: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5809 - accuracy: 0.7848 - val_loss: 0.4532 - val_accuracy: 0.8404\n",
            "Epoch 120/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5712 - accuracy: 0.7836\n",
            "Epoch 120: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5716 - accuracy: 0.7834 - val_loss: 0.4514 - val_accuracy: 0.8422\n",
            "Epoch 121/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5859 - accuracy: 0.7805\n",
            "Epoch 121: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5859 - accuracy: 0.7805 - val_loss: 0.4496 - val_accuracy: 0.8404\n",
            "Epoch 122/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5761 - accuracy: 0.7841\n",
            "Epoch 122: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5759 - accuracy: 0.7842 - val_loss: 0.4536 - val_accuracy: 0.8396\n",
            "Epoch 123/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5747 - accuracy: 0.7856\n",
            "Epoch 123: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5751 - accuracy: 0.7856 - val_loss: 0.4485 - val_accuracy: 0.8413\n",
            "Epoch 124/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5810 - accuracy: 0.7863\n",
            "Epoch 124: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5815 - accuracy: 0.7862 - val_loss: 0.4510 - val_accuracy: 0.8396\n",
            "Epoch 125/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5797 - accuracy: 0.7787\n",
            "Epoch 125: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5798 - accuracy: 0.7787 - val_loss: 0.4477 - val_accuracy: 0.8370\n",
            "Epoch 126/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5808 - accuracy: 0.7815\n",
            "Epoch 126: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5811 - accuracy: 0.7814 - val_loss: 0.4438 - val_accuracy: 0.8473\n",
            "Epoch 127/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5683 - accuracy: 0.7879\n",
            "Epoch 127: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5683 - accuracy: 0.7879 - val_loss: 0.4474 - val_accuracy: 0.8464\n",
            "Epoch 128/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5782 - accuracy: 0.7868\n",
            "Epoch 128: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5782 - accuracy: 0.7868 - val_loss: 0.4508 - val_accuracy: 0.8456\n",
            "Epoch 129/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5645 - accuracy: 0.7862\n",
            "Epoch 129: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5645 - accuracy: 0.7862 - val_loss: 0.4484 - val_accuracy: 0.8464\n",
            "Epoch 130/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5823 - accuracy: 0.7857\n",
            "Epoch 130: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5824 - accuracy: 0.7856 - val_loss: 0.4504 - val_accuracy: 0.8413\n",
            "Epoch 131/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5738 - accuracy: 0.7843\n",
            "Epoch 131: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5739 - accuracy: 0.7843 - val_loss: 0.4522 - val_accuracy: 0.8422\n",
            "Epoch 132/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5751 - accuracy: 0.7849\n",
            "Epoch 132: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5749 - accuracy: 0.7851 - val_loss: 0.4523 - val_accuracy: 0.8396\n",
            "Epoch 133/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5800 - accuracy: 0.7819\n",
            "Epoch 133: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5800 - accuracy: 0.7820 - val_loss: 0.4494 - val_accuracy: 0.8422\n",
            "Epoch 134/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5747 - accuracy: 0.7866\n",
            "Epoch 134: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5746 - accuracy: 0.7865 - val_loss: 0.4490 - val_accuracy: 0.8473\n",
            "Epoch 135/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5770 - accuracy: 0.7870\n",
            "Epoch 135: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5768 - accuracy: 0.7872 - val_loss: 0.4483 - val_accuracy: 0.8490\n",
            "Epoch 136/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5793 - accuracy: 0.7830\n",
            "Epoch 136: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5791 - accuracy: 0.7829 - val_loss: 0.4495 - val_accuracy: 0.8456\n",
            "Epoch 137/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5828 - accuracy: 0.7805\n",
            "Epoch 137: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5828 - accuracy: 0.7805 - val_loss: 0.4484 - val_accuracy: 0.8456\n",
            "Epoch 138/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5761 - accuracy: 0.7826\n",
            "Epoch 138: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5762 - accuracy: 0.7826 - val_loss: 0.4475 - val_accuracy: 0.8456\n",
            "Epoch 139/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5721 - accuracy: 0.7877\n",
            "Epoch 139: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5721 - accuracy: 0.7877 - val_loss: 0.4514 - val_accuracy: 0.8404\n",
            "Epoch 140/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5737 - accuracy: 0.7917\n",
            "Epoch 140: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5737 - accuracy: 0.7917 - val_loss: 0.4459 - val_accuracy: 0.8430\n",
            "Epoch 141/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5793 - accuracy: 0.7798\n",
            "Epoch 141: val_accuracy did not improve from 0.84983\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5791 - accuracy: 0.7799 - val_loss: 0.4409 - val_accuracy: 0.8430\n",
            "Epoch 142/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5746 - accuracy: 0.7822\n",
            "Epoch 142: val_accuracy improved from 0.84983 to 0.85239, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5746 - accuracy: 0.7822 - val_loss: 0.4387 - val_accuracy: 0.8524\n",
            "Epoch 143/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5781 - accuracy: 0.7805\n",
            "Epoch 143: val_accuracy did not improve from 0.85239\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5782 - accuracy: 0.7805 - val_loss: 0.4453 - val_accuracy: 0.8430\n",
            "Epoch 144/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5751 - accuracy: 0.7848\n",
            "Epoch 144: val_accuracy did not improve from 0.85239\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5752 - accuracy: 0.7846 - val_loss: 0.4442 - val_accuracy: 0.8464\n",
            "Epoch 145/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5779 - accuracy: 0.7820\n",
            "Epoch 145: val_accuracy did not improve from 0.85239\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5779 - accuracy: 0.7820 - val_loss: 0.4448 - val_accuracy: 0.8430\n",
            "Epoch 146/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5736 - accuracy: 0.7874\n",
            "Epoch 146: val_accuracy did not improve from 0.85239\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5736 - accuracy: 0.7874 - val_loss: 0.4446 - val_accuracy: 0.8439\n",
            "Epoch 147/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5753 - accuracy: 0.7867\n",
            "Epoch 147: val_accuracy improved from 0.85239 to 0.85324, saving model to best_afinn.h5\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5753 - accuracy: 0.7867 - val_loss: 0.4413 - val_accuracy: 0.8532\n",
            "Epoch 148/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5733 - accuracy: 0.7906\n",
            "Epoch 148: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5731 - accuracy: 0.7906 - val_loss: 0.4487 - val_accuracy: 0.8430\n",
            "Epoch 149/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5813 - accuracy: 0.7856\n",
            "Epoch 149: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5813 - accuracy: 0.7856 - val_loss: 0.4449 - val_accuracy: 0.8439\n",
            "Epoch 150/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5797 - accuracy: 0.7785\n",
            "Epoch 150: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5800 - accuracy: 0.7784 - val_loss: 0.4434 - val_accuracy: 0.8456\n",
            "Epoch 151/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5798 - accuracy: 0.7825\n",
            "Epoch 151: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5798 - accuracy: 0.7824 - val_loss: 0.4415 - val_accuracy: 0.8464\n",
            "Epoch 152/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5833 - accuracy: 0.7800\n",
            "Epoch 152: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 12s 36ms/step - loss: 0.5833 - accuracy: 0.7800 - val_loss: 0.4466 - val_accuracy: 0.8490\n",
            "Epoch 153/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5705 - accuracy: 0.7853\n",
            "Epoch 153: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5704 - accuracy: 0.7853 - val_loss: 0.4436 - val_accuracy: 0.8498\n",
            "Epoch 154/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5782 - accuracy: 0.7831\n",
            "Epoch 154: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5778 - accuracy: 0.7833 - val_loss: 0.4385 - val_accuracy: 0.8447\n",
            "Epoch 155/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5733 - accuracy: 0.7874\n",
            "Epoch 155: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5733 - accuracy: 0.7874 - val_loss: 0.4445 - val_accuracy: 0.8396\n",
            "Epoch 156/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5669 - accuracy: 0.7834\n",
            "Epoch 156: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5669 - accuracy: 0.7834 - val_loss: 0.4443 - val_accuracy: 0.8422\n",
            "Epoch 157/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5649 - accuracy: 0.7868\n",
            "Epoch 157: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5649 - accuracy: 0.7868 - val_loss: 0.4451 - val_accuracy: 0.8507\n",
            "Epoch 158/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5658 - accuracy: 0.7898\n",
            "Epoch 158: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5658 - accuracy: 0.7898 - val_loss: 0.4382 - val_accuracy: 0.8481\n",
            "Epoch 159/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7899\n",
            "Epoch 159: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5611 - accuracy: 0.7900 - val_loss: 0.4399 - val_accuracy: 0.8464\n",
            "Epoch 160/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5680 - accuracy: 0.7835\n",
            "Epoch 160: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5680 - accuracy: 0.7835 - val_loss: 0.4436 - val_accuracy: 0.8404\n",
            "Epoch 161/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5744 - accuracy: 0.7878\n",
            "Epoch 161: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5744 - accuracy: 0.7878 - val_loss: 0.4453 - val_accuracy: 0.8456\n",
            "Epoch 162/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5681 - accuracy: 0.7830\n",
            "Epoch 162: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5681 - accuracy: 0.7829 - val_loss: 0.4432 - val_accuracy: 0.8413\n",
            "Epoch 163/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5712 - accuracy: 0.7877\n",
            "Epoch 163: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5711 - accuracy: 0.7878 - val_loss: 0.4482 - val_accuracy: 0.8430\n",
            "Epoch 164/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5764 - accuracy: 0.7827\n",
            "Epoch 164: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5759 - accuracy: 0.7829 - val_loss: 0.4414 - val_accuracy: 0.8439\n",
            "Epoch 165/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5783 - accuracy: 0.7850\n",
            "Epoch 165: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5783 - accuracy: 0.7850 - val_loss: 0.4466 - val_accuracy: 0.8413\n",
            "Epoch 166/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5708 - accuracy: 0.7892\n",
            "Epoch 166: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5708 - accuracy: 0.7892 - val_loss: 0.4396 - val_accuracy: 0.8413\n",
            "Epoch 167/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5710 - accuracy: 0.7859\n",
            "Epoch 167: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5710 - accuracy: 0.7859 - val_loss: 0.4407 - val_accuracy: 0.8430\n",
            "Epoch 168/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5572 - accuracy: 0.7943\n",
            "Epoch 168: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5568 - accuracy: 0.7945 - val_loss: 0.4431 - val_accuracy: 0.8447\n",
            "Epoch 169/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5624 - accuracy: 0.7907\n",
            "Epoch 169: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5624 - accuracy: 0.7907 - val_loss: 0.4408 - val_accuracy: 0.8447\n",
            "Epoch 170/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5605 - accuracy: 0.7899\n",
            "Epoch 170: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5602 - accuracy: 0.7900 - val_loss: 0.4389 - val_accuracy: 0.8456\n",
            "Epoch 171/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5645 - accuracy: 0.7886\n",
            "Epoch 171: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5643 - accuracy: 0.7887 - val_loss: 0.4429 - val_accuracy: 0.8481\n",
            "Epoch 172/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5701 - accuracy: 0.7901\n",
            "Epoch 172: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5701 - accuracy: 0.7901 - val_loss: 0.4358 - val_accuracy: 0.8473\n",
            "Epoch 173/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5551 - accuracy: 0.7913\n",
            "Epoch 173: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5551 - accuracy: 0.7913 - val_loss: 0.4411 - val_accuracy: 0.8447\n",
            "Epoch 174/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5683 - accuracy: 0.7892\n",
            "Epoch 174: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5683 - accuracy: 0.7892 - val_loss: 0.4395 - val_accuracy: 0.8456\n",
            "Epoch 175/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5634 - accuracy: 0.7888\n",
            "Epoch 175: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5634 - accuracy: 0.7888 - val_loss: 0.4430 - val_accuracy: 0.8447\n",
            "Epoch 176/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5571 - accuracy: 0.7898\n",
            "Epoch 176: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5571 - accuracy: 0.7898 - val_loss: 0.4406 - val_accuracy: 0.8422\n",
            "Epoch 177/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5693 - accuracy: 0.7907\n",
            "Epoch 177: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5696 - accuracy: 0.7906 - val_loss: 0.4409 - val_accuracy: 0.8430\n",
            "Epoch 178/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7896\n",
            "Epoch 178: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5611 - accuracy: 0.7898 - val_loss: 0.4406 - val_accuracy: 0.8490\n",
            "Epoch 179/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5816 - accuracy: 0.7815\n",
            "Epoch 179: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5818 - accuracy: 0.7813 - val_loss: 0.4411 - val_accuracy: 0.8430\n",
            "Epoch 180/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5688 - accuracy: 0.7841\n",
            "Epoch 180: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5688 - accuracy: 0.7841 - val_loss: 0.4440 - val_accuracy: 0.8464\n",
            "Epoch 181/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7853\n",
            "Epoch 181: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5699 - accuracy: 0.7854 - val_loss: 0.4400 - val_accuracy: 0.8464\n",
            "Epoch 182/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5707 - accuracy: 0.7849\n",
            "Epoch 182: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5704 - accuracy: 0.7849 - val_loss: 0.4411 - val_accuracy: 0.8439\n",
            "Epoch 183/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5673 - accuracy: 0.7913\n",
            "Epoch 183: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5673 - accuracy: 0.7913 - val_loss: 0.4409 - val_accuracy: 0.8473\n",
            "Epoch 184/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5569 - accuracy: 0.7901\n",
            "Epoch 184: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5578 - accuracy: 0.7898 - val_loss: 0.4420 - val_accuracy: 0.8447\n",
            "Epoch 185/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5753 - accuracy: 0.7839\n",
            "Epoch 185: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5753 - accuracy: 0.7839 - val_loss: 0.4422 - val_accuracy: 0.8447\n",
            "Epoch 186/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5681 - accuracy: 0.7849\n",
            "Epoch 186: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5681 - accuracy: 0.7849 - val_loss: 0.4396 - val_accuracy: 0.8456\n",
            "Epoch 187/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5633 - accuracy: 0.7891\n",
            "Epoch 187: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5633 - accuracy: 0.7891 - val_loss: 0.4389 - val_accuracy: 0.8413\n",
            "Epoch 188/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5673 - accuracy: 0.7911\n",
            "Epoch 188: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5670 - accuracy: 0.7914 - val_loss: 0.4432 - val_accuracy: 0.8456\n",
            "Epoch 189/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5673 - accuracy: 0.7839\n",
            "Epoch 189: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5673 - accuracy: 0.7839 - val_loss: 0.4452 - val_accuracy: 0.8456\n",
            "Epoch 190/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5757 - accuracy: 0.7808\n",
            "Epoch 190: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5755 - accuracy: 0.7809 - val_loss: 0.4466 - val_accuracy: 0.8456\n",
            "Epoch 191/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5646 - accuracy: 0.7902\n",
            "Epoch 191: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5645 - accuracy: 0.7902 - val_loss: 0.4459 - val_accuracy: 0.8439\n",
            "Epoch 192/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5735 - accuracy: 0.7892\n",
            "Epoch 192: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5736 - accuracy: 0.7891 - val_loss: 0.4446 - val_accuracy: 0.8439\n",
            "Epoch 193/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5667 - accuracy: 0.7901\n",
            "Epoch 193: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5669 - accuracy: 0.7901 - val_loss: 0.4443 - val_accuracy: 0.8447\n",
            "Epoch 194/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7874\n",
            "Epoch 194: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5608 - accuracy: 0.7874 - val_loss: 0.4465 - val_accuracy: 0.8498\n",
            "Epoch 195/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5703 - accuracy: 0.7852\n",
            "Epoch 195: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5703 - accuracy: 0.7852 - val_loss: 0.4418 - val_accuracy: 0.8430\n",
            "Epoch 196/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5637 - accuracy: 0.7908\n",
            "Epoch 196: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5636 - accuracy: 0.7909 - val_loss: 0.4433 - val_accuracy: 0.8413\n",
            "Epoch 197/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7877\n",
            "Epoch 197: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5622 - accuracy: 0.7876 - val_loss: 0.4421 - val_accuracy: 0.8481\n",
            "Epoch 198/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7880\n",
            "Epoch 198: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5637 - accuracy: 0.7881 - val_loss: 0.4449 - val_accuracy: 0.8456\n",
            "Epoch 199/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5671 - accuracy: 0.7885\n",
            "Epoch 199: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5673 - accuracy: 0.7881 - val_loss: 0.4436 - val_accuracy: 0.8464\n",
            "Epoch 200/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5712 - accuracy: 0.7880\n",
            "Epoch 200: val_accuracy did not improve from 0.85324\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5717 - accuracy: 0.7879 - val_loss: 0.4417 - val_accuracy: 0.8481\n"
          ]
        }
      ],
      "source": [
        "hist_afinn = model_afinn.fit(X_train_pad, y_train_afinn, \n",
        "                 batch_size=batch_size,\n",
        "                 epochs=epochs,\n",
        "                 validation_split=0.1,\n",
        "                 callbacks=[mc_afinn])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlDCdUd7xykx",
        "outputId": "6dd40d87-fc81-466a-c1bd-eb6974d325fe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(hist_afinn.history['accuracy'])\n",
        "plt.plot(hist_afinn.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# \"Loss\"\n",
        "plt.plot(hist_afinn.history['loss'])\n",
        "plt.plot(hist_afinn.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1HScY0cxykx",
        "outputId": "326a7a35-ef89-406b-9015-0fbd2ceacac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92/92 [==============================] - 1s 8ms/step\n",
            "Accuracy: 55.50%\n",
            "\n",
            "F1 Score: 55.50\n"
          ]
        }
      ],
      "source": [
        "predictions_afinn = model_afinn.predict(X_test_pad)\n",
        "predictions_afinn = np.argmax(predictions_afinn, axis=1)\n",
        "predictions_afinn = [class_names[pred] for pred in predictions_afinn]\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(test_df.label, predictions_afinn) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions_afinn, average='micro') * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwTkw6-nxykx",
        "outputId": "7549d44a-7334-4416-c009-d52882f33444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 1.0795 - accuracy: 0.4774\n",
            "Epoch 1: val_accuracy improved from -inf to 0.52730, saving model to best_swn.h5\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 1.0795 - accuracy: 0.4774 - val_loss: 0.9802 - val_accuracy: 0.5273\n",
            "Epoch 2/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 1.0210 - accuracy: 0.4973\n",
            "Epoch 2: val_accuracy improved from 0.52730 to 0.53328, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 1.0209 - accuracy: 0.4975 - val_loss: 0.9725 - val_accuracy: 0.5333\n",
            "Epoch 3/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 1.0129 - accuracy: 0.5056\n",
            "Epoch 3: val_accuracy improved from 0.53328 to 0.53413, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 1.0132 - accuracy: 0.5053 - val_loss: 0.9697 - val_accuracy: 0.5341\n",
            "Epoch 4/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 1.0041 - accuracy: 0.5112\n",
            "Epoch 4: val_accuracy improved from 0.53413 to 0.53925, saving model to best_swn.h5\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 1.0041 - accuracy: 0.5110 - val_loss: 0.9652 - val_accuracy: 0.5392\n",
            "Epoch 5/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 1.0028 - accuracy: 0.5129\n",
            "Epoch 5: val_accuracy improved from 0.53925 to 0.54863, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 1.0028 - accuracy: 0.5130 - val_loss: 0.9598 - val_accuracy: 0.5486\n",
            "Epoch 6/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.9990 - accuracy: 0.5231\n",
            "Epoch 6: val_accuracy did not improve from 0.54863\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.9990 - accuracy: 0.5231 - val_loss: 0.9580 - val_accuracy: 0.5435\n",
            "Epoch 7/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.9904 - accuracy: 0.5253\n",
            "Epoch 7: val_accuracy improved from 0.54863 to 0.55119, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.9903 - accuracy: 0.5254 - val_loss: 0.9530 - val_accuracy: 0.5512\n",
            "Epoch 8/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.9847 - accuracy: 0.5297\n",
            "Epoch 8: val_accuracy did not improve from 0.55119\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.9847 - accuracy: 0.5297 - val_loss: 0.9465 - val_accuracy: 0.5495\n",
            "Epoch 9/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.9837 - accuracy: 0.5324\n",
            "Epoch 9: val_accuracy did not improve from 0.55119\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.9835 - accuracy: 0.5326 - val_loss: 0.9417 - val_accuracy: 0.5478\n",
            "Epoch 10/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.9785 - accuracy: 0.5325\n",
            "Epoch 10: val_accuracy improved from 0.55119 to 0.56399, saving model to best_swn.h5\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.9785 - accuracy: 0.5325 - val_loss: 0.9404 - val_accuracy: 0.5640\n",
            "Epoch 11/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.9704 - accuracy: 0.5375\n",
            "Epoch 11: val_accuracy improved from 0.56399 to 0.56911, saving model to best_swn.h5\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.9704 - accuracy: 0.5375 - val_loss: 0.9342 - val_accuracy: 0.5691\n",
            "Epoch 12/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.9668 - accuracy: 0.5408\n",
            "Epoch 12: val_accuracy did not improve from 0.56911\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.9668 - accuracy: 0.5408 - val_loss: 0.9247 - val_accuracy: 0.5674\n",
            "Epoch 13/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.9644 - accuracy: 0.5477\n",
            "Epoch 13: val_accuracy did not improve from 0.56911\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.9644 - accuracy: 0.5476 - val_loss: 0.9206 - val_accuracy: 0.5631\n",
            "Epoch 14/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.9599 - accuracy: 0.5525\n",
            "Epoch 14: val_accuracy improved from 0.56911 to 0.58191, saving model to best_swn.h5\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.9599 - accuracy: 0.5525 - val_loss: 0.9152 - val_accuracy: 0.5819\n",
            "Epoch 15/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.9570 - accuracy: 0.5619\n",
            "Epoch 15: val_accuracy improved from 0.58191 to 0.59044, saving model to best_swn.h5\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.9569 - accuracy: 0.5620 - val_loss: 0.9124 - val_accuracy: 0.5904\n",
            "Epoch 16/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.9491 - accuracy: 0.5631\n",
            "Epoch 16: val_accuracy improved from 0.59044 to 0.59983, saving model to best_swn.h5\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.9491 - accuracy: 0.5631 - val_loss: 0.9012 - val_accuracy: 0.5998\n",
            "Epoch 17/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.9348 - accuracy: 0.5659\n",
            "Epoch 17: val_accuracy did not improve from 0.59983\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.9348 - accuracy: 0.5657 - val_loss: 0.8981 - val_accuracy: 0.5990\n",
            "Epoch 18/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.9426 - accuracy: 0.5600\n",
            "Epoch 18: val_accuracy did not improve from 0.59983\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.9426 - accuracy: 0.5601 - val_loss: 0.8941 - val_accuracy: 0.5981\n",
            "Epoch 19/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.9304 - accuracy: 0.5677\n",
            "Epoch 19: val_accuracy improved from 0.59983 to 0.61007, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.9306 - accuracy: 0.5676 - val_loss: 0.8778 - val_accuracy: 0.6101\n",
            "Epoch 20/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.9191 - accuracy: 0.5800\n",
            "Epoch 20: val_accuracy did not improve from 0.61007\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.9191 - accuracy: 0.5800 - val_loss: 0.8765 - val_accuracy: 0.6007\n",
            "Epoch 21/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.9231 - accuracy: 0.5722\n",
            "Epoch 21: val_accuracy did not improve from 0.61007\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.9230 - accuracy: 0.5720 - val_loss: 0.8752 - val_accuracy: 0.6049\n",
            "Epoch 22/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.9226 - accuracy: 0.5749\n",
            "Epoch 22: val_accuracy did not improve from 0.61007\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.9226 - accuracy: 0.5749 - val_loss: 0.8748 - val_accuracy: 0.6075\n",
            "Epoch 23/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.9179 - accuracy: 0.5809\n",
            "Epoch 23: val_accuracy improved from 0.61007 to 0.61177, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.9179 - accuracy: 0.5809 - val_loss: 0.8677 - val_accuracy: 0.6118\n",
            "Epoch 24/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.9055 - accuracy: 0.5867\n",
            "Epoch 24: val_accuracy improved from 0.61177 to 0.61945, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.9055 - accuracy: 0.5867 - val_loss: 0.8615 - val_accuracy: 0.6195\n",
            "Epoch 25/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.9048 - accuracy: 0.5920\n",
            "Epoch 25: val_accuracy did not improve from 0.61945\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.9048 - accuracy: 0.5920 - val_loss: 0.8629 - val_accuracy: 0.6135\n",
            "Epoch 26/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.9000 - accuracy: 0.5962\n",
            "Epoch 26: val_accuracy improved from 0.61945 to 0.63140, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.9000 - accuracy: 0.5964 - val_loss: 0.8531 - val_accuracy: 0.6314\n",
            "Epoch 27/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8986 - accuracy: 0.5985\n",
            "Epoch 27: val_accuracy did not improve from 0.63140\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8984 - accuracy: 0.5985 - val_loss: 0.8501 - val_accuracy: 0.6288\n",
            "Epoch 28/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8960 - accuracy: 0.5990\n",
            "Epoch 28: val_accuracy did not improve from 0.63140\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8960 - accuracy: 0.5989 - val_loss: 0.8490 - val_accuracy: 0.6271\n",
            "Epoch 29/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8965 - accuracy: 0.5970\n",
            "Epoch 29: val_accuracy did not improve from 0.63140\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8965 - accuracy: 0.5970 - val_loss: 0.8509 - val_accuracy: 0.6220\n",
            "Epoch 30/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8953 - accuracy: 0.5975\n",
            "Epoch 30: val_accuracy improved from 0.63140 to 0.63481, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.8953 - accuracy: 0.5975 - val_loss: 0.8425 - val_accuracy: 0.6348\n",
            "Epoch 31/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8923 - accuracy: 0.5987\n",
            "Epoch 31: val_accuracy did not improve from 0.63481\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8923 - accuracy: 0.5987 - val_loss: 0.8455 - val_accuracy: 0.6314\n",
            "Epoch 32/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8891 - accuracy: 0.6039\n",
            "Epoch 32: val_accuracy improved from 0.63481 to 0.63993, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8890 - accuracy: 0.6040 - val_loss: 0.8388 - val_accuracy: 0.6399\n",
            "Epoch 33/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8899 - accuracy: 0.6073\n",
            "Epoch 33: val_accuracy did not improve from 0.63993\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8899 - accuracy: 0.6074 - val_loss: 0.8444 - val_accuracy: 0.6357\n",
            "Epoch 34/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8882 - accuracy: 0.6126\n",
            "Epoch 34: val_accuracy did not improve from 0.63993\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8882 - accuracy: 0.6126 - val_loss: 0.8396 - val_accuracy: 0.6297\n",
            "Epoch 35/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8893 - accuracy: 0.6082\n",
            "Epoch 35: val_accuracy improved from 0.63993 to 0.64164, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8893 - accuracy: 0.6082 - val_loss: 0.8324 - val_accuracy: 0.6416\n",
            "Epoch 36/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8838 - accuracy: 0.6114\n",
            "Epoch 36: val_accuracy improved from 0.64164 to 0.64761, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8839 - accuracy: 0.6114 - val_loss: 0.8341 - val_accuracy: 0.6476\n",
            "Epoch 37/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8844 - accuracy: 0.6120\n",
            "Epoch 37: val_accuracy did not improve from 0.64761\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8845 - accuracy: 0.6120 - val_loss: 0.8331 - val_accuracy: 0.6459\n",
            "Epoch 38/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8794 - accuracy: 0.6115\n",
            "Epoch 38: val_accuracy did not improve from 0.64761\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8794 - accuracy: 0.6115 - val_loss: 0.8315 - val_accuracy: 0.6365\n",
            "Epoch 39/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8792 - accuracy: 0.6169\n",
            "Epoch 39: val_accuracy did not improve from 0.64761\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8792 - accuracy: 0.6169 - val_loss: 0.8252 - val_accuracy: 0.6442\n",
            "Epoch 40/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8758 - accuracy: 0.6167\n",
            "Epoch 40: val_accuracy did not improve from 0.64761\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8758 - accuracy: 0.6167 - val_loss: 0.8247 - val_accuracy: 0.6433\n",
            "Epoch 41/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8721 - accuracy: 0.6126\n",
            "Epoch 41: val_accuracy did not improve from 0.64761\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8721 - accuracy: 0.6124 - val_loss: 0.8244 - val_accuracy: 0.6442\n",
            "Epoch 42/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8757 - accuracy: 0.6177\n",
            "Epoch 42: val_accuracy did not improve from 0.64761\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8756 - accuracy: 0.6176 - val_loss: 0.8235 - val_accuracy: 0.6374\n",
            "Epoch 43/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8733 - accuracy: 0.6185\n",
            "Epoch 43: val_accuracy improved from 0.64761 to 0.65017, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8737 - accuracy: 0.6183 - val_loss: 0.8202 - val_accuracy: 0.6502\n",
            "Epoch 44/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8715 - accuracy: 0.6246\n",
            "Epoch 44: val_accuracy did not improve from 0.65017\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8715 - accuracy: 0.6246 - val_loss: 0.8196 - val_accuracy: 0.6502\n",
            "Epoch 45/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8698 - accuracy: 0.6166\n",
            "Epoch 45: val_accuracy did not improve from 0.65017\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8698 - accuracy: 0.6166 - val_loss: 0.8176 - val_accuracy: 0.6502\n",
            "Epoch 46/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8657 - accuracy: 0.6199\n",
            "Epoch 46: val_accuracy did not improve from 0.65017\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8657 - accuracy: 0.6199 - val_loss: 0.8184 - val_accuracy: 0.6476\n",
            "Epoch 47/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8664 - accuracy: 0.6213\n",
            "Epoch 47: val_accuracy improved from 0.65017 to 0.65870, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8665 - accuracy: 0.6211 - val_loss: 0.8116 - val_accuracy: 0.6587\n",
            "Epoch 48/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8671 - accuracy: 0.6235\n",
            "Epoch 48: val_accuracy improved from 0.65870 to 0.66553, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8671 - accuracy: 0.6235 - val_loss: 0.8092 - val_accuracy: 0.6655\n",
            "Epoch 49/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8663 - accuracy: 0.6261\n",
            "Epoch 49: val_accuracy did not improve from 0.66553\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8663 - accuracy: 0.6261 - val_loss: 0.8150 - val_accuracy: 0.6493\n",
            "Epoch 50/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8588 - accuracy: 0.6298\n",
            "Epoch 50: val_accuracy did not improve from 0.66553\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8588 - accuracy: 0.6298 - val_loss: 0.8122 - val_accuracy: 0.6510\n",
            "Epoch 51/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8527 - accuracy: 0.6295\n",
            "Epoch 51: val_accuracy did not improve from 0.66553\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8525 - accuracy: 0.6296 - val_loss: 0.8070 - val_accuracy: 0.6570\n",
            "Epoch 52/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8656 - accuracy: 0.6239\n",
            "Epoch 52: val_accuracy did not improve from 0.66553\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8658 - accuracy: 0.6236 - val_loss: 0.8010 - val_accuracy: 0.6655\n",
            "Epoch 53/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8528 - accuracy: 0.6256\n",
            "Epoch 53: val_accuracy did not improve from 0.66553\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8528 - accuracy: 0.6256 - val_loss: 0.8054 - val_accuracy: 0.6630\n",
            "Epoch 54/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8529 - accuracy: 0.6282\n",
            "Epoch 54: val_accuracy did not improve from 0.66553\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8531 - accuracy: 0.6282 - val_loss: 0.8027 - val_accuracy: 0.6613\n",
            "Epoch 55/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8494 - accuracy: 0.6342\n",
            "Epoch 55: val_accuracy improved from 0.66553 to 0.67150, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8493 - accuracy: 0.6343 - val_loss: 0.7941 - val_accuracy: 0.6715\n",
            "Epoch 56/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8543 - accuracy: 0.6309\n",
            "Epoch 56: val_accuracy did not improve from 0.67150\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8543 - accuracy: 0.6309 - val_loss: 0.7961 - val_accuracy: 0.6706\n",
            "Epoch 57/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8487 - accuracy: 0.6268\n",
            "Epoch 57: val_accuracy did not improve from 0.67150\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8487 - accuracy: 0.6267 - val_loss: 0.8002 - val_accuracy: 0.6672\n",
            "Epoch 58/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8563 - accuracy: 0.6361\n",
            "Epoch 58: val_accuracy did not improve from 0.67150\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8563 - accuracy: 0.6361 - val_loss: 0.7931 - val_accuracy: 0.6672\n",
            "Epoch 59/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8502 - accuracy: 0.6311\n",
            "Epoch 59: val_accuracy did not improve from 0.67150\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8502 - accuracy: 0.6311 - val_loss: 0.7936 - val_accuracy: 0.6681\n",
            "Epoch 60/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8481 - accuracy: 0.6349\n",
            "Epoch 60: val_accuracy did not improve from 0.67150\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8481 - accuracy: 0.6349 - val_loss: 0.7895 - val_accuracy: 0.6715\n",
            "Epoch 61/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8476 - accuracy: 0.6346\n",
            "Epoch 61: val_accuracy improved from 0.67150 to 0.67235, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8474 - accuracy: 0.6348 - val_loss: 0.7908 - val_accuracy: 0.6724\n",
            "Epoch 62/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8486 - accuracy: 0.6363\n",
            "Epoch 62: val_accuracy did not improve from 0.67235\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8489 - accuracy: 0.6361 - val_loss: 0.7960 - val_accuracy: 0.6724\n",
            "Epoch 63/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8437 - accuracy: 0.6418\n",
            "Epoch 63: val_accuracy improved from 0.67235 to 0.67662, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8437 - accuracy: 0.6418 - val_loss: 0.7853 - val_accuracy: 0.6766\n",
            "Epoch 64/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8487 - accuracy: 0.6329\n",
            "Epoch 64: val_accuracy did not improve from 0.67662\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8489 - accuracy: 0.6328 - val_loss: 0.7878 - val_accuracy: 0.6732\n",
            "Epoch 65/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8479 - accuracy: 0.6341\n",
            "Epoch 65: val_accuracy did not improve from 0.67662\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8478 - accuracy: 0.6341 - val_loss: 0.7861 - val_accuracy: 0.6706\n",
            "Epoch 66/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8469 - accuracy: 0.6358\n",
            "Epoch 66: val_accuracy improved from 0.67662 to 0.68003, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8471 - accuracy: 0.6360 - val_loss: 0.7832 - val_accuracy: 0.6800\n",
            "Epoch 67/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8337 - accuracy: 0.6408\n",
            "Epoch 67: val_accuracy did not improve from 0.68003\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8337 - accuracy: 0.6408 - val_loss: 0.7872 - val_accuracy: 0.6732\n",
            "Epoch 68/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8445 - accuracy: 0.6382\n",
            "Epoch 68: val_accuracy did not improve from 0.68003\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8446 - accuracy: 0.6380 - val_loss: 0.7803 - val_accuracy: 0.6706\n",
            "Epoch 69/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8398 - accuracy: 0.6394\n",
            "Epoch 69: val_accuracy did not improve from 0.68003\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8398 - accuracy: 0.6395 - val_loss: 0.7869 - val_accuracy: 0.6706\n",
            "Epoch 70/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8362 - accuracy: 0.6415\n",
            "Epoch 70: val_accuracy did not improve from 0.68003\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8362 - accuracy: 0.6415 - val_loss: 0.7813 - val_accuracy: 0.6732\n",
            "Epoch 71/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8331 - accuracy: 0.6447\n",
            "Epoch 71: val_accuracy did not improve from 0.68003\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8333 - accuracy: 0.6444 - val_loss: 0.7764 - val_accuracy: 0.6758\n",
            "Epoch 72/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8409 - accuracy: 0.6397\n",
            "Epoch 72: val_accuracy did not improve from 0.68003\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8409 - accuracy: 0.6398 - val_loss: 0.7733 - val_accuracy: 0.6741\n",
            "Epoch 73/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8417 - accuracy: 0.6364\n",
            "Epoch 73: val_accuracy did not improve from 0.68003\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8417 - accuracy: 0.6364 - val_loss: 0.7773 - val_accuracy: 0.6672\n",
            "Epoch 74/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8362 - accuracy: 0.6437\n",
            "Epoch 74: val_accuracy improved from 0.68003 to 0.68174, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8362 - accuracy: 0.6437 - val_loss: 0.7748 - val_accuracy: 0.6817\n",
            "Epoch 75/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8342 - accuracy: 0.6407\n",
            "Epoch 75: val_accuracy improved from 0.68174 to 0.68430, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8340 - accuracy: 0.6407 - val_loss: 0.7733 - val_accuracy: 0.6843\n",
            "Epoch 76/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8364 - accuracy: 0.6451\n",
            "Epoch 76: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 10s 31ms/step - loss: 0.8361 - accuracy: 0.6453 - val_loss: 0.7790 - val_accuracy: 0.6783\n",
            "Epoch 77/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8352 - accuracy: 0.6453\n",
            "Epoch 77: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8352 - accuracy: 0.6453 - val_loss: 0.7844 - val_accuracy: 0.6681\n",
            "Epoch 78/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8387 - accuracy: 0.6408\n",
            "Epoch 78: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8387 - accuracy: 0.6408 - val_loss: 0.7794 - val_accuracy: 0.6698\n",
            "Epoch 79/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8421 - accuracy: 0.6375\n",
            "Epoch 79: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8421 - accuracy: 0.6375 - val_loss: 0.7766 - val_accuracy: 0.6715\n",
            "Epoch 80/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8342 - accuracy: 0.6425\n",
            "Epoch 80: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8342 - accuracy: 0.6425 - val_loss: 0.7793 - val_accuracy: 0.6706\n",
            "Epoch 81/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8384 - accuracy: 0.6400\n",
            "Epoch 81: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8381 - accuracy: 0.6402 - val_loss: 0.7728 - val_accuracy: 0.6783\n",
            "Epoch 82/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8335 - accuracy: 0.6496\n",
            "Epoch 82: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8339 - accuracy: 0.6492 - val_loss: 0.7725 - val_accuracy: 0.6724\n",
            "Epoch 83/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8325 - accuracy: 0.6436\n",
            "Epoch 83: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8325 - accuracy: 0.6435 - val_loss: 0.7786 - val_accuracy: 0.6732\n",
            "Epoch 84/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8345 - accuracy: 0.6470\n",
            "Epoch 84: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8345 - accuracy: 0.6470 - val_loss: 0.7766 - val_accuracy: 0.6792\n",
            "Epoch 85/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8232 - accuracy: 0.6476\n",
            "Epoch 85: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8232 - accuracy: 0.6476 - val_loss: 0.7752 - val_accuracy: 0.6741\n",
            "Epoch 86/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8244 - accuracy: 0.6498\n",
            "Epoch 86: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8240 - accuracy: 0.6500 - val_loss: 0.7714 - val_accuracy: 0.6724\n",
            "Epoch 87/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8213 - accuracy: 0.6539\n",
            "Epoch 87: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8211 - accuracy: 0.6541 - val_loss: 0.7707 - val_accuracy: 0.6766\n",
            "Epoch 88/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8257 - accuracy: 0.6528\n",
            "Epoch 88: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8256 - accuracy: 0.6528 - val_loss: 0.7739 - val_accuracy: 0.6724\n",
            "Epoch 89/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8308 - accuracy: 0.6453\n",
            "Epoch 89: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8306 - accuracy: 0.6454 - val_loss: 0.7795 - val_accuracy: 0.6664\n",
            "Epoch 90/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8305 - accuracy: 0.6468\n",
            "Epoch 90: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8308 - accuracy: 0.6466 - val_loss: 0.7790 - val_accuracy: 0.6672\n",
            "Epoch 91/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8232 - accuracy: 0.6516\n",
            "Epoch 91: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8230 - accuracy: 0.6517 - val_loss: 0.7694 - val_accuracy: 0.6817\n",
            "Epoch 92/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8305 - accuracy: 0.6532\n",
            "Epoch 92: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8302 - accuracy: 0.6534 - val_loss: 0.7729 - val_accuracy: 0.6792\n",
            "Epoch 93/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8228 - accuracy: 0.6532\n",
            "Epoch 93: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8228 - accuracy: 0.6532 - val_loss: 0.7722 - val_accuracy: 0.6766\n",
            "Epoch 94/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8219 - accuracy: 0.6491\n",
            "Epoch 94: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8219 - accuracy: 0.6491 - val_loss: 0.7699 - val_accuracy: 0.6834\n",
            "Epoch 95/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8214 - accuracy: 0.6517\n",
            "Epoch 95: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8214 - accuracy: 0.6517 - val_loss: 0.7687 - val_accuracy: 0.6826\n",
            "Epoch 96/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8198 - accuracy: 0.6510\n",
            "Epoch 96: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8198 - accuracy: 0.6509 - val_loss: 0.7669 - val_accuracy: 0.6843\n",
            "Epoch 97/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8239 - accuracy: 0.6497\n",
            "Epoch 97: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8239 - accuracy: 0.6497 - val_loss: 0.7697 - val_accuracy: 0.6817\n",
            "Epoch 98/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8159 - accuracy: 0.6524\n",
            "Epoch 98: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8162 - accuracy: 0.6523 - val_loss: 0.7688 - val_accuracy: 0.6783\n",
            "Epoch 99/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8209 - accuracy: 0.6568\n",
            "Epoch 99: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8206 - accuracy: 0.6569 - val_loss: 0.7679 - val_accuracy: 0.6775\n",
            "Epoch 100/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8139 - accuracy: 0.6541\n",
            "Epoch 100: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8139 - accuracy: 0.6540 - val_loss: 0.7687 - val_accuracy: 0.6783\n",
            "Epoch 101/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8239 - accuracy: 0.6518\n",
            "Epoch 101: val_accuracy did not improve from 0.68430\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8239 - accuracy: 0.6518 - val_loss: 0.7703 - val_accuracy: 0.6741\n",
            "Epoch 102/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8282 - accuracy: 0.6516\n",
            "Epoch 102: val_accuracy improved from 0.68430 to 0.68601, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8282 - accuracy: 0.6516 - val_loss: 0.7623 - val_accuracy: 0.6860\n",
            "Epoch 103/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8221 - accuracy: 0.6536\n",
            "Epoch 103: val_accuracy did not improve from 0.68601\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8218 - accuracy: 0.6538 - val_loss: 0.7626 - val_accuracy: 0.6792\n",
            "Epoch 104/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8181 - accuracy: 0.6515\n",
            "Epoch 104: val_accuracy did not improve from 0.68601\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.8181 - accuracy: 0.6514 - val_loss: 0.7613 - val_accuracy: 0.6843\n",
            "Epoch 105/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8191 - accuracy: 0.6541\n",
            "Epoch 105: val_accuracy did not improve from 0.68601\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8191 - accuracy: 0.6541 - val_loss: 0.7642 - val_accuracy: 0.6775\n",
            "Epoch 106/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8297 - accuracy: 0.6481\n",
            "Epoch 106: val_accuracy did not improve from 0.68601\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8301 - accuracy: 0.6478 - val_loss: 0.7627 - val_accuracy: 0.6852\n",
            "Epoch 107/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8183 - accuracy: 0.6549\n",
            "Epoch 107: val_accuracy did not improve from 0.68601\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8183 - accuracy: 0.6549 - val_loss: 0.7624 - val_accuracy: 0.6834\n",
            "Epoch 108/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8151 - accuracy: 0.6549\n",
            "Epoch 108: val_accuracy did not improve from 0.68601\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8147 - accuracy: 0.6551 - val_loss: 0.7633 - val_accuracy: 0.6852\n",
            "Epoch 109/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8212 - accuracy: 0.6509\n",
            "Epoch 109: val_accuracy did not improve from 0.68601\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8212 - accuracy: 0.6509 - val_loss: 0.7630 - val_accuracy: 0.6783\n",
            "Epoch 110/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8264 - accuracy: 0.6504\n",
            "Epoch 110: val_accuracy did not improve from 0.68601\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8263 - accuracy: 0.6505 - val_loss: 0.7594 - val_accuracy: 0.6809\n",
            "Epoch 111/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8164 - accuracy: 0.6592\n",
            "Epoch 111: val_accuracy did not improve from 0.68601\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8164 - accuracy: 0.6592 - val_loss: 0.7576 - val_accuracy: 0.6800\n",
            "Epoch 112/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8101 - accuracy: 0.6653\n",
            "Epoch 112: val_accuracy did not improve from 0.68601\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8098 - accuracy: 0.6654 - val_loss: 0.7579 - val_accuracy: 0.6817\n",
            "Epoch 113/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8097 - accuracy: 0.6593\n",
            "Epoch 113: val_accuracy improved from 0.68601 to 0.68686, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8097 - accuracy: 0.6593 - val_loss: 0.7567 - val_accuracy: 0.6869\n",
            "Epoch 114/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8138 - accuracy: 0.6571\n",
            "Epoch 114: val_accuracy improved from 0.68686 to 0.68771, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8138 - accuracy: 0.6571 - val_loss: 0.7552 - val_accuracy: 0.6877\n",
            "Epoch 115/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8196 - accuracy: 0.6575\n",
            "Epoch 115: val_accuracy improved from 0.68771 to 0.68942, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8196 - accuracy: 0.6575 - val_loss: 0.7551 - val_accuracy: 0.6894\n",
            "Epoch 116/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8148 - accuracy: 0.6582\n",
            "Epoch 116: val_accuracy did not improve from 0.68942\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8148 - accuracy: 0.6582 - val_loss: 0.7591 - val_accuracy: 0.6817\n",
            "Epoch 117/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8097 - accuracy: 0.6616\n",
            "Epoch 117: val_accuracy did not improve from 0.68942\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8098 - accuracy: 0.6615 - val_loss: 0.7601 - val_accuracy: 0.6894\n",
            "Epoch 118/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8152 - accuracy: 0.6628\n",
            "Epoch 118: val_accuracy improved from 0.68942 to 0.69369, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8152 - accuracy: 0.6628 - val_loss: 0.7551 - val_accuracy: 0.6937\n",
            "Epoch 119/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8178 - accuracy: 0.6566\n",
            "Epoch 119: val_accuracy did not improve from 0.69369\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8178 - accuracy: 0.6566 - val_loss: 0.7635 - val_accuracy: 0.6852\n",
            "Epoch 120/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8120 - accuracy: 0.6580\n",
            "Epoch 120: val_accuracy did not improve from 0.69369\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8120 - accuracy: 0.6580 - val_loss: 0.7586 - val_accuracy: 0.6911\n",
            "Epoch 121/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8050 - accuracy: 0.6638\n",
            "Epoch 121: val_accuracy did not improve from 0.69369\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8050 - accuracy: 0.6638 - val_loss: 0.7545 - val_accuracy: 0.6894\n",
            "Epoch 122/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8118 - accuracy: 0.6620\n",
            "Epoch 122: val_accuracy did not improve from 0.69369\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8120 - accuracy: 0.6619 - val_loss: 0.7618 - val_accuracy: 0.6877\n",
            "Epoch 123/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8121 - accuracy: 0.6620\n",
            "Epoch 123: val_accuracy did not improve from 0.69369\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8121 - accuracy: 0.6620 - val_loss: 0.7584 - val_accuracy: 0.6903\n",
            "Epoch 124/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8070 - accuracy: 0.6620\n",
            "Epoch 124: val_accuracy did not improve from 0.69369\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8070 - accuracy: 0.6620 - val_loss: 0.7577 - val_accuracy: 0.6869\n",
            "Epoch 125/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8158 - accuracy: 0.6613\n",
            "Epoch 125: val_accuracy did not improve from 0.69369\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8157 - accuracy: 0.6614 - val_loss: 0.7568 - val_accuracy: 0.6903\n",
            "Epoch 126/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8056 - accuracy: 0.6627\n",
            "Epoch 126: val_accuracy did not improve from 0.69369\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8058 - accuracy: 0.6628 - val_loss: 0.7596 - val_accuracy: 0.6894\n",
            "Epoch 127/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8043 - accuracy: 0.6634\n",
            "Epoch 127: val_accuracy did not improve from 0.69369\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8043 - accuracy: 0.6634 - val_loss: 0.7574 - val_accuracy: 0.6937\n",
            "Epoch 128/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8028 - accuracy: 0.6614\n",
            "Epoch 128: val_accuracy did not improve from 0.69369\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8026 - accuracy: 0.6614 - val_loss: 0.7520 - val_accuracy: 0.6903\n",
            "Epoch 129/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8058 - accuracy: 0.6606\n",
            "Epoch 129: val_accuracy did not improve from 0.69369\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8058 - accuracy: 0.6606 - val_loss: 0.7517 - val_accuracy: 0.6928\n",
            "Epoch 130/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8166 - accuracy: 0.6562\n",
            "Epoch 130: val_accuracy did not improve from 0.69369\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.8166 - accuracy: 0.6563 - val_loss: 0.7608 - val_accuracy: 0.6894\n",
            "Epoch 131/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8100 - accuracy: 0.6607\n",
            "Epoch 131: val_accuracy did not improve from 0.69369\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8102 - accuracy: 0.6605 - val_loss: 0.7508 - val_accuracy: 0.6903\n",
            "Epoch 132/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8029 - accuracy: 0.6630\n",
            "Epoch 132: val_accuracy improved from 0.69369 to 0.69539, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8029 - accuracy: 0.6630 - val_loss: 0.7537 - val_accuracy: 0.6954\n",
            "Epoch 133/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8130 - accuracy: 0.6524\n",
            "Epoch 133: val_accuracy did not improve from 0.69539\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8132 - accuracy: 0.6523 - val_loss: 0.7534 - val_accuracy: 0.6903\n",
            "Epoch 134/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8093 - accuracy: 0.6667\n",
            "Epoch 134: val_accuracy improved from 0.69539 to 0.69625, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8093 - accuracy: 0.6667 - val_loss: 0.7542 - val_accuracy: 0.6962\n",
            "Epoch 135/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8057 - accuracy: 0.6665\n",
            "Epoch 135: val_accuracy improved from 0.69625 to 0.69881, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8057 - accuracy: 0.6665 - val_loss: 0.7512 - val_accuracy: 0.6988\n",
            "Epoch 136/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8113 - accuracy: 0.6578\n",
            "Epoch 136: val_accuracy did not improve from 0.69881\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8113 - accuracy: 0.6579 - val_loss: 0.7514 - val_accuracy: 0.6928\n",
            "Epoch 137/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8034 - accuracy: 0.6600\n",
            "Epoch 137: val_accuracy did not improve from 0.69881\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8037 - accuracy: 0.6600 - val_loss: 0.7451 - val_accuracy: 0.6920\n",
            "Epoch 138/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8084 - accuracy: 0.6665\n",
            "Epoch 138: val_accuracy did not improve from 0.69881\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8084 - accuracy: 0.6665 - val_loss: 0.7516 - val_accuracy: 0.6869\n",
            "Epoch 139/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8027 - accuracy: 0.6604\n",
            "Epoch 139: val_accuracy did not improve from 0.69881\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8027 - accuracy: 0.6604 - val_loss: 0.7499 - val_accuracy: 0.6928\n",
            "Epoch 140/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8016 - accuracy: 0.6696\n",
            "Epoch 140: val_accuracy did not improve from 0.69881\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8016 - accuracy: 0.6696 - val_loss: 0.7449 - val_accuracy: 0.6894\n",
            "Epoch 141/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8088 - accuracy: 0.6611\n",
            "Epoch 141: val_accuracy did not improve from 0.69881\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8087 - accuracy: 0.6611 - val_loss: 0.7520 - val_accuracy: 0.6911\n",
            "Epoch 142/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8138 - accuracy: 0.6588\n",
            "Epoch 142: val_accuracy did not improve from 0.69881\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8138 - accuracy: 0.6588 - val_loss: 0.7502 - val_accuracy: 0.6945\n",
            "Epoch 143/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8056 - accuracy: 0.6638\n",
            "Epoch 143: val_accuracy did not improve from 0.69881\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.8056 - accuracy: 0.6638 - val_loss: 0.7460 - val_accuracy: 0.6962\n",
            "Epoch 144/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8019 - accuracy: 0.6667\n",
            "Epoch 144: val_accuracy did not improve from 0.69881\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8019 - accuracy: 0.6667 - val_loss: 0.7462 - val_accuracy: 0.6945\n",
            "Epoch 145/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7917 - accuracy: 0.6656\n",
            "Epoch 145: val_accuracy did not improve from 0.69881\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7915 - accuracy: 0.6657 - val_loss: 0.7486 - val_accuracy: 0.6911\n",
            "Epoch 146/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8047 - accuracy: 0.6700\n",
            "Epoch 146: val_accuracy did not improve from 0.69881\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8046 - accuracy: 0.6701 - val_loss: 0.7506 - val_accuracy: 0.6928\n",
            "Epoch 147/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8087 - accuracy: 0.6583\n",
            "Epoch 147: val_accuracy did not improve from 0.69881\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8090 - accuracy: 0.6583 - val_loss: 0.7496 - val_accuracy: 0.6988\n",
            "Epoch 148/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7965 - accuracy: 0.6731\n",
            "Epoch 148: val_accuracy did not improve from 0.69881\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7965 - accuracy: 0.6731 - val_loss: 0.7515 - val_accuracy: 0.6937\n",
            "Epoch 149/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8034 - accuracy: 0.6669\n",
            "Epoch 149: val_accuracy improved from 0.69881 to 0.70137, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.8034 - accuracy: 0.6669 - val_loss: 0.7485 - val_accuracy: 0.7014\n",
            "Epoch 150/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7937 - accuracy: 0.6698\n",
            "Epoch 150: val_accuracy did not improve from 0.70137\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7937 - accuracy: 0.6698 - val_loss: 0.7490 - val_accuracy: 0.6980\n",
            "Epoch 151/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7928 - accuracy: 0.6706\n",
            "Epoch 151: val_accuracy did not improve from 0.70137\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7928 - accuracy: 0.6706 - val_loss: 0.7532 - val_accuracy: 0.6962\n",
            "Epoch 152/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8033 - accuracy: 0.6692\n",
            "Epoch 152: val_accuracy did not improve from 0.70137\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8033 - accuracy: 0.6691 - val_loss: 0.7459 - val_accuracy: 0.7014\n",
            "Epoch 153/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8006 - accuracy: 0.6647\n",
            "Epoch 153: val_accuracy did not improve from 0.70137\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8006 - accuracy: 0.6647 - val_loss: 0.7484 - val_accuracy: 0.6962\n",
            "Epoch 154/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8004 - accuracy: 0.6620\n",
            "Epoch 154: val_accuracy did not improve from 0.70137\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8004 - accuracy: 0.6620 - val_loss: 0.7478 - val_accuracy: 0.6988\n",
            "Epoch 155/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8061 - accuracy: 0.6668\n",
            "Epoch 155: val_accuracy did not improve from 0.70137\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8062 - accuracy: 0.6669 - val_loss: 0.7571 - val_accuracy: 0.6997\n",
            "Epoch 156/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8030 - accuracy: 0.6703\n",
            "Epoch 156: val_accuracy did not improve from 0.70137\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8028 - accuracy: 0.6704 - val_loss: 0.7529 - val_accuracy: 0.6937\n",
            "Epoch 157/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7977 - accuracy: 0.6657\n",
            "Epoch 157: val_accuracy did not improve from 0.70137\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7977 - accuracy: 0.6658 - val_loss: 0.7463 - val_accuracy: 0.6997\n",
            "Epoch 158/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8071 - accuracy: 0.6637\n",
            "Epoch 158: val_accuracy did not improve from 0.70137\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.8071 - accuracy: 0.6637 - val_loss: 0.7523 - val_accuracy: 0.6997\n",
            "Epoch 159/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7972 - accuracy: 0.6694\n",
            "Epoch 159: val_accuracy improved from 0.70137 to 0.70222, saving model to best_swn.h5\n",
            "330/330 [==============================] - 12s 36ms/step - loss: 0.7972 - accuracy: 0.6694 - val_loss: 0.7485 - val_accuracy: 0.7022\n",
            "Epoch 160/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8052 - accuracy: 0.6643\n",
            "Epoch 160: val_accuracy did not improve from 0.70222\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.8052 - accuracy: 0.6642 - val_loss: 0.7488 - val_accuracy: 0.6954\n",
            "Epoch 161/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7985 - accuracy: 0.6693\n",
            "Epoch 161: val_accuracy did not improve from 0.70222\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7988 - accuracy: 0.6692 - val_loss: 0.7456 - val_accuracy: 0.7014\n",
            "Epoch 162/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7894 - accuracy: 0.6721\n",
            "Epoch 162: val_accuracy did not improve from 0.70222\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7894 - accuracy: 0.6721 - val_loss: 0.7446 - val_accuracy: 0.6997\n",
            "Epoch 163/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8004 - accuracy: 0.6696\n",
            "Epoch 163: val_accuracy improved from 0.70222 to 0.70563, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.8004 - accuracy: 0.6696 - val_loss: 0.7462 - val_accuracy: 0.7056\n",
            "Epoch 164/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8090 - accuracy: 0.6668\n",
            "Epoch 164: val_accuracy did not improve from 0.70563\n",
            "330/330 [==============================] - 12s 36ms/step - loss: 0.8088 - accuracy: 0.6669 - val_loss: 0.7503 - val_accuracy: 0.6988\n",
            "Epoch 165/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7951 - accuracy: 0.6728\n",
            "Epoch 165: val_accuracy did not improve from 0.70563\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7953 - accuracy: 0.6727 - val_loss: 0.7523 - val_accuracy: 0.7031\n",
            "Epoch 166/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7956 - accuracy: 0.6699\n",
            "Epoch 166: val_accuracy did not improve from 0.70563\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7954 - accuracy: 0.6700 - val_loss: 0.7535 - val_accuracy: 0.6945\n",
            "Epoch 167/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7922 - accuracy: 0.6746\n",
            "Epoch 167: val_accuracy did not improve from 0.70563\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7922 - accuracy: 0.6746 - val_loss: 0.7542 - val_accuracy: 0.6971\n",
            "Epoch 168/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7979 - accuracy: 0.6741\n",
            "Epoch 168: val_accuracy did not improve from 0.70563\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7979 - accuracy: 0.6741 - val_loss: 0.7469 - val_accuracy: 0.7022\n",
            "Epoch 169/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7936 - accuracy: 0.6719\n",
            "Epoch 169: val_accuracy did not improve from 0.70563\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7941 - accuracy: 0.6720 - val_loss: 0.7527 - val_accuracy: 0.6962\n",
            "Epoch 170/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7994 - accuracy: 0.6720\n",
            "Epoch 170: val_accuracy did not improve from 0.70563\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7994 - accuracy: 0.6720 - val_loss: 0.7420 - val_accuracy: 0.6971\n",
            "Epoch 171/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8034 - accuracy: 0.6675\n",
            "Epoch 171: val_accuracy did not improve from 0.70563\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8031 - accuracy: 0.6676 - val_loss: 0.7507 - val_accuracy: 0.7048\n",
            "Epoch 172/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7921 - accuracy: 0.6702\n",
            "Epoch 172: val_accuracy did not improve from 0.70563\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7921 - accuracy: 0.6702 - val_loss: 0.7504 - val_accuracy: 0.6971\n",
            "Epoch 173/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7911 - accuracy: 0.6726\n",
            "Epoch 173: val_accuracy did not improve from 0.70563\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.7912 - accuracy: 0.6725 - val_loss: 0.7505 - val_accuracy: 0.6971\n",
            "Epoch 174/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7849 - accuracy: 0.6772\n",
            "Epoch 174: val_accuracy did not improve from 0.70563\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.7848 - accuracy: 0.6772 - val_loss: 0.7494 - val_accuracy: 0.7039\n",
            "Epoch 175/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7911 - accuracy: 0.6758\n",
            "Epoch 175: val_accuracy improved from 0.70563 to 0.70734, saving model to best_swn.h5\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.7911 - accuracy: 0.6758 - val_loss: 0.7464 - val_accuracy: 0.7073\n",
            "Epoch 176/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7854 - accuracy: 0.6748\n",
            "Epoch 176: val_accuracy did not improve from 0.70734\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.7854 - accuracy: 0.6748 - val_loss: 0.7491 - val_accuracy: 0.7005\n",
            "Epoch 177/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7975 - accuracy: 0.6683\n",
            "Epoch 177: val_accuracy did not improve from 0.70734\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.7975 - accuracy: 0.6683 - val_loss: 0.7421 - val_accuracy: 0.7014\n",
            "Epoch 178/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7981 - accuracy: 0.6673\n",
            "Epoch 178: val_accuracy did not improve from 0.70734\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7979 - accuracy: 0.6672 - val_loss: 0.7415 - val_accuracy: 0.7014\n",
            "Epoch 179/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7917 - accuracy: 0.6680\n",
            "Epoch 179: val_accuracy did not improve from 0.70734\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7915 - accuracy: 0.6680 - val_loss: 0.7492 - val_accuracy: 0.7014\n",
            "Epoch 180/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.8000 - accuracy: 0.6655\n",
            "Epoch 180: val_accuracy did not improve from 0.70734\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8000 - accuracy: 0.6655 - val_loss: 0.7495 - val_accuracy: 0.6971\n",
            "Epoch 181/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7918 - accuracy: 0.6716\n",
            "Epoch 181: val_accuracy did not improve from 0.70734\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7918 - accuracy: 0.6716 - val_loss: 0.7493 - val_accuracy: 0.6971\n",
            "Epoch 182/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7978 - accuracy: 0.6692\n",
            "Epoch 182: val_accuracy did not improve from 0.70734\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7980 - accuracy: 0.6691 - val_loss: 0.7482 - val_accuracy: 0.7022\n",
            "Epoch 183/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8029 - accuracy: 0.6689\n",
            "Epoch 183: val_accuracy did not improve from 0.70734\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.8030 - accuracy: 0.6687 - val_loss: 0.7472 - val_accuracy: 0.7014\n",
            "Epoch 184/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7943 - accuracy: 0.6775\n",
            "Epoch 184: val_accuracy did not improve from 0.70734\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7945 - accuracy: 0.6773 - val_loss: 0.7437 - val_accuracy: 0.7065\n",
            "Epoch 185/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7938 - accuracy: 0.6679\n",
            "Epoch 185: val_accuracy did not improve from 0.70734\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.7938 - accuracy: 0.6677 - val_loss: 0.7414 - val_accuracy: 0.7031\n",
            "Epoch 186/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7851 - accuracy: 0.6805\n",
            "Epoch 186: val_accuracy improved from 0.70734 to 0.70990, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7851 - accuracy: 0.6805 - val_loss: 0.7451 - val_accuracy: 0.7099\n",
            "Epoch 187/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7907 - accuracy: 0.6769\n",
            "Epoch 187: val_accuracy improved from 0.70990 to 0.71075, saving model to best_swn.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7907 - accuracy: 0.6769 - val_loss: 0.7466 - val_accuracy: 0.7108\n",
            "Epoch 188/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7884 - accuracy: 0.6727\n",
            "Epoch 188: val_accuracy did not improve from 0.71075\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7885 - accuracy: 0.6726 - val_loss: 0.7419 - val_accuracy: 0.7056\n",
            "Epoch 189/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7964 - accuracy: 0.6702\n",
            "Epoch 189: val_accuracy did not improve from 0.71075\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7964 - accuracy: 0.6702 - val_loss: 0.7438 - val_accuracy: 0.7082\n",
            "Epoch 190/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7833 - accuracy: 0.6803\n",
            "Epoch 190: val_accuracy did not improve from 0.71075\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7833 - accuracy: 0.6803 - val_loss: 0.7451 - val_accuracy: 0.7056\n",
            "Epoch 191/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7954 - accuracy: 0.6721\n",
            "Epoch 191: val_accuracy did not improve from 0.71075\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7958 - accuracy: 0.6720 - val_loss: 0.7472 - val_accuracy: 0.7090\n",
            "Epoch 192/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7975 - accuracy: 0.6701\n",
            "Epoch 192: val_accuracy did not improve from 0.71075\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7977 - accuracy: 0.6701 - val_loss: 0.7489 - val_accuracy: 0.7090\n",
            "Epoch 193/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7994 - accuracy: 0.6674\n",
            "Epoch 193: val_accuracy did not improve from 0.71075\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7995 - accuracy: 0.6672 - val_loss: 0.7486 - val_accuracy: 0.7022\n",
            "Epoch 194/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7832 - accuracy: 0.6759\n",
            "Epoch 194: val_accuracy did not improve from 0.71075\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7832 - accuracy: 0.6758 - val_loss: 0.7480 - val_accuracy: 0.6971\n",
            "Epoch 195/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7868 - accuracy: 0.6816\n",
            "Epoch 195: val_accuracy did not improve from 0.71075\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7868 - accuracy: 0.6815 - val_loss: 0.7459 - val_accuracy: 0.7022\n",
            "Epoch 196/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7963 - accuracy: 0.6676\n",
            "Epoch 196: val_accuracy did not improve from 0.71075\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7963 - accuracy: 0.6676 - val_loss: 0.7498 - val_accuracy: 0.7022\n",
            "Epoch 197/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7998 - accuracy: 0.6722\n",
            "Epoch 197: val_accuracy did not improve from 0.71075\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7998 - accuracy: 0.6722 - val_loss: 0.7544 - val_accuracy: 0.6980\n",
            "Epoch 198/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7955 - accuracy: 0.6692\n",
            "Epoch 198: val_accuracy did not improve from 0.71075\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7955 - accuracy: 0.6692 - val_loss: 0.7442 - val_accuracy: 0.7108\n",
            "Epoch 199/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7813 - accuracy: 0.6818\n",
            "Epoch 199: val_accuracy did not improve from 0.71075\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7813 - accuracy: 0.6818 - val_loss: 0.7497 - val_accuracy: 0.7022\n",
            "Epoch 200/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7938 - accuracy: 0.6766\n",
            "Epoch 200: val_accuracy did not improve from 0.71075\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.7938 - accuracy: 0.6766 - val_loss: 0.7447 - val_accuracy: 0.7073\n"
          ]
        }
      ],
      "source": [
        "hist_swn = model_swn.fit(X_train_pad, y_train_swn, \n",
        "                 batch_size=batch_size,\n",
        "                 epochs=epochs,\n",
        "                 validation_split=0.1,\n",
        "                 callbacks=[mc_swn])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9acR8FEkxykx",
        "outputId": "813e613a-98eb-4078-b3be-fd24ef7c697f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd1yVR/b/30Ov0rFQBBQEO4q9x2jsRhNLuimmmTV195tkd5Ns2uaXTe9V0zXGJMYYSzRq7A0LCgooIL333ub3x1zg0hQNiOK8X6/7uvcp8zxzrzLnmXPOfI6QUqLRaDQaTUNM2rsDGo1Go7k80QZCo9FoNE2iDYRGo9FomkQbCI1Go9E0iTYQGo1Go2kSbSA0Go1G0yTaQGg0gBDiCyHEiy08N04IcW1b90mjaW+0gdBoNBpNk2gDodF0IIQQZu3dB03HQRsIzRWDwbXzdyFEmBCiSAjxuRCisxBigxCiQAixRQjhZHT+LCFEuBAiVwixXQgRZHQsWAhx2NDue8Cqwb1mCCGOGtruEUL0b2Efpwshjggh8oUQCUKI5xocH224Xq7h+CLDfmshxOtCiLNCiDwhxC7DvvFCiMQmfodrDZ+fE0KsFkJ8I4TIBxYJIYYKIfYa7pEihHhPCGFh1L6PEGKzECJbCJEmhHhaCNFFCFEshHAxOm+QECJDCGHeku+u6XhoA6G50rgBmAQEADOBDcDTgBvq//NSACFEALACeMRwbD3wqxDCwjBYrgG+BpyBHwzXxdA2GFgG3Ae4AB8Da4UQli3oXxFwO+AITAceEEJcb7hud0N/3zX0aSBw1NDuNWAwMNLQp38A1S38TWYDqw33/BaoAh4FXIERwETgQUMf7IEtwEagG9AT+ENKmQpsB+YbXfc2YKWUsqKF/dB0MLSB0FxpvCulTJNSJgE7gf1SyiNSylLgZyDYcN4C4Dcp5WbDAPcaYI0agIcD5sBbUsoKKeVq4KDRPe4FPpZS7pdSVkkpvwTKDO3OiZRyu5TyuJSyWkoZhjJS4wyHbwa2SClXGO6bJaU8KoQwAe4CHpZSJhnuuUdKWdbC32SvlHKN4Z4lUspQKeU+KWWllDIOZeBq+jADSJVSvi6lLJVSFkgp9xuOfQncCiCEMAVuQhlRzVWKNhCaK400o88lTWzbGT53A87WHJBSVgMJgIfhWJKsr1R51uhzd+Bxg4smVwiRC3gZ2p0TIcQwIcQ2g2smD7gf9SSP4RpnmmjminJxNXWsJSQ06EOAEGKdECLV4HZ6uQV9APgF6C2E8EXN0vKklAcusk+aDoA2EJqOSjJqoAdACCFQg2MSkAJ4GPbV4G30OQF4SUrpaPSykVKuaMF9vwPWAl5SSgfgI6DmPglAjybaZAKlzRwrAmyMvocpyj1lTENJ5g+BU4C/lLITygVn3Ae/pjpumIWtQs0ibkPPHq56tIHQdFRWAdOFEBMNQdbHUW6iPcBeoBJYKoQwF0LMBYYatf0UuN8wGxBCCFtD8Nm+Bfe1B7KllKVCiKEot1IN3wLXCiHmCyHMhBAuQoiBhtnNMuANIUQ3IYSpEGKEIeYRBVgZ7m8O/As4XyzEHsgHCoUQgcADRsfWAV2FEI8IISyFEPZCiGFGx78CFgGz0AbiqkcbCE2HREoZiXoSfhf1hD4TmCmlLJdSlgNzUQNhNipe8ZNR20PAYuA9IAc4bTi3JTwIPC+EKACeQRmqmuvGA9NQxiobFaAeYDj8BHAcFQvJBv4fYCKlzDNc8zPU7KcIqJfV1ARPoAxTAcrYfW/UhwKU+2gmkApEAxOMju9GBccPSymN3W6aqxChCwZpNBpjhBBbge+klJ+1d1807Ys2EBqNphYhxBBgMyqGUtDe/dG0L9rFpNFoABBCfIlaI/GINg4a0DMIjUaj0TSDnkFoNBqNpkk6jLCXq6ur9PHxae9uaDQazRVFaGhoppSy4doaoAMZCB8fHw4dOtTe3dBoNJorCiFEs+nM2sWk0Wg0mibRBkKj0Wg0TaINhEaj0WiapMPEIJqioqKCxMRESktL27srHQYrKys8PT0xN9c1ZDSajk6HNhCJiYnY29vj4+NDfeFOzcUgpSQrK4vExER8fX3buzsajaaN6dAuptLSUlxcXLRxaCWEELi4uOgZmUZzldChDQSgjUMro39PjebqocMbCI1Go7lskBJO/wFRm/7adeL3Q9Lh1unTOdAGoo3Jzc3lgw8+uOB206ZNIzc3tw16pNFo2oXyYvh8MnwzF76/FUqa+PsuzYfSvPNfa92j8Ntjrd/HBmgD0cY0ZyAqKyvP2W79+vU4Ojq2Vbc0Gs2lJvZPSDwAIXdBVTmcXKsMQvpJdXz32/CKF7ziDbvebP461dWQHQOpJ6CibeOB2kC0MU8++SRnzpxh4MCBDBkyhDFjxjBr1ix69+4NwPXXX8/gwYPp06cPn3zySW07Hx8fMjMziYuLIygoiMWLF9OnTx8mT55MSUlJe30djebqZd9HkHLs4tvHbAcza5jyCjj3gGPfw7fz4KPREL0FdrwG3iPBtRec/LX56xSmQmUJVFdA2gnY+75q2wZ06DRXY/7zazgRyfmtes3e3Trx7Mw+5zznlVde4cSJExw9epTt27czffp0Tpw4UZsmumzZMpydnSkpKWHIkCHccMMNuLi41LtGdHQ0K1as4NNPP2X+/Pn8+OOP3Hrrra36XTSaDktxNtg4/7VrxO6Ejf8Hva+H+V+2vF1+Cqy8GWa8CTF/QvcRYGYJ/ebBn6+ocyzs4bt5Kj4x4w0I/xl2/E+5oKyb8CJkx9R9TjwIe96DrgMan9cK6BnEJWbo0KH11hC88847DBgwgOHDh5OQkEB0dHSjNr6+vgwcOBCAwYMHExcXd6m6q9Fc2URvhv/1hLSIlreJ3Fg/DiAl/PEf9Tn2T+XiaSlhKyH5MKx9CDJOgu84tb/fPPXe9wa4aYW6x4CbwD0I/MaDrIa4XU1fs8ZAmFrC/o+gIBn63djyPl0AV80M4nxP+pcKW1vb2s/bt29ny5Yt7N27FxsbG8aPH9/kGgNLS8vaz6amptrFpNG0lBM/gayC6N+hc+/zn591BlYsgAn/hHH/UPuiNqondd9xykCkhkG3gS27//HVaiBPPa62/card9eecM9W1Sdza3hwLzj7qWMeIWBuq1xSQTMaXzM7BkzMoccE1TdzW+g1tWX9uUDadAYhhJgihIgUQpwWQjzZxPE3hRBHDa8oIUSu0bE7hBDRhtcdbdnPtsTe3p6CgqarN+bl5eHk5ISNjQ2nTp1i3759l7h3Gk0HoaoC0k/V31ddBdGGdNKY7ZASBq8HQqbRLL04G/KS6rbj96r3s3vq9p34CWxc4foP6651PlKPq/ulnYCJz4CjN1g7QZf+ded4DlbGAdTMwczwIGhmAT6j6u5TVancU6e3QFGWMhBO3cFrqDoeOB0s6h48W5M2m0EIIUyB94FJQCJwUAixVkpZO9eTUj5qdP7fgGDDZ2fgWSAEkECooW1OW/W3rXBxcWHUqFH07dsXa2trOnfuXHtsypQpfPTRRwQFBdGrVy+GDx/ejj3VaNqZuF1QUQL+k9R2dRXs/xj6LwBbl3O3/fUROLYCHj0B5jZwbCW4B0JxFjh4Q/w+2Pk6FKTAma3g6q/a/Xw/ZEbC0qMgRJ2BSDyoBmYTUzVr8BsPDh7gFqTWMJia1zcsQTNVfAHU8e/mq5mDMIX+88F3DJTkgEkLn8n9xqtZT3aMWjex/gm133ecMmrOftB9tNo38KaWXfMiaEsX01DgtJQyBkAIsRKYDTTnDLwJZRQArgM2SymzDW03A1OAFW3Y3zbju+++a3K/paUlGzZsaPJYTZzB1dWVEydO1O5/4oknWr1/Gs1lwYYn1YD48FGwc1cD46anoDhTPYWD8v9XV6qnbCmhIFUN4Ee/Ucdj/oTcs7D9v8pQmJjBNf+Cn++FiDXqnKRQ9V6Yrp7KZZV60u/STxkSM2soL1T7zCyhMA38DLEDv3HK7x+/ByzsAKEyiqI2wkOHlAE4tkLNFtyCwC1AfRc79wv7LYJmwaan4cSPyuC4BUHPibD3PfWdfEaB9zB45AQ4ev3ln7452tLF5AEkGG0nGvY1QgjRHfAFtl5IWyHEvUKIQ0KIQxkZGa3SaY1Gcx6kVANra+TgZ0Qpo1CSqwbkiiL1pA9w/Ie6dynVrOC9EHh3kHq63/cBvBEIP98H3YKVGyhmO0RuALsuUFkGPqOVf16Yqmu596kzEOFrlHEA1aYwA7JOw2CDRzt+rzI4UBc7GHovDL0P7v0Tnk6CpxPh+o8g+4yamZQVqGv1vQHu2gAz376438XRC7qPggOfqdnMgIUw+jE1K6murItXtKFxgMsni2khsFrKmn+tliGl/ERKGSKlDHFza7KkqkajaW1Cl8M3N8ChZX/tOlLCyptg1e1qEESqAfzg50pG4tRvYN8VcuNh41PKEFSWQl4CJOxTsQHXXjDrPbj1J/V0H7UBUo7CsPvgvh0w+wOw6qQG+F7ToN8NygiU5MDxVdC5rwoKR25Q1wQ1uDt4GwzEdjUYO3qrYy49YNqr9YPUvWeDrTsc+ET1ubIU+s3/a78NqMykwtS6Ptm61GUrOff469dvAW1pIJIAY/PmadjXFAup7z66kLYajeZSkRkNG59WnyPXt7xdYbqKLzS8VtZpFdA9tFy5ThZ8DVYOSpKiokitHzCzgv0fgucQeGA3mFpA6JeQdEiliw66Ta1z8Btfl57aayp06aviBgA3r4L5X4HHYLV9+GtllPrNg15TVCrqn6+qJ/SuA8B7uFqsFrWxLjW1OcwsYPAiFRD/5SFlTGoCyH+F3ter38R7ZN1MYfSj0OMaFeC+BLRlDOIg4C+E8EUN7guBmxueJIQIBJyAvUa7NwEvCyGcDNuTgafasK8ajaY5kkJVlpD3cNjynPLL95kDYd+rJ3Frp3O3j/kTVtwETj5w289gb0jUqDEwwgQif1ODt0sPuHMDfH29ChD7XweBM9QT/txP1L18xqinf1CDew01A7mTD7gF1u+DqWGo6xas3jc/A7ZuEHwbFKXD1pdUAHvGG+r7jXlMHRdCSWOcjxEPKvmMqnLwn6za/VVsnGHup+o3qcHVX/2Gl4g2MxBSykohxEOowd4UWCalDBdCPA8cklKuNZy6EFgppZRGbbOFEC+gjAzA8zUBa41Gcw6kvLjBqbl2UsKP90B5EfztsAocB9+qMnOOfae2z7VI6+xeJSfh6AU5sbB8Kty7Xbl9ojaqtE9bV+W/9zZkAbkFwAN7lKvGxEQN2hP/rQZ+ULODM3+Ag5dyEdXg1F0ZD7/xzf8GVg7gGgCZUcr9ZOuiXg/uVdc3Tjud8nLLfz9rJ5j0n5af31L6zm39a14AbRqDkFKul1IGSCl7SClfMux7xsg4IKV8TkrZaI2ElHKZlLKn4bW8Lfup0XQIcuPhVV84s+3C2p36DV7prhaJVVfVLeoCFQvIjlGZPNteVhk7vaaqp31bN+WGqXu2U1pF5cXqc3W1kqew6wx3b4ZbflDB3L3vqXz+hP3qWjX++hoDAUpiwr6L+mzlUGccAAKm1L03NASL1sHY82T6jX4MJr8EAZPr9rkH1RkHTS2XS5BaY8DOzg6A5ORkbryx6Sez8ePHc+jQoXNe56233qK4uLh2W8uHXwUc+FS5fE5vubB2u96CsjxlAP74jxKPi/hFHTv+g/L5W9iprCELO5UZZGKqfOQRa2DZdZBzVmUBfXpNXc7+yV+UwZjwtHKX+IxWbfa8Bz8tVnISgTNUHOCGz1u+GtjRC25ZDeMbPVe2jIE3wciHLq7tVYY2EJcp3bp1Y/Xq1RfdvqGB0PLhHZzyYjj8lfqcFKoCwt/cCEe+qTtHSigrrPtcnA3JR5QEtbMfnFgNu99RMYE/XlBpoid+hIDrVF4+UgVIa1b8Tn4Rpv5PrRje9aYK0lZXqnUAMX+qeIVbkHJH1XDNv5Tr6MxWFYDu2l/FB/rdqIxOS/GfpFxTmjZFG4g25sknn+T999+v3X7uued48cUXmThxIoMGDaJfv3788ssvjdrFxcXRt6/yr5aUlLBw4UKCgoKYM2dOPS2mBx54gJCQEPr06cOzz6p1hu+88w7JyclMmDCBCRMmAHXy4QBvvPEGffv2pW/fvrz11lu199Oy4lcYaRF1tQROrIbSXOg6EJKPqlnE6c3wyxIlBw2w4R/wZm+lMLr7beWOWnGz0vK5fa3yozv7wZyPISsa3h+mArj95tcN8oFG2kDmVjDsXiX1ELEGItYqd5KFHXw1S2UuzXiz/sDv6q8kK25a2bLgr6ZduWrE+tjwZH3famvQpR9MfeWcpyxYsIBHHnmEJUuWALBq1So2bdrE0qVL6dSpE5mZmQwfPpxZs2Y1W+/5ww8/xMbGhpMnTxIWFsagQYNqj7300ks4OztTVVXFxIkTCQsLY+nSpbzxxhts27YNV9f6T1mhoaEsX76c/fv3I6Vk2LBhjBs3DicnJy0rfiURuQFW3aF88w8dUE/6rgEwYoly3+x8Q8lI95igVuQmHKhbSbzpaWVAXHup2MKQu5XbZvE2sOyk3EGHv1Juo+mvKxkJIeDOjeA1rHFf+s9XBip6E4TcrbJudr4BC79Tq30bMmBBm/40mtbj6jEQ7URwcDDp6ekkJyeTkZGBk5MTXbp04dFHH2XHjh2YmJiQlJREWloaXbp0afIaO3bsYOnSpQD079+f/v3rBL9WrVrFJ598QmVlJSkpKURERNQ73pBdu3YxZ86cWlXZuXPnsnPnTmbNmqVlxa8EchNUrYAj3yg9oMxIKEiDhIMQfEtdnn/yYeXvv+Fz+HUpHP0WOvcDj0Fw+EtAwF2bVHC2Buc6GXpuX6uMgvFDS3ejILIxPa4Ba2coyVZxBP9JMOyBlusOaS5brh4DcZ4n/bZk3rx5rF69mtTUVBYsWMC3335LRkYGoaGhmJub4+Pj06TM9/mIjY3ltdde4+DBgzg5ObFo0aKLuk4NWla8jUkLh+zYpiWcW4KU8N0CgxzEIjUYf3sjHPxULSrzHq5cRNZOKljda5ry7896TwWIfcaogPOJH1VM4Vzy1xcyuJuaq0Bz2PfqHhfaXnPZov8VLwELFixg5cqVrF69mnnz5pGXl4e7uzvm5uZs27aNs2fPnrP92LFjawX/Tpw4QVhYGAD5+fnY2tri4OBAWlpaPeG/5mTGx4wZw5o1ayguLqaoqIiff/6ZMWPGtOK3bScqy+unW15qqqvU61zs+B+svksFkKur1eIzUNLSq+5QPvvwNbD6bvV9GpJ6HNLDYcp/1doA37Fq5e8BQ6lar+Hqid9jsAo016iimpjAwJuVG8m+Myw5oGIDrcmk5+HBfSouoekwXD0ziHakT58+FBQU4OHhQdeuXbnllluYOXMm/fr1IyQkhMDAwHO2f+CBB7jzzjsJCgoiKCiIwYOVG2HAgAEEBwcTGBiIl5cXo0aNqm1z7733MmXKFLp168a2bXV58YMGDWLRokUMHaqkAO655x6Cg4OvbHdScTZ8PA76zFaZNZeK9FMqVdPSHr6ZqwK0t62pW7XbkMzTUFWm8v/j9ys30cNHlaZRxBol/ZCfDEgIuVM99Rtz/AclvdBnjto2s1Quo/i9St6hRlZi1MPQ89rmy2w6NKmZ+dcwtwLzrq1/XU27ImR7PnW1IiEhIbLh2oCTJ08SFBTUTAvNxXJZ/a5SKrG3k2uhkwc8Gt46Mgctue8bvVW5R8tOSmKhshQm/AvG/b3x+dXV8HI3tdBs1MOq0lh+kgoMr7pdLQzLjYdug5R89ZjHVUqocfu3+qqVxzevrNu/5TmVYtp/Icz9uM2/tqbjIYQIlVKGNHVMu5g0VzaRG5Rx8AhRA25qWNvc58w2+Hisqj8AkHFKGQf/ySrQe/fvSnFz+38h9UTj9vlJyjiAEqbLN2hPHv1WqZMOvBUeO6V0djwGKxVRKVUmEai1BflJjWUtvEca3pvIFtJ0COKzinnqp+OUV9avhZ2QXcz4/23jdHphm91bGwjN5UX6yfrlHs9H4kGDCug3gFAF59uC2B1qVfAvS9TAXVMnYPrryjh0HQDTXgNLO/jj+cbts06r9+6joCxfrT1w7lG3uK37CLCwUbMfv/FK4mLjU/B2f2WYflmijEHQzPrX7TEBJr2ggsSay5ri8kpuX3aAYwn1FQ2klLy1JYqTKflNtvtiTxwrDsRzKrX+8UNns4nLKmbVoYQm27UGHd5AdBQX2uVCm/+em56G729TLpWWkBsPDp7QqauSg74QCeryYvhxcf0axee6D0KtHzjwaeM6AaB8/qMeUesBzu6t377GQAy5R70HTlcrlKvK1XoFY9E533GqkM3+D5VBKc5RM5Vbf6xbxVyDqTmMWqriIJrLmpMp+eyIyuCFdRH1/o7SC8p4a0s0X+1tnKwipWRTuJq1JmTXzyqMzVRKCeuOJVNd3TZ/lx3aQFhZWZGVlaWNRCshpSQrKwsrqzbKVJFSrQIuzlTZOi0hN75ukO41RRWLKUxv+ty1f1PZQjVE/65ko/d92LL7+IxWA/Xmf6s4QVN1Aobdr6qZrb5TyVW8PwyWT1NGyMJOzQAGL1JxCL/xqo3X0Pqrjb2GqnKZjt3ViuNHwuDm79UMQ3PJkVI2cu9cCJmFZQDEGQb0Q2dz2BGdWXu8ZuZwJD6nUdvw5HyScpVhiM8urnfsbFYRAMl5pYQ20bY16NBZTJ6eniQmJqLLkbYeVlZWeHp6ts7FygqUDMSIJeoJOD9JLbYC9YTepd/5r5EbD/7Xqs81q3xTw1QWjzHV1UoKoqygroZBpCEtOPxnmPr/1NN4SpgqRNNQBiI3Xl3z2mfhgxHKiPmNb9wfCxtY+C1s+ifsfE1VGss4pWYQLj3VPWrKUDp1VwHuHtfUv4aZJdy0QslZW3U6/2+gaVOW747jtd8jeeWG/swa0O2C2p5OL2Dymzv4+u5hxGUVYSKgq4M1b2yOYqy/K0IITqaodPSotAKKyiqxtawbln8PT8VEgJW5KQk59Q1EXGYRg7wdCU/O59djyQzxaSZr7S/QoQ2Eubk5vr6+5z9R0z4c+FQFdavKVVH6FEOA2dRCGYiRf6t/fmk+/LAIAqcpV01FqSrJ6GCYQbgbFn6lhTc2EJmRSqsIVA2DPnPUDKKTJ+QnKvG4gOuUXlH8XuXvdzekH1eWqfs4eqvi83M+hm0vNm0gADxD4O5NKqBtaQ9vBClJC58G600s7eHhY0rOuiHNXVtzSamulnyxJ47SiiqWrjjCzqgM/j6lF+72LZtF74/NplrCwTgVL/BwsubeMX78+5dwjibkEuztVBtbqJYQlpjHiB4ute1/j0gjxMeZsooqEoxmEFJKYjOLmD3Qg64O1kSmNl7z1Bp0aBeT5jKmqrKupvG+D9VgmhoGCCUOd3ZP48ViG/6hCsX89rgqbF+TCVTjYrJxVjWM0yIa3y/eEBMwszbUHz6gZivXPqtmE2HfKwNVc97BT+va5iXWv4//tarojfV51HHtu4CFrapaBkqoriE2zhemYqppFdILSpn7wW4e/f4ou4zcPQ05EJdNfHYxr9zQn/vH9WDN0SSmvrWTgtKKFt2nJiAdnpzP2awifFxsmTPIE1sLU77ep2IOp1IKGNxdVeU7klDnKsovreBUagGje7ri6WxTz0DkFleQX1pJdxcb/jevPyvvHX7Bv0FL0AZC0z5EbVTpnZNeUDOI7a+oAdrVX0lIVBRDvFE204mfVKrnmMeVxtAfLyhZa6gfKHbv3XT8In6fcvf0vQGiN8O+98HEXBWd6b9QyU/8eI8yIL2mw9EVdfWNc882vs+FMOQe5UpqSuhOQ1FZJf9df7LFg+5f4UxGIVJKVh5I4HB8LjuiMrhj+QH+jGraDf3DoUTsLc2Y2b8bT04N5NPbQ8gqKmf36awW3e9Ygvo/FJGcT2ymMhB2lmbMHeTJurAU0vJLOZNRyDBfZ/zcbDkSX5fhdCJRtR3o5YiXkw1JuSVUGYLRsYb4g6+rLTYWZs0Kff5VtIHQtB6VZaoqWXNkx8CaJfBSV/j+VuXeGf4gDFkMocshZptaCOY3Tg3mG59WbqS8JFj3qFofMP4plbWDhCNfq+saD9yde0NGlJqhgJqZFKSqrCLv4SqQXZanKqENv1/5+Cf9R8lYZ0YqZdJxf1faRmGGuse58Y3vcyE4+8L/xamUVE0jdp/O5OMdMawLS2nV6765OYpVB+tSQHdGZzDx9T/5Zt9Zvj+YwBh/V/78xwQCOtuz5NvDLP7qEO/+UZfRdiajkN+OJzNjQFesLdQsb1RPV+wszZo1KDEZhbXB46KySqLTC+hkZUZSbgkFhid+gFuHd6e8sponfjhGZbUksGsngr2cOHw2h9IKJdlyNFEZi/6eDng721BRJdl9OpMFH+9l7xlloLq72Lbqb9YQbSA0rcfe9+HdQbB8OmRE1j9WXgTLpihZ6L5zYfSjcMOnSpZi4jNKqrqiWK0nsLSH2e+rmcCq2+H7W5Ru0dxPVZC360Dlt4/dodZA2BtJPLj3UXIW2WfUDOOD4fBmX8iLh+4jIWCqWq+w5ECdLIeZJcz7Ema+AxOfVYXtXXspIwLKQDS8z4Wi3UiNqHkarsnS2R6ZXrt/66k0tp5Ka7ZtRHI+xeWVzR7fdiqdt/+I5smfwjgQqxIf1h9X6aLPrg0nKbeEhUO8sbM0Y9miEIK9HYlIzuf1zVHEZhZRUFrBvV8dwtbCjL9dU+caNDc1YWQPF3ZEZTSZHbn4q0PM+2gvOUXlnEjKo1rC3EF1SR2+rmpA79XFnkUjfdhpcG8FdbFnxoCuZBWVs+Tbw1RUVXMsIRcfFxscbSzwclblUJ9fF8H+2Gze2hKFiQBv57bNbNMGQtN6JIUq2ef0cFjzQH3xvH0fqkDt7b+owf/aZ9WADSrz54bPVG2DmoyegMkw9u8qnTTrDMx8S9UZADXY+o5Vnzt1q699VKNQevAz+HKWMiSDblcpo/6T1blDF4Nbr/p9NzWDwXeoAvagZhpndys3U2584/tomuVkSj5vbYk6Z3r57tOZ9Hl2IwnZxSTlKAOxKzqTs1lFTHx9O3d9cYi7vjjEqxtPNcrxP51ewIkr/usAACAASURBVIx3d/LqxrqHkMzCMlYdSkBKSUl5Fc+sPYGfmy1ezjY8vPIIOUXlbI5IY5ivM7aWZrjYWjCpd2dAZRV9ffcwfn5wJGYmgm/2neXpn08Ql1XMezcPoptj/VrV43q5kZRbwpmMonr74zKLOJNRRHpBGU/9dJxDZ1U84aahdTNP4yf+p6cFMcDLERsLU3xdbZnQy50Xr+/LH6fSeXn9ScIS8+jvqeJcNYbgdHohTjbmVFRJPJyssTBr2yFc/4/XtB5p4eA7BnpOgrUPwal1Ku+/IE2VsgyYqtw8TdF1gMroMeaaf9XXIzLGd5x6wnfsXn+/ay8Qpkrh1C1Qied1uogn/17TVNW101tUDYaG99E0y7f7z/LNvnjmhXjh0WBwreGznTGUVlRzPCmvdgZRVF7F7csOkJ5fxns3B7P7dBYfbD+Dl7NNvUH2zS3RVEv46XAiT04NxMrclI//PMOnO2Pp1dmeI/E5JGSX8N3iYdhamDH3wz0sWn6AzMIy/j0jCE8nayqrZKPB1b2TFdf17cJXe+OoqJI8PimgXkZRDWP93QD4LSyFh6/1JyqtACcbC7aeUjOgW4d7882+eDaGp+LhaE2vLva421uSWVhWOxMAsDAz4as7h5KYW4yZqYmhbXei0gr4Yk8cUsIAL2UgujlaYyJUptMHtwzm6Z+P09Pd7iL/hVqONhCa1qG8CHLiYMBN6rXnHVXF7/gPKihcVQET/9169/Mz+PMbxgXMrVQwuLJUFba3bfwH3iI8h4CNCxz9TpXfDJjy1/rbQSitqCIlr5TOnSyxsWh6+DiRpNI2jyXk1jMQUkoyCsooq6xmu8GHH5NRSFJuCSHdnQhLzONsVjFPTg1kRv9uTO/XlbDEXL7cE8fCIV4IIYhIzue3sBRG+LmwNyaL9cdTmBPswe8Ryh219lgyB+Oy6dOtEyN7qGqK943144PtZzA3FUwIdKeTlXmz3++24d35LSyFkO5OPDihZ5PneDnbMNzPmTe3RPHb8WSi0grp4WaLi60lPdxseWF2Xyb0cudgXA79PVUKc18PB85kFGJpVt/V6GBjjoNN/TTnxyf3Yv3xVDILyxjopY6Zm5rQzdEaO0szhvs5s+bBUZek5IZ2MWnOT3MrkwtS69xI6acAqVw8pmaqZoG5lcpM6n093L8LOvdpvT659FD6Q4HTGx+7fQ0s3nrxxgGUG8v/OjWDKCuoX4v5KuaZX04w4bXt9H12Ezuj6wK1y3bFMuWtHZRVVtWuDD7aQHPos52xDH35D279fD8CsLc0IyajiKScEvw72zGulxuBXey5e7RauySE4Lbh3TmVWkCowV3z2c4Y7CzN+PDWQfi42LDiQDzR6YWczSrG2tyUVQcTCEvMq+f3XzrRn16d7RkXcG7jADDM15n3bg7mw1sHY2rSfGbQl3cN5dFrAzA3NeGOEd05k1HEgbhsrgl0RwjBxKDOPDk1kGn91Oz1uZl9eP/mQc1ezxgHa3P+O7cfwd6O9OlWZzxevaE/r88fgBACBxtz7M/zXVoDPYPQnJvU40os7o514FNXb4KiTHirHwy9F657qS61tGaxWs9r4W+hbdcvIVTcoika6hVdLGOfUGm3A266ODdVK3IwLptFyw6w/uExbZ65ci4i0woJ7GJPTGYRO6MzGWNwt6w5msSp1AJ+OpxEmUGWwthAJOYU88bmKPzd7YjPLmZy7y7kl1YQkZJPVlE5Ho7WPDuzD1Kqp+UaZg3sxkvrT/LV3rMEdu3EhhOpXB/sgaONBbeP8OH5dRE8tuooAE9c14sX1kVgaiLqrXi2MjdlzZJRLVKBF0Iwo//5V0tbmpny8LX+PHytCmCbmZrw+a5Yrgns3OT53i4XFkye1LtzbYykhpE9XZs5u+3QMwjNuYnapIriZJysvz/xkFq/sPc9teo5LULpBzl1oJXrLj1gzGPtbhwAfjmaRFF5FRtOpF5QuyPxObz+eyQVVc1rCZ3NKmJbZHqzQeXqall7LDG7mGBvJ3p37VRrALKLyjmepHL239+mRAnH+LtyPDGPSsN9X1inFi9+cddQDjx9LW8uGIifmy2nDCuAPZyssTI3rU0nrcHGwoybhnrza1gyL66LoKSiihsHq9nBHSN9mBjozomkfIK9HblpqBc2FqaM9XfFzb7+Q4K1hSlW5m2XSfbk1EC+u2cYw/1aX+6iPdEGQnNuYg2y1nlJ9fcnhapgsEtPpYgas03VRdC1iFsdKSXbTil3ztaTzbj7UFlAcz7YzQnDYA3wxuYo3t16msdWHSMlr4SzWUWcTi+gtKKKpNwS5n+0l3H/286dyw/y5pbGqrZV1ZI7lh9gyXeHKS6vJKuoHE8nawZ6OXIiKY+qasmu05lICS62FiTmlGBjYcr1Az0oqagiOr2QxJxiNoWnsXiMLx6O1jjYmGNtYYqfa12Q1cOx+SfspRP98XSyZuXBBHxdbRnkrQK3piaCtxYO5Nogd+4b64eNhRnfLR7Oy3NboOHVypibmjCyp2ubLVhrL7SLSdM85cVqBTIYSmEakXRIuZNu+Ay+mqUE6WokJTStSlSaCuR6Ollz6Gw2ucXlONpY1Dvnq71xPLs2HClh5cF4XvToR15JBXvPZNHDzZZfjyXz67G6f0MbC1PMTARSwtPTAjmVWsA7f0RTUl7JPWP86NxJaQ29t/U0O6MzcbQxJ9GQjurpZE1XByu+2BPH6fRCdkRl4GBtzt1jfHl1YyS9u3ZikEE64mhCLtlFSjJlXohXvT77udW5yjycms52ArCzNON/Nw7gpk/3MT/Eq94gbG9lzmd3DKndHuh1HvkTzQWhDYSmeRL2KTeSiXmd7hGowHRSqBK8cw+EOzeogjZ9rm+/vnZgatIn/zU9iPu/OcyfURnMHlhXVzo9v5T/rj/FWH83qg2zDSkl2yPTqayWvHpjf/JLK0nNK8XC1AQTEzgYl0N6fhlPTwvEz82u1hX02a5Yvt53lnV/G0N5ZTVv/xGFk405OcUVHDXIQHg62eBgrQKkRxNy2Bmdweierkzu3ZlXN0bS18MBHxcbujpY8cmOGKSUDPV1xqvBoq4ebmoGYWoi6Gx/7rjRcD8Xtj0+/pyGRNP6aAOhaZ6Y7co49JwImVF1+7Nj1AIyj8Fq26UH3NVGldw0bItMJ6hrJyb37oKrnQWrQxOZNaBb7ZP0m1uiqayu5vnZfdh9Oounfz5OdHohv0ek4WpnyUAvp0YZOXOC60u2m5ma8Mb8gTw4vifT3tnJV3vjKC6vwsrclJfm9OPBbw/XGiovJ2tc7SyxtzTj1Y2RZBWVM7lPZ3q42fGv6UFMMGTyvL0wmJs/3UdlteT+cT0afa9ujmqhl5udZe06gHPh49p+wfmrFW0gNM2TcAA8BqlMnpoayULUieTVGAhNm1EjuXDr8O6YmAjuH9eDF387yWu/RxLs5cSm8FR+OpLEbcO7093Ftnbx1/LdcWw/lc6sgd3Oma7ZkJ7udszo15UfQxOpqJbMG+xJiMFdtOt0JhZmJrjaWWJiIujn6cCeM1ksHOJVa7DuGeNXe62hvs68cH1fvt57lmn9Gwf6TU0Efq62ODVwl2kuH7SB0DSNlKo+dJ850MlDLTwrzlZrCxIPqowlt8D27uVlS3JuCfd8eYiX5vQl2Nvpoq8TmVpAWWV17YKru0f7EpGSz/vblCiinaUZNw7y5NFJAYCSjejdtRMrDsTjbKtSQS+UW0d056cjyqV4x0gf3OwtcbA2J6+kAj9XW0wMBueWYcooPT+7T7PB2ZuGetdbBd2Q1+YNqJfWqrm80AZC0zRFGarAjlug0iECVVjHxlkV2vEZrQXozsHKA/FEpOTz5I/HWbd0dIsGwcScYk6lFGBhZsLYALW+IMwg+TzAoMkjhOC/c/sxsocr3V1s6Ofh0Ch9875xfmw5mc6/pgfVBpsvhGAvR4K9HbG3Miegs6p1HdDZjoNxOfViANP7d2V6EzODC6GvRxPFkjSXDdpAaJom45R6d+ulahmAymQytVSSGqMebreutSUJ2cUcjs9hRv8Lc80YU1Ut+SE0ka4OVkSmFbB8dyz3jq3zwSdkF9dq66w9lswwXxecbS2Y9vZO8kuVQum6v42mr4cDx5NycbA2r5WJBrVIq2YtQFPMHuhRL4h9oQgh+O6e4fUWlvl3tudgXA6eTrou9tWEnttpmqZGrtutFzgYBpu8RIgy1HHuoNpET/10nIdXHmXhJ3tJLyitdyy9oJQFH+9l1Ctbuf/rUIrKmpab3hGdQUpeKf+e0Ztrgzrz2qaoWqmITeGpjHl1G0/+GMavYSk8vPIo722LJiwxl/zSSp6aGoiNhSnLd8cBquBMf0+HS55f33BhWYBBGM5TZxFdVbSpgRBCTBFCRAohTgshnmzmnPlCiAghRLgQ4juj/VVCiKOG19q27KemCTIi1czBvivYuql6CPnJqlxn14F1bqcOxKnUfHadzmRioDtHE3L55M+Y2mNp+aUs+HgfYYl5DPByYGN4KqsOJVBQWsHX+86SaCgoL6Vk2a5YnG0tlHGY15+ujlbc9/Uh3twcxaPfH6WTlRk/hCby9x+Ueu32yAz2nslCCJgf4sWNgz359VgyiTnFRKYV1MYf2pOALsrV1DBVVdOxaTMXkxDCFHgfmAQkAgeFEGullBFG5/gDTwGjpJQ5Qgh3o0uUSCkHtlX/NEaU5KrynzW4BigXk1svlbUkTMG+m8pkSj4C45u09Vc8y3bFYm1uyuvzB3Df16HsNxSaAfj4zxiSckpYce8wBnd3Zu4Hu1m2O5bQszmsC0vBRKjU0YDOduyMzuTfM3pjYWaChZkFn98Rwl1fHOLtP6LxcrZm1X0jeOKHY4SezeHu0b58viuW7w8lENilE062Ftwx0oev9p5l0fKDVFXL2poA7ckwXxdeuL4vk3s3rTWk6Zi0ZQxiKHBaShkDIIRYCcwGjCvKLwbel1LmAEgpm9cR0LQdX82CFKNaDF0HKGkNYzdSp25q4ZyDtyrAc4UipWR7VAZjerrWy70/nV7AmqPJzA/xxNHGQql6bjtNQWkFNhZm/BqWzIRANwZ3V1o794zx48FvD5OQXcLiMUp/atnuOKqqJaN7unLnSJ/aa/d0t2fHPyZQWlGFmYnAzNSE5YuGkl1UTpWUfL4rlsScEu4a1QVQC8j+OS2I7w7EY29pVlvQvj0xNVHKqpqri7Y0EB6A0WMpiUDDqu0BAEKI3YAp8JyUsmbFlZUQ4hBQCbwipVzT8AZCiHuBewG8vS+yXvDVTnWVEtrrPVvJZ+echd//qY4ZV13zDIGyfFVj4Qp2L4WezeHO5Qd59Yb+zB+ipB+Kyyt54JvD2FvWlZcc5ufCO1tPE3o2BzMTEzIKyrjeKPA7uXfn2hoA/zclEDNTE2YN8ODb/Wd5bFJAbSqoMcY+fQszE7o4qAyjXp3tiUwrqFecZvFYPxaP9aO6WjZ5LY3mUtDeWUxmgD8wHvAEdggh+kkpc4HuUsokIYQfsFUIcVxKeca4sZTyE+ATgJCQkObrG16N7P8EDn4KC74Ft4Dmz8tLhOoK6DFRVX8DyDoNocvrr3OY/KJaG3GFi/HVKJD+HpHKjYM9WbY7lm/3xxOXVcTXdw2rTQsN9nbEzESwPzabzIIy7C3NmBBY5wE1MzVh7UOjsTI3rc126ufpwCue/S+4T9f2dic2s4ihvo2VQLVx0LQnbWkgkgBjdS5Pwz5jEoH9UsoKIFYIEYUyGAellEkAUsoYIcR2IBg4g6ZlhH6h5DGWT1UyGK7+TZ+XbQjEOtetgGXKf9Uq6R4T6vYJQYsE9duZyqpqJDS77qBG6XRHdCarDiXw4m8nCenuxJNTAxntX6e3b2NhRj9PB344lEh+SQWzB3ZrtN7A1rJ1/nwemuDP9QM9avWNNJrLhbZ8HDwI+AshfIUQFsBCoGE20hrU7AEhhCvK5RQjhHASQlga7R9F/diF5lzkxqsCPiF3Q3khHFqu9leUKpeS8edsg811MdLKMbeGQbeB6eU3YEWlFVBSXtXksS0RaQT8awP+/9zAc2vDmzwnLCkPN3tLyiureWZtOL0627PqvhFc16dLo3NH93Qls7CMiUHuPDm17VaNW1uY4m9YkKbRXE60mYGQUlYCDwGbgJPAKilluBDieSHELMNpm4AsIUQEsA34u5QyCwgCDgkhjhn2v2Kc/aQ5D5GGMM6IJarMZ2qYMgbvDoJ3B8OaJfD/fOCP/0B2LJhZg13jAfJy40RSHlPe2sEbmyNr90Uk5/PUT2GUVlSxbHcs7vZWjOzhwooD8eQVV9RrX1hWSWxmETcN9cbRxpzyymoem9x0vADgwfE9+W3paD68dTAudq1UpU6juYJo0xiElHI9sL7BvmeMPkvgMcPL+Jw9wKWv+tFRiFwPLv5qVtB1AJz4UZUOzU9S6xrCVoK1M5xcpwLRzr7tFlsoLq/ExuL8/w2rqiX//Pk41RJ+PJzE368LxMLMhJUH41lxIIHqathzJosnJgcwvpc7M97dxU9HErlzlC95xRW8ty2afp6OSKmkJEorvIhIzj9n2qa1hWm9msAazdXGlR1x1DSmvAjidkEvQ4pql/5Kmjtsldq+ezP8MxVGP6rcS/H76scfLiGH43Po/9zvRCTnNzoWm1lEdXVd3sGyXbEcS8zjhkGeZBeVs+VkGgD7Y9Rahe8PJWBqIpgX4kVfDwcGejny7f54pJSsOBjPpztjecKwMK2vhwNPTwvim3uGdbgKYBpNa6INxJVOShic2Va3nRmlspI8h6rtroasmiPfgIMXOHqp2ILfeLW/JFvNINqBX44kUVkt2ReTVW9/RHI+17y+nXe3qvrGv4Wl8PKGk0zq3ZlXb+xPNwcrVh5MILuonMi0Am4e5o2VuQnXBLrXZiHdMsyb0+mFbI/MYM2RJFztVNyhSyerRvWKNRpN07R3mqvmr7LpaUg6DI+fBCsHyDIEnWuyltz7qJXQZXkQMLmunXsQ2LpDUTo4Ny7m0tZIKfk9Qs0CjhvVUAb4ITQBKeH9bacxEfDO1mgGezvxzsLg2lnCO1uj+XJPHABzgz1YNNIHF9u6ugKzB3rw3rbTPPXTcVLzS3lhdh8qqyVmOm1Uo2kxegZxJVNdBclHoaIIjq1U+zKjAQFOhlmBuVXdegbv4XVthaibRbSDiyksMY+UvFIszUzqGYjyymp+OZrMyB4uWFuY8vrmKIb4OPP5HUOwtlBppneN8qWTlTnvbI3GytyE/p6OBHS2rxdItjAz4e/X9SI1vxQzE8H0/t24c5Qvt11EfQSN5mpFG4grmcxoKC9QQnoHPoHqasiKBkdvZRhqqHEzeY+o3z5wupLvdg9qsy6ezSoir6Si0f6N4amYmghuGurNmYzCWmXU7ZHpZBeVc/doX967OZjHJgXw5V1DcbCpS7l1sDFnyYQeSAmDvJ1qq6g1ZHq/rgz1dWZqv6442+qqZRrNhaJdTFciFSUgqyHpkNoe/Sjs+B/E/qmMRsNFcX1vgJIccGtgCHrPVrMI67YRg5NScuNHe5Wu0c2DSM4tITm3hONJeXy+K5bRPV0Z4+/KF3viCE/OJ7CrPW9ticbVzpKxAW6Ym5owxt+tyWvfPsKHTeFp9eQvGiKEYOXi4VfC+j6N5rJEG4grkZ/uVUV7ugUrSe4xj8PeDyDiFxWD6D6y/vn+k9SrIUK0qnHYF5PF6tBEXpnbDzNTE5JyS8goKOP3iDQyC8uY88Fu0vLLABgX4Mbr8wfUZirtjM7g1Y2niE4v4JPbQs5bgc3K3JQfHxh5znNAS1VoNH8FbSCuRFKOQe5ZJcntPUKtfO55jVrvUFEELj3bpVvLd8eyKTyNIT5OLBjiTbghfbW8spoHvgklLb+M52f3oYebHSP8XGoH786dLHl362nMTQVvLwyup3mk0WjaDx2DuJzJTVCuIWMqy+tqN1SVK5VVgICpSm0VmtddakPKKqvYFZ0JwFtboimtqCIiOR8hwMPRmoNxOQR7O3Lb8O6M6ula78l+at+uDPBy5Jclo5nW76/VONZoNK2HNhCXMysWwvLpSjephtx4FX/oN19t17iT/CcDhkG3HWYQB2NzKCqv4u7RvqTklfLt/njCk/Pxc7VlXoiqn/zg+J5NLkx7blYfflkyit7dOl3qbms0mnOgXUyXK1IqnaSKItj6Alz3ktpfo746dDFMfAYcDMXr7dzAcwiknVDV39qA9PxSbv5sPwKYEOjO09Pqgt5bT6VjYWbC45MDCEvM5cs9cVRUVTPEx5l7xvjRw82Oa4O060ijuZLQM4jLlbICZRxsXGDvexD6pdpvLM/t6FVfgnviMzD5hTbTVfp4RwyxmUVYmZvy6c4YMgtVwFlKybbIdEb4uWBjYcYdI32Izy4mJa+UPt06YWdpxswB3bSshUZzhaENxOVKQap6v/Y56DkJfl2qZLuzY1Tmko1L4za+Y2DIPW3SnazCMr7df5bZA7rx37n9kFLNGgC+OxBPbGYRM/qr+MF1fbrQuZNatKbdRhrNlYs2EJcrhQYD4eQLC7+D7qNg+ytqIZyz7yUv3vPRn2coq6zmwQk96dOtE10drPjjZBpnMgp5YV0EY/xduWGQcneZm5pw+wgfLMxM6KvVUDWaKxZtIC5XamYQ9l3BzELNDApTIebPNpHGKCmv4vuD8fUUVGvYH5PF57timT/Yi57udgghmBjkzs7oTO758hDW5qa8Nm9AvcykB8b1YOvj43DSK5g1misWbSAuVwpS1Lu9oV5Br6lgYQeyqk0MxA+hCfzfj8fZ20BZNa+4gke/P4q3sw3/ntm7dv/EoM4Ul1eRklfCZ3eE1Kqo1mBiIvB0smn1fmo0mkuHNhCXKwWpyiBYGkpRmltD0Ez1uQ0MxDZDPCH0bN26CyklT/0cRnpBGW8vDMbOqAbzyB4uzB3kwce3hTC4u3Or90ej0bQ/2kBcrhSkgH2DMqADbwGEqhLXipRWVLHnjJo51BgIKSVf7Ilj/fFUHpscwACv+pIclmamvDF/IOMCmtZK0mg0Vz56HcTlRlkBmFlBQZqKPxjjOwb+EQM2rfvEvjcmi7LKarydbTgcn0N+aQWPrDzK1lPpjA1w476xl75ehEajaX/0DOJyQkr4eCz8/u+mZxDQ6sYB4M/IDKzMTbh3rB8FpZU89v1RtkWm86/pQSy7IwRTLXin0VyV6BnE5UR2jHqF/wSl+WDXuW1vV1TOv9YcZ1N4GtcEujOqpysAW06mMzfYg3vGtE+tao1Gc3nQIgMhhPgJ+BzYIKWsbtsuXcXE71PvhaoUZyMX01+gqloy491dpOaVMMjbifdvGcSnO2PYFJ7GPaN9WTzWDxdbC5xtLSgsreSxyQGtdm+NRnNl0tIZxAfAncA7QogfgOVSysi269ZVSvxesLBXEhuyumkX00VyIDabkyn5jPBz4Y9T6aw9lszao8mM8XflKSNNpb9d0xMLMxOdoqrRaFoWg5BSbpFS3gIMAuKALUKIPUKIO4UQ5udurWkx8fvAZzR4GWpHt+IM4rfjyVibm/L5ohACOtvxyoZTJOWWMHtgfWG/O0f5csuw7q12X41Gc+XS4iC1EMIFWATcAxwB3kYZjM1t0rOrjcIMJaPhPRwCp6l9NUqtLSQ9v5QIQ5EeY6qqJRtPpHJNoDs2FmbcPsKH7KJyrMxNmNS79WYpGo2mY9HSGMTPQC/ga2CmlNKwzJfvhRCH2qpzVxXxe9W79wjoNhA69wGnC3uSf/rnE+w9k8n+f15LVbVkxYF4dkRlYGFmQmZhOdMNYnpzgj14deMpJgS611v8ptFoNMa0dHR4R0q5rakDUsqQVuxPx6eyDMoKwbaBGuvxH8DaSRkHM0vocc0FXTa3uJw/o9KpqJL8cjSJP06ms/VUOoFd7EkvKMPF1oIJvVQ9BltLM35bOgYHG+0d1Gg0zdNSA9FbCHFESpkLIIRwAm6SUn7Qdl3roOx4DQ5/CY+dqqvbkJcIp36DkQ8p43ARbDyRSkWVxMXWgjd+jyKrqJwnpwZy/7geVFdLKqqrsTQzrT3fy1kHoTUazblpaQxicY1xAJBS5gCL26ZLHZyUoyqNNSe2bt+hZYCEkLsv+rK/hiXj62rLI5MCyCoqx8vZmkUjfQAlnGdsHDQajaYltNRAmAqjcmBCCFNA6zhfDJnR6j3lqHovylSFgAKmXnDMoYak3BL2nsliZv+uzAn2YKCXI8/P6ouVuTYKGo3m4mmpi2kjKiD9sWH7PsM+zYVQWQa5Z9XnlDDoMxfW/g3KC+Gaf170Zd/ZEo2ZiQkLhnpjZ2nGmiWjWqnDGo3maqalBuL/UEbhAcP2ZuCzNulRRyYnTi2AA0gNg7DvIXI9XPeyylq6CE6nF/JDaAKLRvri4Wjden3VaDRXPS0yEAZ5jQ8NL83FUuNecu8DKceUYmvnfjDsgXO3OwcfbDuNtbkpSyZoxVWNRtO6tCgGIYTwF0KsFkJECCFial4taDdFCBEphDgthHiymXPmG64bLoT4zmj/HUKIaMPrjpZ/pcuYLIOB6DsHirMgPRyG3VeXzXSBlJRXsTE8lVkDu+Fid3HZTxqNRtMcLXUxLQeeBd4EJqB0mc45qhkC2e8Dk4BE4KAQYq2UMsLoHH/gKWCUlDJHCOFu2O9suF8IIIFQQ9uchve5osg8Dbbu4DNGbVs7Qb8bL/py2yLTKS6vYkb/buc/WaPRaC6Qlj66Wksp/wCElPKslPI5YPp52gwFTkspY6SU5cBKYHaDcxYD79cM/FLKdMP+64DNUspsw7HNwJQW9vXyQUo48CkkH1HbWafB1R869wVzGxh8pyolepGsC0vG1c6CYb665KdGo2l9WjqDKBNCmADRQoiHgCTA7jxtPIAEo+1EYFiDcwIAhBC7AVPgOSnlxmbaejS8gRDiXuBeAG9v7xZ+lUtI8mFY/4T67DcB0k9Cn+vB0g6W7Af7TM5XPAAAF15JREFU8z/5l1ZU8cOhBI4k5BLS3Zmbh6nvmZZfytZT6cwb7IWZqa77pNFoWp+WGoiHARtgKfACys3UGnEBM8AfGA94AjuEEP1a2lhK+QnwCUBISIhshf60LinH1PvIpXBsJZTlgVug2ud4foNWVS15eOURNoWnYW9pxk+Hk0jLL8XcVPDxjhiqJSwY4tWGX0Cj0VzNnNdAGGIJC6SUTwCFqPhDS0gCjEcvT8M+YxKB/VLKCiBWCBGFMhhJKKNh3HZ7C+97+ZByDKwcYdLzMOFpOLMNfMe2uPn/NkWyKTyNf8/ozR0jurN05RHe/kMFusf4u/KfWX3wczvfRE6j0WgujvMaCClllRBi9EVc+yDgL4TwRQ34C4GbG5yzBrgJWC6EcEW5nGKAM8DLBs0ngMmoYPaVRUoYdOkHQqhYQ42MdwtILyjl810x3DjYk7tH+wLw1oJgZvZPY4CXI930mgeNRtPGtNTFdEQIsRb4ASiq2Sml/Km5BlLKSkO8YhMqvrBMShkuhHgeOCSlXGs4NlkIEQFUAX+XUmYBCCFeQBkZgOellNkX+N3al6oKSAuHoRcnWbXqYAIVVZIHxtetb7AwM2Fqv9YrIqTRaDTnoqUGwgrIAow1qCXQrIEAkFKuB9Y32PeM0WcJPGZ4NWy7DFjWwv5dfmRGQVUZdB1wwU0rq6r5bn88o3u60kO7kDQaTTvR0pXULY07aGpICVPvXfpfcNPtkRkk55XyzMzerdwpjUajaTktrSi3HDVjqIeU8q5W71FHITUMzKzVuocL5JdjyTjZmDMxqHMbdEyj0WhaRktdTOuMPlsBc4Dk1u9OByL5CHTpCyYXJrldUl7FHyfTmD3QA3O9vkGj0bQjLXUx/Wi8LYRYAexqkx51BKoqIfkoDF50wU23nlLyGTP762C0RqNpXy72EdUfcG/NjnQoMk5CZQl4DL7gpr8dT8bVzpJhfi7nP1mj0WjakJbGIAqoH4NIRdWI0DRFUqh69xh0Qc1KK6rYeiqd+SFemJqI8zfQaDSaNqSlLib7tu5IhyIpVCm1OvtdULPdpzMprahmUm8dnNZoNO1PS+tBzBFCOBhtOwoh/n979x4eVX3ncfz9JZBwSQiXhDsCKgLiFSO1Wq3VqtS2UJe661pb7c3dfbTVp+22+tha1z7dbnefdnf71K66lpZ2bWW12lJXa8VdaWm9AYZrQAGDEEKuQBJuIZnv/nFOcBgmwwQ5cybk83qeeebMb87JfPObyXzz+/3O+f0+Fl1YvVzNyqB7yXrWClhSVUdxUX/eM0XdSyISv2zHIL7p7nu6Hrj7boL1GiTVwTaoX9/j8YdEwllSVc/7p5VT2F9nL4lI/LL9Jkq3X7anyPYtDRuDdad7eAX16po9NLQe5Cpd+yAieSLbBLHczL5vZqeFt+8DK6IMrNdqrQ3uhx61fMVh7s4/PlPF2prDjTKWrK+joJ9x+bTyqCMUEclKtgniC0A7sIhgZbgDwG1RBdWrtdUF98XdtwTqWw/y8B+28OgrWw+XLamq48LJwxk2uDDqCEVEspLtWUx7gbsijuXk0FYHGAzpviWwpSGYEHfF1mCJ7W3N+9iws5Wvf3hGLiIUEclKtmcxPW9mw5IeDzez56ILqxdr3QlDyqCg+9xb3RQkiDfq2tiz/xBLqoJWh05vFZF8km0XU1l45hIA7r4LXUmdXls9FI/JuEt14+ElNXj97V0sqapj6qhiJo0cEnV0IiJZyzZBJMzs8CLKZjaZNLO7CtC2E0oytwTeatzL+GGDKOhn/Pylrby0uYlrZmZOKiIiuZbtqar3AMvMbClgwKXArZFF1Zu11sGozOs4VDftZcbYoQwfMoAXNtQzqqSIz1/Ws6uuRUSillULwt1/B1QAG4FfAl8G9kcYV++USMDe+oxnMCUSTnXTPqaUDeaCU4Ilt/9h7kxKBw3IVZQiIlnJdrK+zwF3ABOASuAi4CWOXIJU9jdDoiNjgtixZz/tHQkmlw3hsqnlnDW+lDlnqXtJRPJPtmMQdwAXAlvd/QPA+cDuzIf0EVtfgrdfDra7roHIMAZR3bgPgCllQ5g4YjDXV0zEejhnk4hILmQ7BnHA3Q+YGWZW5O4bzGxapJH1Bp2HYNFNsH8XXPcQDB4RlGc4i+mt8BTXKWU6Y0lE8lu2CWJ7eB3Er4HnzWwXsPUYx5z8tiyFfY1Qego8+XmoCJfoLu7+DODN9W0MGlDA6JKBOQpSROT4ZDtIfZ2773b3+4BvAD8GNN33msdhYCn8zVIoGgorFwblKWMQL26sZ8GytwB4rbqZ8yYOo58WBBKRPNfjeaXdfam7L3b39igC6jXa98GGp+HMeUHX0vk3BQPUhcVQVHx4tx279/OFX7zOt5+pYlvzPqpqW5g9ZUSMgYuIZEcLDxyvt/4A7W1w1vzg8YWfBeyI1kN7R4KvPrGa/Yc66Uw43/3dBhIO71GCEJFeQAnieO2qDu5HnxXcjzwtSBbhQkFVtS18+Ad/ZNmmRu6fdxbjSgfy9Opa+vczzg+vfxARyWda9Od4tdRAQREMTloedP4jh5cZ/c6zG2ja286CWyq4Yvpo3qhr5ad/rubsCaUMKiyIKWgRkeypBXG8Wmpg6Lgj150Ot92ddTV7uHL6KK6YHnQ5dc21NHuyupdEpHdQC+J4tezodtW4upaDNO1tZ+a4oYfLZk8Zwe0fOJ3rKybkKkIRkXdFCeJ47amBSe9N+9T62mAp0ZnjSw+XFfQzvnKNri0Ukd5DXUzHI5GA1h1BF1Ma62paAJg+piSXUYmInFBKEMdjb31wzUM3XUzrdrQweeRgSgZqhlYR6b2UII5HS01w302CWF/bwsxxpWmfExHpLSJNEGY2x8w2mtkmM7srzfO3mFmDmVWGt88lPdeZVL44yjh7bE+YIEqPThAtBw7xdvM+zkwaoBYR6Y0iG6Q2swLgAeAqYDvwmpktdvf1Kbsucvfb0/yI/e5+XlTx9VjTZhg0PJhWo2VHUJamBfHS5iYAzp0wLJfRiYiccFG2IGYDm9x9Szhv02PAvAhfLzqJTvjxVfDgpdC4CVq2H32RXOhXK7ZTXlLERafqegcR6d2iTBDjgW1Jj7eHZanmm9lqM3vCzCYmlQ80s+Vm9rKZpZ051sxuDfdZ3tDQcAJDDx3YA50dUL8e9jUFZy79ZA5U/+noi+SApraD/O+Geq47fzz9CzS8IyK9W9zfYr8FJrv7OcDzwMKk5ya5ewVwI/BvZnZa6sHu/rC7V7h7RXl5+YmNLNEJP7wQXvzOOyvG3fg4FBTCjpVQevQFb4tX7aAj4cyfpYvhRKT3izJB1ADJLYIJYdlh7t7k7gfDh48AFyQ9VxPebwFeJFjmNHeaNgdLiFY+CtXLoGQcnH4lfPpZGDkVxh09PPL06lrOHDuUabr+QUROAlEmiNeAqWY2xcwKgRuAI85GMrOxSQ/nAlVh+XAzKwq3y4BLgNTB7WjtXB3ct9YG6z6cclHQpTR8Etz2Clz1rSN2bz1wiMptu7lieveryYmI9CaRncXk7h1mdjvwHFAALHD3dWZ2P7Dc3RcDXzSzuUAH0AzcEh4+A3jIzBIESeyf0pz9FK3ayqA7qaAwWPdh0sXvPNfv6NlYX9nSTGfCufj0oweuRUR6o0jnYnL3Z4BnUsruTdq+G7g7zXF/Bs6OMrZjql0No86EUTNg1S+DFkQGyzY1MnBAP2ZprQcROUlosr503IMuphlz4eIvQMkYGDUz4yF/2tTIhZNHMHCA1noQkZND3Gcx5ac922H/Lhh7DpRNhQ/eB/26r6r6lgO8Wd/GJaeX5SxEEZGoKUGkU7squB9zbla7/+ylrQAaoBaRk4oSRDrbXgErgNGZu5UA6loO8MiyLXz03HGcMVqnt4rIyUMJIlUiAet+DaddAYWDj7n7v7/wJp0J5++v1mJAInJyUYJIte0V2PM2nH39MXdtO9jBkyu3M3/WBE4ZeexkIiLSmyhBpFrzOPQfBNM/fMxdn11Ty4FDCa0zLSInJZ3m2qXzEKxeBGuegOnXQlHxMQ95cmUNk0YO1rUPInJSUoLo8uJ34I/fgzFnw/u/lnHX31TW8PKWZl5+q4k7rzwDS5nVVUTkZKAEAcGFcWt/FQxM3/TkUdN4J6vctps7F1UyeEABZcVFzL8g/bKjIiK9nRIEQMNG2FUNF38xY3Lo6Exwz1NrGFVSxAtfvpziIlWfiJy8NEgN8Mazwf0ZczLu9uTrNazb0cK9H5mp5CAiJz0lCICNz8LYc6G0++4id+cnf6pm+pgSrj17TA6DExGJhxLE3kbY9ipMuzbjbsu37qKqtoWbL56sQWkR6RPUT9K/COb+AE65OONuP/1zNUMH9mfeeeNyFJiISLyUIIpKYNanMu7yQlUd/7O6lr+7/DQGF6rKRKRvUBfTMdTs3s+X/nsVM8cN5Y4rp8YdjohIzihBHMMTy7fTcuAQP/rELC0GJCJ9ihLEMWzY2cKkEYOZNHJI3KGIiOSUEsQxbNjZyoyxQ+MOQ0Qk55QgMtjX3kF1016mj1GCEJG+Rwkigzfq2nCH6WO1UpyI9D1KEBlsqG0BYIZaECLSBylBZLBhZytDCguYMHxQ3KGIiOScEkQGVbUtTBtTQr9+mlpDRPoeJYhu7NxzgHU7Wpim7iUR6aOUINLYs+8QNy94FXfnU++dFHc4IiKx0MRCafzoxU1sbmhj4Wdm6xoIEemz1IJI4e48u3Ynl5xexiWnl8UdjohIbJQgUmzY2crbzfuYc5YWBRKRvk0JIsVz63ZiBh+cMTruUEREYqUEkeK5dXVUTBpOeUlR3KGIiMRKCSJJfcsBqmpb1HoQEUEJ4gjLt+4CYPaUETFHIiISv0gThJnNMbONZrbJzO5K8/wtZtZgZpXh7XNJz91sZm+Gt5ujjLPL8updDBzQj5njSnPxciIieS2y6yDMrAB4ALgK2A68ZmaL3X19yq6L3P32lGNHAN8EKgAHVoTH7ooqXoAVW5s5d8IwCvurYSUiEuU34Wxgk7tvcfd24DFgXpbHXgM87+7NYVJ4HpgTUZxAsPbD2h0tVEweHuXLiIj0GlEmiPHAtqTH28OyVPPNbLWZPWFmE3tyrJndambLzWx5Q0PDuwq2cttuOhNOxSSNP4iIQPyD1L8FJrv7OQSthIU9OdjdH3b3CnevKC8vf1eBrKgOeq9mnaIWhIgIRJsgaoCJSY8nhGWHuXuTux8MHz4CXJDtsSfa2837GD20iNLBA6J8GRGRXiPKBPEaMNXMpphZIXADsDh5BzMbm/RwLlAVbj8HXG1mw81sOHB1WBaZxraDlBXr4jgRkS6RncXk7h1mdjvBF3sBsMDd15nZ/cByd18MfNHM5gIdQDNwS3hss5l9iyDJANzv7s1RxQrQ2NauBCEikiTS6b7d/RngmZSye5O27wbu7ubYBcCCKONL1th2kDNGl+Tq5URE8l7cg9R5wd1pamunrKQw7lBERPKGEgTQsr+D9s4E5epiEhE5TAkCaNwbnEilMQgRkXcoQQCNrUoQIiKplCAIzmACNAYhIpJECYLgDCZQC0JEJJkSBEGC6GcwfLBaECIiXZQgCBLEiCFFFPSzuEMREckbShBAQ2s7ZcVqPYiIJFOCIGhBlJdo/EFEJJkSBJqoT0QknT6fINw9TBDqYhIRSdbnE8Te9k4OHEqoBSEikqLPJ4hDHQk+cs5YZowdGncoIiJ5JdLpvnuD4UMK+eGNs+IOQ0Qk7/T5FoSIiKSnBCEiImkpQYiISFpKECIikpYShIiIpKUEISIiaSlBiIhIWkoQIiKSlrl73DGcEGbWAGx9Fz+iDGg8QeGcSIqrZ/I1Lsjf2BRXz+RrXHB8sU1y9/J0T5w0CeLdMrPl7l4RdxypFFfP5GtckL+xKa6eyde44MTHpi4mERFJSwlCRETSUoJ4x8NxB9ANxdUz+RoX5G9siqtn8jUuOMGxaQxCRETSUgtCRETSUoIQEZG0+nyCMLM5ZrbRzDaZ2V0xxjHRzP7PzNab2TozuyMsv8/MasysMrxdG1N81Wa2JoxheVg2wsyeN7M3w/vhOY5pWlK9VJpZi5ndGUedmdkCM6s3s7VJZWnrxwI/CD9zq80sshWruonrX8xsQ/jaT5nZsLB8spntT6q3B6OKK0Ns3b53ZnZ3WGcbzeyaHMe1KCmmajOrDMtzVmcZviOi+5y5e5+9AQXAZuBUoBBYBZwZUyxjgVnhdgnwBnAmcB/wlTyoq2qgLKXsn4G7wu27gO/G/F7uBCbFUWfAZcAsYO2x6ge4FngWMOAi4JUcx3U10D/c/m5SXJOT94upztK+d+HfwiqgCJgS/t0W5CqulOe/B9yb6zrL8B0R2eesr7cgZgOb3H2Lu7cDjwHz4gjE3WvdfWW43QpUAePjiKUH5gELw+2FwMdijOVKYLO7v5ur6Y+bu/8BaE4p7q5+5gE/88DLwDAzG5uruNz99+7eET58GZgQxWsfSzd11p15wGPuftDd3wI2Efz95jQuMzPgL4FfRvHamWT4jojsc9bXE8R4YFvS4+3kwZeymU0GzgdeCYtuD5uIC3LdjZPEgd+b2QozuzUsG+3uteH2TmB0PKEBcANH/tHmQ511Vz/59Ln7DMF/mV2mmNnrZrbUzC6NKaZ0712+1NmlQJ27v5lUlvM6S/mOiOxz1tcTRN4xs2LgV8Cd7t4C/AdwGnAeUEvQvI3D+9x9FvAh4DYzuyz5SQ/atLGcM21mhcBc4PGwKF/q7LA466c7ZnYP0AE8GhbVAqe4+/nAl4BfmNnQHIeVd+9dir/myH9Ecl5nab4jDjvRn7O+niBqgIlJjyeEZbEwswEEb/yj7v4kgLvXuXunuyeA/ySiZvWxuHtNeF8PPBXGUdfVZA3v6+OIjSBprXT3ujDGvKgzuq+f2D93ZnYL8BHgE+GXCmH3TVO4vYKgn/+MXMaV4b3LhzrrD/wFsKirLNd1lu47ggg/Z309QbwGTDWzKeF/oTcAi+MIJOzb/DFQ5e7fTypP7jO8DlibemwOYhtiZiVd2wSDnGsJ6urmcLebgd/kOrbQEf/V5UOdhbqrn8XAp8KzTC4C9iR1EUTOzOYAXwXmuvu+pPJyMysIt08FpgJbchVX+LrdvXeLgRvMrMjMpoSxvZrL2IAPAhvcfXtXQS7rrLvvCKL8nOVi9D2fbwQj/W8QZP57YozjfQRNw9VAZXi7Fvg5sCYsXwyMjSG2UwnOIFkFrOuqJ2Ak8ALwJrAEGBFDbEOAJqA0qSzndUaQoGqBQwR9vZ/trn4Izip5IPzMrQEqchzXJoK+6a7P2YPhvvPD97cSWAl8NIY66/a9A+4J62wj8KFcxhWW/xT425R9c1ZnGb4jIvucaaoNERFJq693MYmISDeUIEREJC0lCBERSUsJQkRE0lKCEBGRtJQgRPKAmV1uZk/HHYdIMiUIERFJSwlCpAfM7CYzezWc+/8hMyswszYz+9dwjv4XzKw83Pc8M3vZ3ll3oWue/tPNbImZrTKzlWZ2Wvjji83sCQvWang0vHJWJDZKECJZMrMZwF8Bl7j7eUAn8AmCq7mXu/tMYCnwzfCQnwFfc/dzCK5k7Sp/FHjA3c8FLia4aheC2TnvJJjj/1Tgksh/KZEM+scdgEgvciVwAfBa+M/9IIKJ0RK8M4HbfwFPmlkpMMzdl4blC4HHwzmtxrv7UwDufgAg/HmvejjPjwUrlk0GlkX/a4mkpwQhkj0DFrr73UcUmn0jZb/jnb/mYNJ2J/r7lJipi0kkey8AHzezUXB4LeBJBH9HHw/3uRFY5u57gF1JC8h8EljqwUpg283sY+HPKDKzwTn9LUSypP9QRLLk7uvN7OsEK+v1I5jt8zZgLzA7fK6eYJwCgqmXHwwTwBbg02H5J4GHzOz+8Gdcn8NfQyRrms1V5F0yszZ3L447DpETTV1MIiKSlloQIiKSlloQIiKSlhKEiIikpQQhIiJpKUGIiEhaShAiIpLW/wNP7f9mQHvuZAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(hist_swn.history['accuracy'])\n",
        "plt.plot(hist_swn.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# \"Loss\"\n",
        "plt.plot(hist_swn.history['loss'])\n",
        "plt.plot(hist_swn.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g15vdSkQxykx",
        "outputId": "b15a2292-9a77-4b9e-e2dc-8ea37c41a41c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92/92 [==============================] - 1s 8ms/step\n",
            "Accuracy: 49.01%\n",
            "\n",
            "F1 Score: 49.01\n"
          ]
        }
      ],
      "source": [
        "predictions_swn = model_swn.predict(X_test_pad)\n",
        "predictions_swn = np.argmax(predictions_swn, axis=1)\n",
        "predictions_swn = [class_names[pred] for pred in predictions_swn]\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(test_df.label, predictions_swn) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions_swn, average='micro') * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2W64uqkxyky",
        "outputId": "a411512e-f47c-4db7-c905-88ab0f89fe39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 1.0406 - accuracy: 0.4898\n",
            "Epoch 1: val_accuracy improved from -inf to 0.48549, saving model to best_liu.h5\n",
            "330/330 [==============================] - 20s 39ms/step - loss: 1.0406 - accuracy: 0.4898 - val_loss: 0.9956 - val_accuracy: 0.4855\n",
            "Epoch 2/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.9624 - accuracy: 0.5372\n",
            "Epoch 2: val_accuracy improved from 0.48549 to 0.62116, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.9621 - accuracy: 0.5374 - val_loss: 0.8776 - val_accuracy: 0.6212\n",
            "Epoch 3/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8826 - accuracy: 0.5970\n",
            "Epoch 3: val_accuracy improved from 0.62116 to 0.66297, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8826 - accuracy: 0.5970 - val_loss: 0.7592 - val_accuracy: 0.6630\n",
            "Epoch 4/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8363 - accuracy: 0.6305\n",
            "Epoch 4: val_accuracy improved from 0.66297 to 0.69881, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8362 - accuracy: 0.6304 - val_loss: 0.7047 - val_accuracy: 0.6988\n",
            "Epoch 5/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.8003 - accuracy: 0.6468\n",
            "Epoch 5: val_accuracy improved from 0.69881 to 0.72867, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.8003 - accuracy: 0.6469 - val_loss: 0.6636 - val_accuracy: 0.7287\n",
            "Epoch 6/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7632 - accuracy: 0.6719\n",
            "Epoch 6: val_accuracy improved from 0.72867 to 0.73976, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.7632 - accuracy: 0.6719 - val_loss: 0.6336 - val_accuracy: 0.7398\n",
            "Epoch 7/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7506 - accuracy: 0.6792\n",
            "Epoch 7: val_accuracy did not improve from 0.73976\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.7508 - accuracy: 0.6789 - val_loss: 0.6192 - val_accuracy: 0.7363\n",
            "Epoch 8/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7289 - accuracy: 0.6933\n",
            "Epoch 8: val_accuracy improved from 0.73976 to 0.75256, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.7290 - accuracy: 0.6931 - val_loss: 0.5950 - val_accuracy: 0.7526\n",
            "Epoch 9/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7313 - accuracy: 0.6878\n",
            "Epoch 9: val_accuracy did not improve from 0.75256\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.7312 - accuracy: 0.6880 - val_loss: 0.5879 - val_accuracy: 0.7415\n",
            "Epoch 10/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7245 - accuracy: 0.6967\n",
            "Epoch 10: val_accuracy improved from 0.75256 to 0.76536, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.7247 - accuracy: 0.6964 - val_loss: 0.5678 - val_accuracy: 0.7654\n",
            "Epoch 11/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.7115 - accuracy: 0.6924\n",
            "Epoch 11: val_accuracy did not improve from 0.76536\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.7115 - accuracy: 0.6924 - val_loss: 0.5546 - val_accuracy: 0.7628\n",
            "Epoch 12/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7074 - accuracy: 0.7007\n",
            "Epoch 12: val_accuracy improved from 0.76536 to 0.77474, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.7074 - accuracy: 0.7006 - val_loss: 0.5451 - val_accuracy: 0.7747\n",
            "Epoch 13/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6978 - accuracy: 0.7067\n",
            "Epoch 13: val_accuracy did not improve from 0.77474\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6978 - accuracy: 0.7065 - val_loss: 0.5499 - val_accuracy: 0.7526\n",
            "Epoch 14/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.7037 - accuracy: 0.7012\n",
            "Epoch 14: val_accuracy did not improve from 0.77474\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.7036 - accuracy: 0.7011 - val_loss: 0.5553 - val_accuracy: 0.7517\n",
            "Epoch 15/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6908 - accuracy: 0.7072\n",
            "Epoch 15: val_accuracy improved from 0.77474 to 0.79266, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6907 - accuracy: 0.7072 - val_loss: 0.5303 - val_accuracy: 0.7927\n",
            "Epoch 16/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6865 - accuracy: 0.7115\n",
            "Epoch 16: val_accuracy did not improve from 0.79266\n",
            "330/330 [==============================] - 10s 32ms/step - loss: 0.6866 - accuracy: 0.7114 - val_loss: 0.5170 - val_accuracy: 0.7747\n",
            "Epoch 17/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6781 - accuracy: 0.7142\n",
            "Epoch 17: val_accuracy did not improve from 0.79266\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6786 - accuracy: 0.7140 - val_loss: 0.5082 - val_accuracy: 0.7892\n",
            "Epoch 18/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6804 - accuracy: 0.7091\n",
            "Epoch 18: val_accuracy did not improve from 0.79266\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.6807 - accuracy: 0.7090 - val_loss: 0.5078 - val_accuracy: 0.7927\n",
            "Epoch 19/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6714 - accuracy: 0.7171\n",
            "Epoch 19: val_accuracy improved from 0.79266 to 0.79608, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6713 - accuracy: 0.7173 - val_loss: 0.4969 - val_accuracy: 0.7961\n",
            "Epoch 20/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6781 - accuracy: 0.7132\n",
            "Epoch 20: val_accuracy improved from 0.79608 to 0.79778, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6778 - accuracy: 0.7135 - val_loss: 0.5068 - val_accuracy: 0.7978\n",
            "Epoch 21/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6718 - accuracy: 0.7188\n",
            "Epoch 21: val_accuracy did not improve from 0.79778\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6716 - accuracy: 0.7189 - val_loss: 0.4943 - val_accuracy: 0.7944\n",
            "Epoch 22/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6637 - accuracy: 0.7254\n",
            "Epoch 22: val_accuracy improved from 0.79778 to 0.80205, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6637 - accuracy: 0.7254 - val_loss: 0.4901 - val_accuracy: 0.8020\n",
            "Epoch 23/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6656 - accuracy: 0.7240\n",
            "Epoch 23: val_accuracy did not improve from 0.80205\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6653 - accuracy: 0.7240 - val_loss: 0.4952 - val_accuracy: 0.7978\n",
            "Epoch 24/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6650 - accuracy: 0.7244\n",
            "Epoch 24: val_accuracy improved from 0.80205 to 0.80546, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6649 - accuracy: 0.7243 - val_loss: 0.4835 - val_accuracy: 0.8055\n",
            "Epoch 25/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6548 - accuracy: 0.7320\n",
            "Epoch 25: val_accuracy did not improve from 0.80546\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6553 - accuracy: 0.7317 - val_loss: 0.4818 - val_accuracy: 0.8046\n",
            "Epoch 26/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6587 - accuracy: 0.7267\n",
            "Epoch 26: val_accuracy did not improve from 0.80546\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6584 - accuracy: 0.7269 - val_loss: 0.4853 - val_accuracy: 0.8029\n",
            "Epoch 27/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6557 - accuracy: 0.7282\n",
            "Epoch 27: val_accuracy did not improve from 0.80546\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6554 - accuracy: 0.7283 - val_loss: 0.4790 - val_accuracy: 0.8046\n",
            "Epoch 28/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6472 - accuracy: 0.7316\n",
            "Epoch 28: val_accuracy improved from 0.80546 to 0.80802, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6472 - accuracy: 0.7316 - val_loss: 0.4770 - val_accuracy: 0.8080\n",
            "Epoch 29/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6403 - accuracy: 0.7395\n",
            "Epoch 29: val_accuracy did not improve from 0.80802\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6405 - accuracy: 0.7393 - val_loss: 0.4767 - val_accuracy: 0.8038\n",
            "Epoch 30/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6336 - accuracy: 0.7416\n",
            "Epoch 30: val_accuracy improved from 0.80802 to 0.80887, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6336 - accuracy: 0.7417 - val_loss: 0.4725 - val_accuracy: 0.8089\n",
            "Epoch 31/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6472 - accuracy: 0.7329\n",
            "Epoch 31: val_accuracy improved from 0.80887 to 0.82082, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6474 - accuracy: 0.7327 - val_loss: 0.4690 - val_accuracy: 0.8208\n",
            "Epoch 32/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6318 - accuracy: 0.7441\n",
            "Epoch 32: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6318 - accuracy: 0.7441 - val_loss: 0.4700 - val_accuracy: 0.8080\n",
            "Epoch 33/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6341 - accuracy: 0.7317\n",
            "Epoch 33: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6341 - accuracy: 0.7317 - val_loss: 0.4584 - val_accuracy: 0.8200\n",
            "Epoch 34/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6398 - accuracy: 0.7369\n",
            "Epoch 34: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6396 - accuracy: 0.7368 - val_loss: 0.4608 - val_accuracy: 0.8148\n",
            "Epoch 35/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6415 - accuracy: 0.7282\n",
            "Epoch 35: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6414 - accuracy: 0.7283 - val_loss: 0.4592 - val_accuracy: 0.8200\n",
            "Epoch 36/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6391 - accuracy: 0.7415\n",
            "Epoch 36: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6396 - accuracy: 0.7414 - val_loss: 0.4614 - val_accuracy: 0.8140\n",
            "Epoch 37/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6394 - accuracy: 0.7384\n",
            "Epoch 37: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6393 - accuracy: 0.7384 - val_loss: 0.4647 - val_accuracy: 0.8166\n",
            "Epoch 38/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6453 - accuracy: 0.7345\n",
            "Epoch 38: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6453 - accuracy: 0.7345 - val_loss: 0.4496 - val_accuracy: 0.8174\n",
            "Epoch 39/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6225 - accuracy: 0.7429\n",
            "Epoch 39: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6229 - accuracy: 0.7427 - val_loss: 0.4440 - val_accuracy: 0.8208\n",
            "Epoch 40/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6368 - accuracy: 0.7437\n",
            "Epoch 40: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6368 - accuracy: 0.7437 - val_loss: 0.4605 - val_accuracy: 0.8174\n",
            "Epoch 41/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6276 - accuracy: 0.7404\n",
            "Epoch 41: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6276 - accuracy: 0.7404 - val_loss: 0.4579 - val_accuracy: 0.8157\n",
            "Epoch 42/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6207 - accuracy: 0.7478\n",
            "Epoch 42: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6205 - accuracy: 0.7479 - val_loss: 0.4523 - val_accuracy: 0.8166\n",
            "Epoch 43/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6274 - accuracy: 0.7413\n",
            "Epoch 43: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6270 - accuracy: 0.7414 - val_loss: 0.4568 - val_accuracy: 0.8183\n",
            "Epoch 44/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6150 - accuracy: 0.7480\n",
            "Epoch 44: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6150 - accuracy: 0.7480 - val_loss: 0.4434 - val_accuracy: 0.8166\n",
            "Epoch 45/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6084 - accuracy: 0.7519\n",
            "Epoch 45: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6089 - accuracy: 0.7518 - val_loss: 0.4512 - val_accuracy: 0.8200\n",
            "Epoch 46/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6167 - accuracy: 0.7470\n",
            "Epoch 46: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6163 - accuracy: 0.7472 - val_loss: 0.4421 - val_accuracy: 0.8183\n",
            "Epoch 47/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6304 - accuracy: 0.7436\n",
            "Epoch 47: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6299 - accuracy: 0.7439 - val_loss: 0.4534 - val_accuracy: 0.8200\n",
            "Epoch 48/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6097 - accuracy: 0.7524\n",
            "Epoch 48: val_accuracy did not improve from 0.82082\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6099 - accuracy: 0.7524 - val_loss: 0.4533 - val_accuracy: 0.8183\n",
            "Epoch 49/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6084 - accuracy: 0.7535\n",
            "Epoch 49: val_accuracy improved from 0.82082 to 0.82253, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6086 - accuracy: 0.7534 - val_loss: 0.4468 - val_accuracy: 0.8225\n",
            "Epoch 50/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6096 - accuracy: 0.7528\n",
            "Epoch 50: val_accuracy improved from 0.82253 to 0.82338, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6096 - accuracy: 0.7528 - val_loss: 0.4474 - val_accuracy: 0.8234\n",
            "Epoch 51/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6179 - accuracy: 0.7546\n",
            "Epoch 51: val_accuracy did not improve from 0.82338\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6179 - accuracy: 0.7546 - val_loss: 0.4396 - val_accuracy: 0.8217\n",
            "Epoch 52/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6158 - accuracy: 0.7535\n",
            "Epoch 52: val_accuracy did not improve from 0.82338\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6156 - accuracy: 0.7536 - val_loss: 0.4505 - val_accuracy: 0.8225\n",
            "Epoch 53/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6125 - accuracy: 0.7524\n",
            "Epoch 53: val_accuracy did not improve from 0.82338\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6124 - accuracy: 0.7525 - val_loss: 0.4396 - val_accuracy: 0.8200\n",
            "Epoch 54/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6207 - accuracy: 0.7453\n",
            "Epoch 54: val_accuracy did not improve from 0.82338\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.6206 - accuracy: 0.7454 - val_loss: 0.4492 - val_accuracy: 0.8225\n",
            "Epoch 55/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6088 - accuracy: 0.7533\n",
            "Epoch 55: val_accuracy did not improve from 0.82338\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6085 - accuracy: 0.7535 - val_loss: 0.4460 - val_accuracy: 0.8217\n",
            "Epoch 56/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6107 - accuracy: 0.7513\n",
            "Epoch 56: val_accuracy did not improve from 0.82338\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6111 - accuracy: 0.7512 - val_loss: 0.4509 - val_accuracy: 0.8234\n",
            "Epoch 57/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6111 - accuracy: 0.7487\n",
            "Epoch 57: val_accuracy did not improve from 0.82338\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6111 - accuracy: 0.7487 - val_loss: 0.4439 - val_accuracy: 0.8200\n",
            "Epoch 58/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6120 - accuracy: 0.7537\n",
            "Epoch 58: val_accuracy improved from 0.82338 to 0.82509, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6118 - accuracy: 0.7537 - val_loss: 0.4518 - val_accuracy: 0.8251\n",
            "Epoch 59/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6128 - accuracy: 0.7500\n",
            "Epoch 59: val_accuracy did not improve from 0.82509\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.6128 - accuracy: 0.7500 - val_loss: 0.4364 - val_accuracy: 0.8208\n",
            "Epoch 60/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6018 - accuracy: 0.7554\n",
            "Epoch 60: val_accuracy did not improve from 0.82509\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6018 - accuracy: 0.7554 - val_loss: 0.4468 - val_accuracy: 0.8217\n",
            "Epoch 61/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5987 - accuracy: 0.7523\n",
            "Epoch 61: val_accuracy did not improve from 0.82509\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5987 - accuracy: 0.7523 - val_loss: 0.4361 - val_accuracy: 0.8225\n",
            "Epoch 62/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5924 - accuracy: 0.7562\n",
            "Epoch 62: val_accuracy did not improve from 0.82509\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5924 - accuracy: 0.7562 - val_loss: 0.4375 - val_accuracy: 0.8225\n",
            "Epoch 63/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5995 - accuracy: 0.7561\n",
            "Epoch 63: val_accuracy did not improve from 0.82509\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5995 - accuracy: 0.7561 - val_loss: 0.4379 - val_accuracy: 0.8225\n",
            "Epoch 64/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6007 - accuracy: 0.7588\n",
            "Epoch 64: val_accuracy did not improve from 0.82509\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6006 - accuracy: 0.7588 - val_loss: 0.4358 - val_accuracy: 0.8217\n",
            "Epoch 65/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5971 - accuracy: 0.7613\n",
            "Epoch 65: val_accuracy did not improve from 0.82509\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5970 - accuracy: 0.7612 - val_loss: 0.4286 - val_accuracy: 0.8191\n",
            "Epoch 66/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6064 - accuracy: 0.7551\n",
            "Epoch 66: val_accuracy did not improve from 0.82509\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6062 - accuracy: 0.7553 - val_loss: 0.4320 - val_accuracy: 0.8251\n",
            "Epoch 67/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5972 - accuracy: 0.7579\n",
            "Epoch 67: val_accuracy improved from 0.82509 to 0.82679, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5976 - accuracy: 0.7579 - val_loss: 0.4304 - val_accuracy: 0.8268\n",
            "Epoch 68/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5945 - accuracy: 0.7623\n",
            "Epoch 68: val_accuracy did not improve from 0.82679\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5945 - accuracy: 0.7624 - val_loss: 0.4364 - val_accuracy: 0.8251\n",
            "Epoch 69/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6015 - accuracy: 0.7539\n",
            "Epoch 69: val_accuracy did not improve from 0.82679\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6015 - accuracy: 0.7540 - val_loss: 0.4335 - val_accuracy: 0.8234\n",
            "Epoch 70/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5951 - accuracy: 0.7595\n",
            "Epoch 70: val_accuracy did not improve from 0.82679\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5948 - accuracy: 0.7597 - val_loss: 0.4309 - val_accuracy: 0.8234\n",
            "Epoch 71/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.6040 - accuracy: 0.7550\n",
            "Epoch 71: val_accuracy did not improve from 0.82679\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6041 - accuracy: 0.7548 - val_loss: 0.4362 - val_accuracy: 0.8225\n",
            "Epoch 72/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5959 - accuracy: 0.7579\n",
            "Epoch 72: val_accuracy did not improve from 0.82679\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5959 - accuracy: 0.7579 - val_loss: 0.4396 - val_accuracy: 0.8191\n",
            "Epoch 73/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5947 - accuracy: 0.7600\n",
            "Epoch 73: val_accuracy did not improve from 0.82679\n",
            "330/330 [==============================] - 13s 38ms/step - loss: 0.5947 - accuracy: 0.7600 - val_loss: 0.4340 - val_accuracy: 0.8234\n",
            "Epoch 74/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5989 - accuracy: 0.7548\n",
            "Epoch 74: val_accuracy did not improve from 0.82679\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5987 - accuracy: 0.7549 - val_loss: 0.4338 - val_accuracy: 0.8242\n",
            "Epoch 75/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5937 - accuracy: 0.7624\n",
            "Epoch 75: val_accuracy did not improve from 0.82679\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5936 - accuracy: 0.7624 - val_loss: 0.4316 - val_accuracy: 0.8268\n",
            "Epoch 76/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5901 - accuracy: 0.7628\n",
            "Epoch 76: val_accuracy did not improve from 0.82679\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5901 - accuracy: 0.7628 - val_loss: 0.4278 - val_accuracy: 0.8268\n",
            "Epoch 77/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5978 - accuracy: 0.7536\n",
            "Epoch 77: val_accuracy improved from 0.82679 to 0.83020, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5977 - accuracy: 0.7537 - val_loss: 0.4243 - val_accuracy: 0.8302\n",
            "Epoch 78/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5932 - accuracy: 0.7596\n",
            "Epoch 78: val_accuracy did not improve from 0.83020\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5940 - accuracy: 0.7593 - val_loss: 0.4222 - val_accuracy: 0.8259\n",
            "Epoch 79/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5870 - accuracy: 0.7592\n",
            "Epoch 79: val_accuracy improved from 0.83020 to 0.83106, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5870 - accuracy: 0.7592 - val_loss: 0.4304 - val_accuracy: 0.8311\n",
            "Epoch 80/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.6015 - accuracy: 0.7590\n",
            "Epoch 80: val_accuracy improved from 0.83106 to 0.83191, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.6015 - accuracy: 0.7590 - val_loss: 0.4278 - val_accuracy: 0.8319\n",
            "Epoch 81/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5924 - accuracy: 0.7540\n",
            "Epoch 81: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5927 - accuracy: 0.7537 - val_loss: 0.4311 - val_accuracy: 0.8276\n",
            "Epoch 82/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5886 - accuracy: 0.7617\n",
            "Epoch 82: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5889 - accuracy: 0.7616 - val_loss: 0.4305 - val_accuracy: 0.8319\n",
            "Epoch 83/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5873 - accuracy: 0.7612\n",
            "Epoch 83: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5873 - accuracy: 0.7612 - val_loss: 0.4219 - val_accuracy: 0.8294\n",
            "Epoch 84/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5886 - accuracy: 0.7680\n",
            "Epoch 84: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5888 - accuracy: 0.7681 - val_loss: 0.4195 - val_accuracy: 0.8319\n",
            "Epoch 85/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5909 - accuracy: 0.7636\n",
            "Epoch 85: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5909 - accuracy: 0.7636 - val_loss: 0.4309 - val_accuracy: 0.8285\n",
            "Epoch 86/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5939 - accuracy: 0.7588\n",
            "Epoch 86: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5939 - accuracy: 0.7588 - val_loss: 0.4261 - val_accuracy: 0.8302\n",
            "Epoch 87/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5831 - accuracy: 0.7659\n",
            "Epoch 87: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5832 - accuracy: 0.7658 - val_loss: 0.4302 - val_accuracy: 0.8311\n",
            "Epoch 88/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5805 - accuracy: 0.7667\n",
            "Epoch 88: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5802 - accuracy: 0.7668 - val_loss: 0.4196 - val_accuracy: 0.8285\n",
            "Epoch 89/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5818 - accuracy: 0.7642\n",
            "Epoch 89: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5818 - accuracy: 0.7640 - val_loss: 0.4255 - val_accuracy: 0.8302\n",
            "Epoch 90/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5878 - accuracy: 0.7645\n",
            "Epoch 90: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5877 - accuracy: 0.7646 - val_loss: 0.4263 - val_accuracy: 0.8302\n",
            "Epoch 91/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5722 - accuracy: 0.7725\n",
            "Epoch 91: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5723 - accuracy: 0.7724 - val_loss: 0.4196 - val_accuracy: 0.8319\n",
            "Epoch 92/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5827 - accuracy: 0.7623\n",
            "Epoch 92: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5826 - accuracy: 0.7622 - val_loss: 0.4161 - val_accuracy: 0.8285\n",
            "Epoch 93/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5834 - accuracy: 0.7646\n",
            "Epoch 93: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5834 - accuracy: 0.7646 - val_loss: 0.4260 - val_accuracy: 0.8276\n",
            "Epoch 94/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5822 - accuracy: 0.7657\n",
            "Epoch 94: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5830 - accuracy: 0.7656 - val_loss: 0.4249 - val_accuracy: 0.8311\n",
            "Epoch 95/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5723 - accuracy: 0.7636\n",
            "Epoch 95: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5725 - accuracy: 0.7636 - val_loss: 0.4154 - val_accuracy: 0.8319\n",
            "Epoch 96/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5801 - accuracy: 0.7673\n",
            "Epoch 96: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5803 - accuracy: 0.7671 - val_loss: 0.4201 - val_accuracy: 0.8319\n",
            "Epoch 97/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5720 - accuracy: 0.7690\n",
            "Epoch 97: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5725 - accuracy: 0.7692 - val_loss: 0.4230 - val_accuracy: 0.8302\n",
            "Epoch 98/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5761 - accuracy: 0.7646\n",
            "Epoch 98: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5767 - accuracy: 0.7646 - val_loss: 0.4151 - val_accuracy: 0.8311\n",
            "Epoch 99/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5683 - accuracy: 0.7698\n",
            "Epoch 99: val_accuracy did not improve from 0.83191\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5681 - accuracy: 0.7697 - val_loss: 0.4119 - val_accuracy: 0.8302\n",
            "Epoch 100/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5761 - accuracy: 0.7672\n",
            "Epoch 100: val_accuracy improved from 0.83191 to 0.83447, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5762 - accuracy: 0.7673 - val_loss: 0.4144 - val_accuracy: 0.8345\n",
            "Epoch 101/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5771 - accuracy: 0.7740\n",
            "Epoch 101: val_accuracy did not improve from 0.83447\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5772 - accuracy: 0.7741 - val_loss: 0.4141 - val_accuracy: 0.8328\n",
            "Epoch 102/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5811 - accuracy: 0.7665\n",
            "Epoch 102: val_accuracy did not improve from 0.83447\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5811 - accuracy: 0.7665 - val_loss: 0.4214 - val_accuracy: 0.8345\n",
            "Epoch 103/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5764 - accuracy: 0.7666\n",
            "Epoch 103: val_accuracy improved from 0.83447 to 0.83532, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5764 - accuracy: 0.7666 - val_loss: 0.4190 - val_accuracy: 0.8353\n",
            "Epoch 104/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5708 - accuracy: 0.7667\n",
            "Epoch 104: val_accuracy improved from 0.83532 to 0.83788, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5709 - accuracy: 0.7665 - val_loss: 0.4191 - val_accuracy: 0.8379\n",
            "Epoch 105/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5731 - accuracy: 0.7692\n",
            "Epoch 105: val_accuracy improved from 0.83788 to 0.83959, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5731 - accuracy: 0.7692 - val_loss: 0.4161 - val_accuracy: 0.8396\n",
            "Epoch 106/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5737 - accuracy: 0.7698\n",
            "Epoch 106: val_accuracy did not improve from 0.83959\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5737 - accuracy: 0.7698 - val_loss: 0.4171 - val_accuracy: 0.8353\n",
            "Epoch 107/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5780 - accuracy: 0.7729\n",
            "Epoch 107: val_accuracy did not improve from 0.83959\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5780 - accuracy: 0.7729 - val_loss: 0.4114 - val_accuracy: 0.8336\n",
            "Epoch 108/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5768 - accuracy: 0.7733\n",
            "Epoch 108: val_accuracy did not improve from 0.83959\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5768 - accuracy: 0.7733 - val_loss: 0.4093 - val_accuracy: 0.8387\n",
            "Epoch 109/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5749 - accuracy: 0.7703\n",
            "Epoch 109: val_accuracy did not improve from 0.83959\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5749 - accuracy: 0.7703 - val_loss: 0.4139 - val_accuracy: 0.8345\n",
            "Epoch 110/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5832 - accuracy: 0.7660\n",
            "Epoch 110: val_accuracy did not improve from 0.83959\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5831 - accuracy: 0.7660 - val_loss: 0.4106 - val_accuracy: 0.8379\n",
            "Epoch 111/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5759 - accuracy: 0.7699\n",
            "Epoch 111: val_accuracy did not improve from 0.83959\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5758 - accuracy: 0.7701 - val_loss: 0.4170 - val_accuracy: 0.8336\n",
            "Epoch 112/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5698 - accuracy: 0.7721\n",
            "Epoch 112: val_accuracy did not improve from 0.83959\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5698 - accuracy: 0.7721 - val_loss: 0.4091 - val_accuracy: 0.8387\n",
            "Epoch 113/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5765 - accuracy: 0.7710\n",
            "Epoch 113: val_accuracy did not improve from 0.83959\n",
            "330/330 [==============================] - 11s 32ms/step - loss: 0.5765 - accuracy: 0.7710 - val_loss: 0.4119 - val_accuracy: 0.8362\n",
            "Epoch 114/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5820 - accuracy: 0.7699\n",
            "Epoch 114: val_accuracy did not improve from 0.83959\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5818 - accuracy: 0.7698 - val_loss: 0.4250 - val_accuracy: 0.8362\n",
            "Epoch 115/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5680 - accuracy: 0.7661\n",
            "Epoch 115: val_accuracy improved from 0.83959 to 0.84130, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5680 - accuracy: 0.7660 - val_loss: 0.4084 - val_accuracy: 0.8413\n",
            "Epoch 116/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5734 - accuracy: 0.7688\n",
            "Epoch 116: val_accuracy did not improve from 0.84130\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5732 - accuracy: 0.7688 - val_loss: 0.4027 - val_accuracy: 0.8362\n",
            "Epoch 117/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5700 - accuracy: 0.7733\n",
            "Epoch 117: val_accuracy did not improve from 0.84130\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5700 - accuracy: 0.7733 - val_loss: 0.4040 - val_accuracy: 0.8404\n",
            "Epoch 118/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5680 - accuracy: 0.7680\n",
            "Epoch 118: val_accuracy did not improve from 0.84130\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5678 - accuracy: 0.7681 - val_loss: 0.4092 - val_accuracy: 0.8413\n",
            "Epoch 119/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5688 - accuracy: 0.7739\n",
            "Epoch 119: val_accuracy did not improve from 0.84130\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5688 - accuracy: 0.7739 - val_loss: 0.4068 - val_accuracy: 0.8413\n",
            "Epoch 120/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5702 - accuracy: 0.7709\n",
            "Epoch 120: val_accuracy improved from 0.84130 to 0.84642, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5700 - accuracy: 0.7711 - val_loss: 0.4105 - val_accuracy: 0.8464\n",
            "Epoch 121/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5736 - accuracy: 0.7754\n",
            "Epoch 121: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5737 - accuracy: 0.7753 - val_loss: 0.4071 - val_accuracy: 0.8370\n",
            "Epoch 122/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5599 - accuracy: 0.7782\n",
            "Epoch 122: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5599 - accuracy: 0.7782 - val_loss: 0.4042 - val_accuracy: 0.8379\n",
            "Epoch 123/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5695 - accuracy: 0.7767\n",
            "Epoch 123: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5695 - accuracy: 0.7766 - val_loss: 0.4065 - val_accuracy: 0.8379\n",
            "Epoch 124/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5662 - accuracy: 0.7707\n",
            "Epoch 124: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5659 - accuracy: 0.7710 - val_loss: 0.4037 - val_accuracy: 0.8404\n",
            "Epoch 125/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5763 - accuracy: 0.7773\n",
            "Epoch 125: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5763 - accuracy: 0.7773 - val_loss: 0.4122 - val_accuracy: 0.8387\n",
            "Epoch 126/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5670 - accuracy: 0.7780\n",
            "Epoch 126: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5674 - accuracy: 0.7778 - val_loss: 0.4029 - val_accuracy: 0.8404\n",
            "Epoch 127/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5732 - accuracy: 0.7689\n",
            "Epoch 127: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5731 - accuracy: 0.7691 - val_loss: 0.4017 - val_accuracy: 0.8328\n",
            "Epoch 128/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5654 - accuracy: 0.7724\n",
            "Epoch 128: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5651 - accuracy: 0.7725 - val_loss: 0.4076 - val_accuracy: 0.8379\n",
            "Epoch 129/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7763\n",
            "Epoch 129: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5640 - accuracy: 0.7761 - val_loss: 0.4033 - val_accuracy: 0.8387\n",
            "Epoch 130/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5730 - accuracy: 0.7659\n",
            "Epoch 130: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5731 - accuracy: 0.7657 - val_loss: 0.3980 - val_accuracy: 0.8439\n",
            "Epoch 131/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5689 - accuracy: 0.7738\n",
            "Epoch 131: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5689 - accuracy: 0.7739 - val_loss: 0.4127 - val_accuracy: 0.8464\n",
            "Epoch 132/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5683 - accuracy: 0.7775\n",
            "Epoch 132: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5683 - accuracy: 0.7776 - val_loss: 0.4101 - val_accuracy: 0.8387\n",
            "Epoch 133/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5743 - accuracy: 0.7699\n",
            "Epoch 133: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5743 - accuracy: 0.7699 - val_loss: 0.4037 - val_accuracy: 0.8413\n",
            "Epoch 134/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5772 - accuracy: 0.7703\n",
            "Epoch 134: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5772 - accuracy: 0.7703 - val_loss: 0.4102 - val_accuracy: 0.8430\n",
            "Epoch 135/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5767 - accuracy: 0.7725\n",
            "Epoch 135: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5767 - accuracy: 0.7725 - val_loss: 0.4034 - val_accuracy: 0.8404\n",
            "Epoch 136/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5661 - accuracy: 0.7756\n",
            "Epoch 136: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5658 - accuracy: 0.7757 - val_loss: 0.4184 - val_accuracy: 0.8353\n",
            "Epoch 137/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5580 - accuracy: 0.7801\n",
            "Epoch 137: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5580 - accuracy: 0.7801 - val_loss: 0.3981 - val_accuracy: 0.8422\n",
            "Epoch 138/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5657 - accuracy: 0.7766\n",
            "Epoch 138: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5657 - accuracy: 0.7766 - val_loss: 0.4081 - val_accuracy: 0.8345\n",
            "Epoch 139/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5646 - accuracy: 0.7753\n",
            "Epoch 139: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5646 - accuracy: 0.7753 - val_loss: 0.4030 - val_accuracy: 0.8422\n",
            "Epoch 140/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5569 - accuracy: 0.7795\n",
            "Epoch 140: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5569 - accuracy: 0.7795 - val_loss: 0.4006 - val_accuracy: 0.8345\n",
            "Epoch 141/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7780\n",
            "Epoch 141: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5607 - accuracy: 0.7778 - val_loss: 0.4035 - val_accuracy: 0.8328\n",
            "Epoch 142/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5634 - accuracy: 0.7737\n",
            "Epoch 142: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5634 - accuracy: 0.7737 - val_loss: 0.3993 - val_accuracy: 0.8404\n",
            "Epoch 143/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7782\n",
            "Epoch 143: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5615 - accuracy: 0.7782 - val_loss: 0.3959 - val_accuracy: 0.8422\n",
            "Epoch 144/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5703 - accuracy: 0.7731\n",
            "Epoch 144: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5703 - accuracy: 0.7731 - val_loss: 0.4021 - val_accuracy: 0.8456\n",
            "Epoch 145/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5717 - accuracy: 0.7724\n",
            "Epoch 145: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5715 - accuracy: 0.7726 - val_loss: 0.4034 - val_accuracy: 0.8345\n",
            "Epoch 146/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5584 - accuracy: 0.7720\n",
            "Epoch 146: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5584 - accuracy: 0.7720 - val_loss: 0.4054 - val_accuracy: 0.8379\n",
            "Epoch 147/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5655 - accuracy: 0.7773\n",
            "Epoch 147: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5655 - accuracy: 0.7773 - val_loss: 0.4138 - val_accuracy: 0.8404\n",
            "Epoch 148/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5533 - accuracy: 0.7797\n",
            "Epoch 148: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5529 - accuracy: 0.7800 - val_loss: 0.4128 - val_accuracy: 0.8353\n",
            "Epoch 149/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5677 - accuracy: 0.7742\n",
            "Epoch 149: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5677 - accuracy: 0.7742 - val_loss: 0.4089 - val_accuracy: 0.8379\n",
            "Epoch 150/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5597 - accuracy: 0.7769\n",
            "Epoch 150: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5597 - accuracy: 0.7769 - val_loss: 0.4032 - val_accuracy: 0.8447\n",
            "Epoch 151/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5592 - accuracy: 0.7754\n",
            "Epoch 151: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5592 - accuracy: 0.7754 - val_loss: 0.4042 - val_accuracy: 0.8396\n",
            "Epoch 152/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5615 - accuracy: 0.7765\n",
            "Epoch 152: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5615 - accuracy: 0.7765 - val_loss: 0.4062 - val_accuracy: 0.8379\n",
            "Epoch 153/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5522 - accuracy: 0.7764\n",
            "Epoch 153: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5520 - accuracy: 0.7766 - val_loss: 0.3967 - val_accuracy: 0.8439\n",
            "Epoch 154/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5751 - accuracy: 0.7717\n",
            "Epoch 154: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5749 - accuracy: 0.7718 - val_loss: 0.3951 - val_accuracy: 0.8422\n",
            "Epoch 155/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5583 - accuracy: 0.7751\n",
            "Epoch 155: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5581 - accuracy: 0.7752 - val_loss: 0.4120 - val_accuracy: 0.8345\n",
            "Epoch 156/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5542 - accuracy: 0.7766\n",
            "Epoch 156: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5541 - accuracy: 0.7766 - val_loss: 0.4052 - val_accuracy: 0.8362\n",
            "Epoch 157/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7714\n",
            "Epoch 157: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5635 - accuracy: 0.7715 - val_loss: 0.3948 - val_accuracy: 0.8413\n",
            "Epoch 158/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5655 - accuracy: 0.7750\n",
            "Epoch 158: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5655 - accuracy: 0.7750 - val_loss: 0.4030 - val_accuracy: 0.8362\n",
            "Epoch 159/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5529 - accuracy: 0.7768\n",
            "Epoch 159: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5539 - accuracy: 0.7766 - val_loss: 0.3972 - val_accuracy: 0.8379\n",
            "Epoch 160/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5529 - accuracy: 0.7753\n",
            "Epoch 160: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5528 - accuracy: 0.7753 - val_loss: 0.3958 - val_accuracy: 0.8396\n",
            "Epoch 161/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7783\n",
            "Epoch 161: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5615 - accuracy: 0.7784 - val_loss: 0.3968 - val_accuracy: 0.8370\n",
            "Epoch 162/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5614 - accuracy: 0.7794\n",
            "Epoch 162: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5620 - accuracy: 0.7793 - val_loss: 0.4046 - val_accuracy: 0.8379\n",
            "Epoch 163/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5566 - accuracy: 0.7796\n",
            "Epoch 163: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5567 - accuracy: 0.7797 - val_loss: 0.4004 - val_accuracy: 0.8379\n",
            "Epoch 164/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5574 - accuracy: 0.7780\n",
            "Epoch 164: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5577 - accuracy: 0.7777 - val_loss: 0.3960 - val_accuracy: 0.8396\n",
            "Epoch 165/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5628 - accuracy: 0.7793\n",
            "Epoch 165: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5628 - accuracy: 0.7793 - val_loss: 0.3959 - val_accuracy: 0.8353\n",
            "Epoch 166/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5529 - accuracy: 0.7849\n",
            "Epoch 166: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5529 - accuracy: 0.7849 - val_loss: 0.3945 - val_accuracy: 0.8362\n",
            "Epoch 167/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5580 - accuracy: 0.7810\n",
            "Epoch 167: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5576 - accuracy: 0.7811 - val_loss: 0.3958 - val_accuracy: 0.8404\n",
            "Epoch 168/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5639 - accuracy: 0.7825\n",
            "Epoch 168: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5639 - accuracy: 0.7825 - val_loss: 0.4033 - val_accuracy: 0.8396\n",
            "Epoch 169/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5528 - accuracy: 0.7800\n",
            "Epoch 169: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 33ms/step - loss: 0.5529 - accuracy: 0.7798 - val_loss: 0.3965 - val_accuracy: 0.8404\n",
            "Epoch 170/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5503 - accuracy: 0.7843\n",
            "Epoch 170: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5503 - accuracy: 0.7843 - val_loss: 0.4065 - val_accuracy: 0.8387\n",
            "Epoch 171/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.7820\n",
            "Epoch 171: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5619 - accuracy: 0.7821 - val_loss: 0.4019 - val_accuracy: 0.8456\n",
            "Epoch 172/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5560 - accuracy: 0.7792\n",
            "Epoch 172: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5563 - accuracy: 0.7791 - val_loss: 0.3952 - val_accuracy: 0.8439\n",
            "Epoch 173/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5550 - accuracy: 0.7773\n",
            "Epoch 173: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5550 - accuracy: 0.7773 - val_loss: 0.3970 - val_accuracy: 0.8413\n",
            "Epoch 174/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5582 - accuracy: 0.7786\n",
            "Epoch 174: val_accuracy did not improve from 0.84642\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5582 - accuracy: 0.7786 - val_loss: 0.3965 - val_accuracy: 0.8413\n",
            "Epoch 175/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7781\n",
            "Epoch 175: val_accuracy improved from 0.84642 to 0.85068, saving model to best_liu.h5\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5643 - accuracy: 0.7782 - val_loss: 0.3940 - val_accuracy: 0.8507\n",
            "Epoch 176/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5506 - accuracy: 0.7859\n",
            "Epoch 176: val_accuracy did not improve from 0.85068\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5506 - accuracy: 0.7859 - val_loss: 0.4045 - val_accuracy: 0.8447\n",
            "Epoch 177/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5532 - accuracy: 0.7793\n",
            "Epoch 177: val_accuracy did not improve from 0.85068\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5530 - accuracy: 0.7793 - val_loss: 0.3976 - val_accuracy: 0.8456\n",
            "Epoch 178/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5653 - accuracy: 0.7747\n",
            "Epoch 178: val_accuracy did not improve from 0.85068\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5653 - accuracy: 0.7747 - val_loss: 0.4036 - val_accuracy: 0.8464\n",
            "Epoch 179/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5572 - accuracy: 0.7786\n",
            "Epoch 179: val_accuracy did not improve from 0.85068\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5572 - accuracy: 0.7786 - val_loss: 0.3929 - val_accuracy: 0.8481\n",
            "Epoch 180/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5620 - accuracy: 0.7816\n",
            "Epoch 180: val_accuracy did not improve from 0.85068\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5620 - accuracy: 0.7816 - val_loss: 0.3932 - val_accuracy: 0.8447\n",
            "Epoch 181/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5551 - accuracy: 0.7757\n",
            "Epoch 181: val_accuracy did not improve from 0.85068\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5549 - accuracy: 0.7758 - val_loss: 0.3958 - val_accuracy: 0.8464\n",
            "Epoch 182/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5567 - accuracy: 0.7794\n",
            "Epoch 182: val_accuracy did not improve from 0.85068\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5566 - accuracy: 0.7794 - val_loss: 0.3906 - val_accuracy: 0.8439\n",
            "Epoch 183/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5526 - accuracy: 0.7755\n",
            "Epoch 183: val_accuracy did not improve from 0.85068\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5523 - accuracy: 0.7756 - val_loss: 0.3966 - val_accuracy: 0.8507\n",
            "Epoch 184/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5544 - accuracy: 0.7808\n",
            "Epoch 184: val_accuracy did not improve from 0.85068\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5544 - accuracy: 0.7808 - val_loss: 0.4003 - val_accuracy: 0.8473\n",
            "Epoch 185/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5545 - accuracy: 0.7806\n",
            "Epoch 185: val_accuracy improved from 0.85068 to 0.85410, saving model to best_liu.h5\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5543 - accuracy: 0.7807 - val_loss: 0.3906 - val_accuracy: 0.8541\n",
            "Epoch 186/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5558 - accuracy: 0.7794\n",
            "Epoch 186: val_accuracy did not improve from 0.85410\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5558 - accuracy: 0.7794 - val_loss: 0.4011 - val_accuracy: 0.8473\n",
            "Epoch 187/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5569 - accuracy: 0.7769\n",
            "Epoch 187: val_accuracy did not improve from 0.85410\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5568 - accuracy: 0.7769 - val_loss: 0.3912 - val_accuracy: 0.8515\n",
            "Epoch 188/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5584 - accuracy: 0.7805\n",
            "Epoch 188: val_accuracy did not improve from 0.85410\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5584 - accuracy: 0.7805 - val_loss: 0.3993 - val_accuracy: 0.8464\n",
            "Epoch 189/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5436 - accuracy: 0.7878\n",
            "Epoch 189: val_accuracy did not improve from 0.85410\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5436 - accuracy: 0.7878 - val_loss: 0.3993 - val_accuracy: 0.8447\n",
            "Epoch 190/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5434 - accuracy: 0.7860\n",
            "Epoch 190: val_accuracy did not improve from 0.85410\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5440 - accuracy: 0.7858 - val_loss: 0.3998 - val_accuracy: 0.8464\n",
            "Epoch 191/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5530 - accuracy: 0.7796\n",
            "Epoch 191: val_accuracy did not improve from 0.85410\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5533 - accuracy: 0.7795 - val_loss: 0.3946 - val_accuracy: 0.8464\n",
            "Epoch 192/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5576 - accuracy: 0.7801\n",
            "Epoch 192: val_accuracy did not improve from 0.85410\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5576 - accuracy: 0.7801 - val_loss: 0.3926 - val_accuracy: 0.8439\n",
            "Epoch 193/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5497 - accuracy: 0.7812\n",
            "Epoch 193: val_accuracy did not improve from 0.85410\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5497 - accuracy: 0.7812 - val_loss: 0.3899 - val_accuracy: 0.8473\n",
            "Epoch 194/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5482 - accuracy: 0.7741\n",
            "Epoch 194: val_accuracy did not improve from 0.85410\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5480 - accuracy: 0.7742 - val_loss: 0.4004 - val_accuracy: 0.8447\n",
            "Epoch 195/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5584 - accuracy: 0.7841\n",
            "Epoch 195: val_accuracy did not improve from 0.85410\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5585 - accuracy: 0.7843 - val_loss: 0.3870 - val_accuracy: 0.8464\n",
            "Epoch 196/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7804\n",
            "Epoch 196: val_accuracy did not improve from 0.85410\n",
            "330/330 [==============================] - 11s 34ms/step - loss: 0.5630 - accuracy: 0.7804 - val_loss: 0.4033 - val_accuracy: 0.8422\n",
            "Epoch 197/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5514 - accuracy: 0.7837\n",
            "Epoch 197: val_accuracy did not improve from 0.85410\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5511 - accuracy: 0.7840 - val_loss: 0.3909 - val_accuracy: 0.8413\n",
            "Epoch 198/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5485 - accuracy: 0.7786\n",
            "Epoch 198: val_accuracy did not improve from 0.85410\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5485 - accuracy: 0.7786 - val_loss: 0.3961 - val_accuracy: 0.8456\n",
            "Epoch 199/200\n",
            "330/330 [==============================] - ETA: 0s - loss: 0.5545 - accuracy: 0.7808\n",
            "Epoch 199: val_accuracy did not improve from 0.85410\n",
            "330/330 [==============================] - 12s 35ms/step - loss: 0.5545 - accuracy: 0.7808 - val_loss: 0.3869 - val_accuracy: 0.8422\n",
            "Epoch 200/200\n",
            "329/330 [============================>.] - ETA: 0s - loss: 0.5575 - accuracy: 0.7756\n",
            "Epoch 200: val_accuracy did not improve from 0.85410\n",
            "330/330 [==============================] - 11s 35ms/step - loss: 0.5579 - accuracy: 0.7754 - val_loss: 0.3947 - val_accuracy: 0.8439\n"
          ]
        }
      ],
      "source": [
        "hist_liu = model_liu.fit(X_train_pad, y_train_liu, \n",
        "                 batch_size=batch_size,\n",
        "                 epochs=epochs,\n",
        "                 validation_split=0.1,\n",
        "                 callbacks=[mc_liu])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(hist_liu.history['accuracy'])\n",
        "plt.plot(hist_liu.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# \"Loss\"\n",
        "plt.plot(hist_liu.history['loss'])\n",
        "plt.plot(hist_liu.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "Jc5TKJ3e2ya_",
        "outputId": "8dcc633f-ea67-4a1b-e61e-e7e445b2830c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mCdJVPBxyky",
        "outputId": "6ddbe3b0-7f6f-4e18-8570-0f1acff56a45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92/92 [==============================] - 2s 8ms/step\n",
            "Accuracy: 25.75%\n",
            "\n",
            "F1 Score: 25.75\n"
          ]
        }
      ],
      "source": [
        "predictions_liu = model_liu.predict(X_test_pad)\n",
        "predictions_liu = np.argmax(predictions_liu, axis=1)\n",
        "predictions_liu = [class_names[pred] for pred in predictions_liu]\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(test_df.label, predictions_liu) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions_liu, average='micro') * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Final"
      ],
      "metadata": {
        "id": "bMJp4_pleddi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading best model\n",
        "from keras.models import load_model\n",
        "model_manual = load_model('/content/best_manual.h5')\n",
        "model_vader = load_model('/content/best_vader.h5')\n",
        "model_afinn = load_model('/content/best_afinn.h5')\n",
        "model_swn = load_model('/content/best_swn.h5')\n",
        "model_liu = load_model('/content/best_liu.h5')"
      ],
      "metadata": {
        "id": "sBgFwJQcvBc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_prob_manual = model_manual.predict(X_test_pad) \n",
        "y_classes_manual = y_prob_manual.argmax(axis=-1)\n",
        "\n",
        "y_prob_vader = model_vader.predict(X_test_pad) \n",
        "y_classes_vader = y_prob_vader.argmax(axis=-1)\n",
        "\n",
        "y_prob_afinn = model_afinn.predict(X_test_pad) \n",
        "y_classes_afinn = y_prob_afinn.argmax(axis=-1)\n",
        "\n",
        "y_prob_swn = model_swn.predict(X_test_pad) \n",
        "y_classes_swn = y_prob_swn.argmax(axis=-1)\n",
        "\n",
        "y_prob_liu = model_liu.predict(X_test_pad) \n",
        "y_classes_liu = y_prob_liu.argmax(axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2WUbpaCklRJ",
        "outputId": "0c3069fe-ccb5-4ff3-b0f1-87054c58ec18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92/92 [==============================] - 4s 14ms/step\n",
            "92/92 [==============================] - 4s 12ms/step\n",
            "92/92 [==============================] - 3s 10ms/step\n",
            "92/92 [==============================] - 4s 9ms/step\n",
            "92/92 [==============================] - 4s 12ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_label(list_name):\n",
        "  result = []\n",
        "  for index_num in range(0,len(list_name)):\n",
        "    if list_name[index_num]==0:\n",
        "      result.append('negative')\n",
        "    elif list_name[index_num]==1:\n",
        "      result.append('neutral')\n",
        "    elif list_name[index_num]==2:\n",
        "      result.append('positive')\n",
        "  return result"
      ],
      "metadata": {
        "id": "a-f_8xtfn5nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_result_manual = convert_to_label(y_classes_manual)\n",
        "label_result_vader = convert_to_label(y_classes_vader)\n",
        "label_result_afinn = convert_to_label(y_classes_afinn)\n",
        "label_result_swn = convert_to_label(y_classes_swn)\n",
        "label_result_liu = convert_to_label(y_classes_liu)"
      ],
      "metadata": {
        "id": "191g_DQ4rZh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['model_manual_label'] = convert_to_label(y_classes_manual)\n",
        "test_df['model_vader_label'] = convert_to_label(y_classes_vader)\n",
        "test_df['model_afinn_label'] = convert_to_label(y_classes_afinn)\n",
        "test_df['model_swn_label'] = convert_to_label(y_classes_swn)\n",
        "test_df['model_liu_label'] = convert_to_label(y_classes_liu)"
      ],
      "metadata": {
        "id": "_2pvfuYytYQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "Qg22yQLmtiXp",
        "outputId": "b0f29a32-dee7-479e-8f0d-a5eb7cb0fd2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text     label  \\\n",
              "13288  complete lack faith companies really shame thi...  negative   \n",
              "1054              8602947 jon http tco 58tutgli0d thanks   neutral   \n",
              "8500   waiting flt 105 chicago hasnt aircraft left bu...  negative   \n",
              "6595                                               thank   neutral   \n",
              "3937   sfogt bos nonstop flights competitively priced...  negative   \n",
              "\n",
              "      vader_label afinn_label swn_label liu_label model_manual_label  \\\n",
              "13288    negative    negative  negative  positive           negative   \n",
              "1054     positive    positive  positive   neutral           positive   \n",
              "8500      neutral     neutral  positive   neutral           negative   \n",
              "6595     positive    positive   neutral  negative           positive   \n",
              "3937      neutral     neutral  negative   neutral            neutral   \n",
              "\n",
              "      model_vader_label model_afinn_label model_swn_label model_liu_label  \n",
              "13288          negative          negative        positive        positive  \n",
              "1054           positive          positive        positive         neutral  \n",
              "8500            neutral           neutral        positive         neutral  \n",
              "6595           positive          positive         neutral        negative  \n",
              "3937            neutral           neutral         neutral        negative  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aa5126b5-cc26-44de-b6c4-5b8b05453297\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>vader_label</th>\n",
              "      <th>afinn_label</th>\n",
              "      <th>swn_label</th>\n",
              "      <th>liu_label</th>\n",
              "      <th>model_manual_label</th>\n",
              "      <th>model_vader_label</th>\n",
              "      <th>model_afinn_label</th>\n",
              "      <th>model_swn_label</th>\n",
              "      <th>model_liu_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13288</th>\n",
              "      <td>complete lack faith companies really shame thi...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1054</th>\n",
              "      <td>8602947 jon http tco 58tutgli0d thanks</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8500</th>\n",
              "      <td>waiting flt 105 chicago hasnt aircraft left bu...</td>\n",
              "      <td>negative</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "      <td>negative</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6595</th>\n",
              "      <td>thank</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3937</th>\n",
              "      <td>sfogt bos nonstop flights competitively priced...</td>\n",
              "      <td>negative</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>negative</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa5126b5-cc26-44de-b6c4-5b8b05453297')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aa5126b5-cc26-44de-b6c4-5b8b05453297 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aa5126b5-cc26-44de-b6c4-5b8b05453297');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.to_csv('test_df_with_label.csv', index=False)"
      ],
      "metadata": {
        "id": "Z9ByxGgwxk6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "print('Accuracy Manual: {:.2f}'.format(accuracy_score(test_df['label'], test_df['model_manual_label'])))\n",
        "print('Accuracy VADER: {:.2f}'.format(accuracy_score(test_df['label'], test_df['model_vader_label'])))\n",
        "print('Accuracy Afinn: {:.2f}'.format(accuracy_score(test_df['label'], test_df['model_afinn_label'])))\n",
        "print('Accuracy SWN: {:.2f}'.format(accuracy_score(test_df['label'], test_df['model_swn_label'])))\n",
        "print('Accuracy Liu: {:.2f}'.format(accuracy_score(test_df['label'], test_df['model_liu_label'])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHW7nhzzt3WB",
        "outputId": "dc9cd10a-3232-4646-c516-9d3a64856ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Manual: 0.80\n",
            "Accuracy VADER: 0.54\n",
            "Accuracy Afinn: 0.55\n",
            "Accuracy SWN: 0.47\n",
            "Accuracy Liu: 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_manual.predict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "eOeVl87Wwu-L",
        "outputId": "a3c88fbd-344a-4a5a-c272-986c2ecd19ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-159-e6c1d992c20c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_manual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_proba'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_classes_manual[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iirr71qpWIB",
        "outputId": "0c83bd0d-0dbc-44fa-cb13-3f9dba1a6264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgcKN_fHxyky"
      },
      "source": [
        "######Model Testing and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hhAI9H8xyky"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9NY1aw6xyky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a76410a6-47b5-4a86-c591-6a62b624b428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92/92 [==============================] - 1s 7ms/step\n",
            "Accuracy: 25.75%\n",
            "\n",
            "F1 Score: 25.75\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(X_test_pad)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "predictions = [class_names[pred] for pred in predictions]\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(test_df.label, predictions) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions, average='micro') * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK_Ju5J4xykz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VisPsSdWxykz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2855695-5c5d-400b-f007-484e84f5a80a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 19, 300)           4602000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 19, 8)             9888      \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 19, 8)             544       \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 19, 8)             544       \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 8)                0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                144       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,613,171\n",
            "Trainable params: 11,171\n",
            "Non-trainable params: 4,602,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5wygBH8xykz"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    '''\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    '''\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    \n",
        "    # Set size\n",
        "    fig.set_size_inches(12.5, 7.5)\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.grid(False)\n",
        "    \n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8OAlR55xykz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d7b66c3-ae91-45ab-b0d9-584b6e4acd03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "F1 Score: 25.75\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x540 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions, average='micro') * 100))\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix(test_df.label, predictions, classes=class_names, normalize=True, title='Normalized confusion matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKNn5sCGR_kg"
      },
      "source": [
        "##5. Embedding & Modeling - Err"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CAA9mzN1z04"
      },
      "source": [
        "####Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up9j6zO2Zu0v",
        "outputId": "2aebc0e3-a14b-4943-ca8c-2ef1138d3bc0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-33663aca-ec2e-46d9-ba35-d4a4bad63aa1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment values</th>\n",
              "      <th>splitset_label</th>\n",
              "      <th>label</th>\n",
              "      <th>vader_score</th>\n",
              "      <th>vader_label</th>\n",
              "      <th>textblob_score</th>\n",
              "      <th>textblob_label</th>\n",
              "      <th>afinn_score</th>\n",
              "      <th>afinn_label</th>\n",
              "      <th>swn_score</th>\n",
              "      <th>swn_label</th>\n",
              "      <th>liu_score</th>\n",
              "      <th>liu_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rock destined 21st century new conan going mak...</td>\n",
              "      <td>0.69444</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3612</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.212121</td>\n",
              "      <td>positive</td>\n",
              "      <td>3.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.875</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gorgeously elaborate continuation lord rings t...</td>\n",
              "      <td>0.83333</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.8069</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.244444</td>\n",
              "      <td>positive</td>\n",
              "      <td>2.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.625</td>\n",
              "      <td>positive</td>\n",
              "      <td>-1</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>effective tootepid biopic</td>\n",
              "      <td>0.51389</td>\n",
              "      <td>2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.2617</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>positive</td>\n",
              "      <td>2.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.125</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sometimes like go movies fun wasabi good place...</td>\n",
              "      <td>0.73611</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.8271</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>positive</td>\n",
              "      <td>9.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>1.125</td>\n",
              "      <td>positive</td>\n",
              "      <td>-3</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>emerges something rare issue movie honest keen...</td>\n",
              "      <td>0.86111</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.6592</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>positive</td>\n",
              "      <td>4.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>1.000</td>\n",
              "      <td>positive</td>\n",
              "      <td>-2</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33663aca-ec2e-46d9-ba35-d4a4bad63aa1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-33663aca-ec2e-46d9-ba35-d4a4bad63aa1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-33663aca-ec2e-46d9-ba35-d4a4bad63aa1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text  sentiment values  \\\n",
              "0  rock destined 21st century new conan going mak...           0.69444   \n",
              "1  gorgeously elaborate continuation lord rings t...           0.83333   \n",
              "2                          effective tootepid biopic           0.51389   \n",
              "3  sometimes like go movies fun wasabi good place...           0.73611   \n",
              "4  emerges something rare issue movie honest keen...           0.86111   \n",
              "\n",
              "   splitset_label     label  vader_score vader_label  textblob_score  \\\n",
              "0               1  positive       0.3612    positive        0.212121   \n",
              "1               1  positive       0.8069    positive        0.244444   \n",
              "2               2   neutral       0.2617    positive        0.600000   \n",
              "3               2  positive       0.8271    positive        0.500000   \n",
              "4               2  positive       0.6592    positive        0.450000   \n",
              "\n",
              "  textblob_label  afinn_score afinn_label  swn_score swn_label  liu_score  \\\n",
              "0       positive          3.0    positive      0.875  positive          0   \n",
              "1       positive          2.0    positive      0.625  positive         -1   \n",
              "2       positive          2.0    positive      0.125  positive          0   \n",
              "3       positive          9.0    positive      1.125  positive         -3   \n",
              "4       positive          4.0    positive      1.000  positive         -2   \n",
              "\n",
              "  liu_label  \n",
              "0   neutral  \n",
              "1  negative  \n",
              "2   neutral  \n",
              "3  negative  \n",
              "4  negative  "
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#2nd method preprocessing\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "    text = text.replace('x', '')\n",
        "#    text = re.sub(r'\\W+', '', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "    return text\n",
        "\n",
        "data['text'] = data['text'].apply(clean_text)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx8Vrr2G3BkX"
      },
      "outputs": [],
      "source": [
        "data.to_csv('labelled-preprocessed.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KaxeqvYOge0"
      },
      "source": [
        "####Select and Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTs8LV7lSVWt"
      },
      "outputs": [],
      "source": [
        "data_clean = data[['text','label','vader_label','textblob_label','afinn_label','swn_label','liu_label', 'splitset_label']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4qsUId4vJGH",
        "outputId": "d2c6f0a4-942f-42d5-94ac-c22435f21f30"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ca1b0b5b-7dec-4b18-a64a-22e4389b1c5b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>vader_label</th>\n",
              "      <th>textblob_label</th>\n",
              "      <th>afinn_label</th>\n",
              "      <th>swn_label</th>\n",
              "      <th>liu_label</th>\n",
              "      <th>splitset_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rock destined 21st century new conan going mak...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gorgeously elaborate continuation lord rings t...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>effective tootepid biopic</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sometimes like go movies fun wasabi good place...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>emerges something rare issue movie honest keen...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca1b0b5b-7dec-4b18-a64a-22e4389b1c5b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ca1b0b5b-7dec-4b18-a64a-22e4389b1c5b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ca1b0b5b-7dec-4b18-a64a-22e4389b1c5b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text     label vader_label  \\\n",
              "0  rock destined 21st century new conan going mak...  positive    positive   \n",
              "1  gorgeously elaborate continuation lord rings t...  positive    positive   \n",
              "2                          effective tootepid biopic   neutral    positive   \n",
              "3  sometimes like go movies fun wasabi good place...  positive    positive   \n",
              "4  emerges something rare issue movie honest keen...  positive    positive   \n",
              "\n",
              "  textblob_label afinn_label swn_label liu_label  splitset_label  \n",
              "0       positive    positive  positive   neutral               1  \n",
              "1       positive    positive  positive  negative               1  \n",
              "2       positive    positive  positive   neutral               2  \n",
              "3       positive    positive  positive  negative               2  \n",
              "4       positive    positive  positive  negative               2  "
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_clean.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0s6r_XXSZvw"
      },
      "outputs": [],
      "source": [
        "#data splitp\n",
        "train_df = data_clean.loc[data_clean['splitset_label'] == 1]\n",
        "test_df = data_clean.loc[data_clean['splitset_label'] == 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ1ZIdssf_AS",
        "outputId": "c1d06ad1-8a88-44bd-be94-c0a511b755ff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2acf16d0-5551-4bc8-b23b-8ac9301504aa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>vader_label</th>\n",
              "      <th>textblob_label</th>\n",
              "      <th>afinn_label</th>\n",
              "      <th>swn_label</th>\n",
              "      <th>liu_label</th>\n",
              "      <th>splitset_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rock destined 21st century new conan going mak...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gorgeously elaborate continuation lord rings t...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>singer composer bryan adams contributes slew s...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>think america would enough plucky british ecce...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>yet act still charming</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2acf16d0-5551-4bc8-b23b-8ac9301504aa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2acf16d0-5551-4bc8-b23b-8ac9301504aa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2acf16d0-5551-4bc8-b23b-8ac9301504aa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                 text     label vader_label  \\\n",
              "0   rock destined 21st century new conan going mak...  positive    positive   \n",
              "1   gorgeously elaborate continuation lord rings t...  positive    positive   \n",
              "60  singer composer bryan adams contributes slew s...  positive    positive   \n",
              "61  think america would enough plucky british ecce...   neutral    positive   \n",
              "62                             yet act still charming  positive    positive   \n",
              "\n",
              "   textblob_label afinn_label swn_label liu_label  splitset_label  \n",
              "0        positive    positive  positive   neutral               1  \n",
              "1        positive    positive  positive  negative               1  \n",
              "60        neutral    positive  negative  positive               1  \n",
              "61        neutral    positive  negative  negative               1  \n",
              "62       positive    positive  negative  negative               1  "
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AbGNJC9gAXC",
        "outputId": "812f0fd3-9d7e-4351-b8a4-f7761fd20a46"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e89c4728-80eb-4b2f-9f8b-2f4045aa9d4a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>vader_label</th>\n",
              "      <th>textblob_label</th>\n",
              "      <th>afinn_label</th>\n",
              "      <th>swn_label</th>\n",
              "      <th>liu_label</th>\n",
              "      <th>splitset_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>effective tootepid biopic</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sometimes like go movies fun wasabi good place...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>emerges something rare issue movie honest keen...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>film provides great insight neurotic mindset c...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>negative</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>offers rare combination entertainment education</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e89c4728-80eb-4b2f-9f8b-2f4045aa9d4a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e89c4728-80eb-4b2f-9f8b-2f4045aa9d4a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e89c4728-80eb-4b2f-9f8b-2f4045aa9d4a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text     label vader_label  \\\n",
              "2                          effective tootepid biopic   neutral    positive   \n",
              "3  sometimes like go movies fun wasabi good place...  positive    positive   \n",
              "4  emerges something rare issue movie honest keen...  positive    positive   \n",
              "5  film provides great insight neurotic mindset c...   neutral    positive   \n",
              "6    offers rare combination entertainment education  positive    positive   \n",
              "\n",
              "  textblob_label afinn_label swn_label liu_label  splitset_label  \n",
              "2       positive    positive  positive   neutral               2  \n",
              "3       positive    positive  positive  negative               2  \n",
              "4       positive    positive  positive  negative               2  \n",
              "5       positive    positive  positive  negative               2  \n",
              "6       positive     neutral  positive   neutral               2  "
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mECKmwbSOSMv",
        "outputId": "541d8bd9-b4b9-4bc7-beda-0543d44c4b0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2210, 8)"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAdYp4MS_AUZ"
      },
      "source": [
        "###."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEmt7v9tQF9k"
      },
      "source": [
        "####Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSHc1gmJCsEm"
      },
      "outputs": [],
      "source": [
        "# Number of labels: positive, negative, neutral\n",
        "num_classes = 3\n",
        "\n",
        "# Number of dimensions for word embedding\n",
        "embed_num_dims = 300\n",
        "\n",
        "#category\n",
        "class_names = ['negative', 'neutral', 'positive']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4UvbAERBVE_"
      },
      "outputs": [],
      "source": [
        "#text split\n",
        "X_train = train_df.text\n",
        "X_test = test_df.text\n",
        "\n",
        "#label split\n",
        "y_train_manual = train_df.label\n",
        "y_test_manual = test_df.label\n",
        "\n",
        "y_train_vader = train_df.vader_label\n",
        "y_test_vader = test_df.vader_label\n",
        "\n",
        "y_train_textblob = train_df.textblob_label\n",
        "y_test_textblob = test_df.textblob_label\n",
        "\n",
        "y_train_afinn = train_df.afinn_label\n",
        "y_test_afinn = test_df.afinn_label\n",
        "\n",
        "y_train_swn = train_df.swn_label\n",
        "y_test_swn = test_df.swn_label\n",
        "\n",
        "y_train_liu = train_df.liu_label\n",
        "y_test_liu = test_df.liu_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCpg2gBIFn40"
      },
      "outputs": [],
      "source": [
        "encoding = {\n",
        "    'negative': 1,\n",
        "    'neutral': 2,\n",
        "    'positive': 3\n",
        "}\n",
        "\n",
        "# Integer labels\n",
        "y_train_manual = [encoding[x] for x in train_df.label]\n",
        "y_test_manual = [encoding[x] for x in test_df.label]\n",
        "\n",
        "y_train_vader = [encoding[x] for x in train_df.vader_label]\n",
        "y_test_vader = [encoding[x] for x in test_df.vader_label]\n",
        "\n",
        "y_train_textblob = [encoding[x] for x in train_df.textblob_label]\n",
        "y_test_textblob = [encoding[x] for x in test_df.textblob_label]\n",
        "\n",
        "y_train_afinn = [encoding[x] for x in train_df.afinn_label]\n",
        "y_test_afinn = [encoding[x] for x in test_df.afinn_label]\n",
        "\n",
        "y_train_swn = [encoding[x] for x in train_df.swn_label]\n",
        "y_test_swn = [encoding[x] for x in test_df.swn_label]\n",
        "\n",
        "y_train_liu = [encoding[x] for x in train_df.liu_label]\n",
        "y_test_liu = [encoding[x] for x in test_df.liu_label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guGYrPBCFzgk",
        "outputId": "f338b2fb-f191-4141-b684-d3600d63864c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0.],\n",
              "       [0., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_manual = to_categorical(y_train_manual)\n",
        "y_test_manual = to_categorical(y_test_manual)\n",
        "\n",
        "y_train_vader = to_categorical(y_train_vader)\n",
        "y_test_vader = to_categorical(y_test_vader)\n",
        "\n",
        "y_train_textblob = to_categorical(y_train_textblob)\n",
        "y_test_textblob = to_categorical(y_test_textblob)\n",
        "\n",
        "y_train_afinn = to_categorical(y_train_afinn)\n",
        "y_test_afinn = to_categorical(y_test_afinn)\n",
        "\n",
        "y_train_swn = to_categorical(y_train_swn)\n",
        "y_test_swn = to_categorical(y_test_swn)\n",
        "\n",
        "y_train_liu = to_categorical(y_train_liu)\n",
        "y_test_liu = to_categorical(y_test_liu)\n",
        "\n",
        "y_train_manual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dTLj2eGQLfE"
      },
      "source": [
        "####Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUrUYVUUAzI-"
      },
      "outputs": [],
      "source": [
        "texts = data_clean.text.tolist()\n",
        "\n",
        "texts_train = X_train.tolist()\n",
        "texts_test = X_test.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Llt0TmW_IFZ",
        "outputId": "c959845d-240d-4716-929f-b2adcc6f0b99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique words: 19183\n"
          ]
        }
      ],
      "source": [
        "#tokenization + fitting\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data.text)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data.text)\n",
        "sequence_train = tokenizer.texts_to_sequences(texts_train)\n",
        "sequence_test = tokenizer.texts_to_sequences(texts_test)\n",
        "\n",
        "index_of_words = tokenizer.word_index\n",
        "\n",
        "# vacab size is number of unique words + reserved 0 index for padding\n",
        "vocab_size = len(tokenizer.word_counts)+1\n",
        "\n",
        "print('Number of unique words: {}'.format(vocab_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkX4Y-9URTLN"
      },
      "source": [
        "####Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGbg-844Ld12",
        "outputId": "5b2bfa3a-622d-495b-a658-571554bfa5ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean nmber of tokens: 9.41830451286377\n",
            "std of tokens: 4.734384110704758\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'fraction of reviews')"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_tokens = [len(tokens) for tokens in sequences]\n",
        "num_tokens = np.array(num_tokens)\n",
        "sn.distplot( num_tokens )\n",
        "plt.grid(True)\n",
        "\n",
        "mean_num_tokens = num_tokens.mean()\n",
        "std_num_tokens = num_tokens.std()\n",
        "print(\"mean nmber of tokens: {}\".format(mean_num_tokens))\n",
        "print(\"std of tokens: {}\".format(std_num_tokens))\n",
        "plt.xlabel('number of tokens')\n",
        "plt.ylabel('fraction of reviews')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKPYhOs7Lgks",
        "outputId": "b3f36a6f-8769-46d4-8c4c-4c8e03fc2bc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max length taken: 21\n",
            "untrimmed: 0.9821172501054407\n"
          ]
        }
      ],
      "source": [
        "# Max input length (max number of words) \n",
        "# max_seq_len = 500\n",
        "\n",
        "max_seq_len = int(mean_num_tokens + 2.5 * std_num_tokens)\n",
        "print(\"max length taken: {}\".format(max_seq_len))\n",
        "print(\"untrimmed: {}\".format(np.sum(num_tokens < max_seq_len) / len(num_tokens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvn_UKWdCpIm",
        "outputId": "f6748743-739b-4af8-ba81-58ad8583bf32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ..., 6898,  750, 9880],\n",
              "       [   0, 3109, 1935, ..., 9881, 5300, 9882],\n",
              "       [   0, 4365, 6925, ..., 5320,  342,  148],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,   20,  360,  292],\n",
              "       [   0,    0,    0, ...,  265, 1964, 3439],\n",
              "       [   0,    0,    0, ...,    0,  401, 5209]], dtype=int32)"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#padding\n",
        "\n",
        "X_train_pad = pad_sequences(sequence_train, maxlen = max_seq_len )\n",
        "X_test_pad = pad_sequences(sequence_test, maxlen = max_seq_len )\n",
        "\n",
        "X_train_pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofKRp9kYRDZ_",
        "outputId": "133c0f6b-5597-4118-d531-b43dc63a581d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2210, 21)"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test_pad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skDzIhNbRWYU"
      },
      "source": [
        "####Pretrained Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3eN3LEsDtbX"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "    return embedding_matrix\n",
        "\n",
        "fname = 'embeddings/wiki-news-300d-1M.vec'\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "    print('Downloading word vectors...')\n",
        "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip',\n",
        "                              'wiki-news-300d-1M.vec.zip')\n",
        "    print('Unzipping...')\n",
        "    with zipfile.ZipFile('wiki-news-300d-1M.vec.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('embeddings')\n",
        "    print('done.')\n",
        "    \n",
        "    os.remove('wiki-news-300d-1M.vec.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsS70aETDypw",
        "outputId": "ca203e17-dfcd-4cc8-d347-fe434ac27f39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(19183, 300)"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedd_matrix = create_embedding_matrix(fname, index_of_words, embed_num_dims)\n",
        "embedd_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcYdCRVrKFUW",
        "outputId": "f93743f6-6379-4a27-8e39-c9df5e795ed1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words found in wiki vocab: 16185\n",
            "New words found: 2997\n"
          ]
        }
      ],
      "source": [
        "# Inspect unseen words\n",
        "new_words = 0\n",
        "\n",
        "for word in index_of_words:\n",
        "    entry = embedd_matrix[index_of_words[word]]\n",
        "    if all(v == 0 for v in entry):\n",
        "        new_words = new_words + 1\n",
        "\n",
        "print('Words found in wiki vocab: ' + str(len(index_of_words) - new_words))\n",
        "print('New words found: ' + str(new_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ame80cLwKQfe"
      },
      "outputs": [],
      "source": [
        "# Embedding layer before the actaul LSTM \n",
        "embedd_layer = Embedding(vocab_size,\n",
        "                         embed_num_dims,\n",
        "                         input_length = max_seq_len,\n",
        "                         weights = [embedd_matrix],\n",
        "                         trainable=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4bvyCnPT-7N"
      },
      "source": [
        "####Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nWZ8z73pgt9",
        "outputId": "32d5ac55-1e31-4b4c-b04f-dbe710d677ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 21, 300)           5754900   \n",
            "                                                                 \n",
            " lstm_12 (LSTM)              (None, 21, 64)            93440     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 21, 64)            0         \n",
            "                                                                 \n",
            " lstm_13 (LSTM)              (None, 21, 64)            33024     \n",
            "                                                                 \n",
            " lstm_14 (LSTM)              (None, 21, 64)            33024     \n",
            "                                                                 \n",
            " global_max_pooling1d_4 (Glo  (None, 64)               0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,918,743\n",
            "Trainable params: 163,843\n",
            "Non-trainable params: 5,754,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#model\n",
        "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "batch_size = 36\n",
        "epochs = 200\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedd_layer)\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKMmhf99OePY"
      },
      "outputs": [],
      "source": [
        "model_manual = model\n",
        "model_vader = model\n",
        "model_textblob = model\n",
        "model_afinn = model\n",
        "model_swn = model\n",
        "model_liu = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQvVs-uVpgt-",
        "outputId": "3a9ced25-620d-406d-de9d-1290ade950a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-fb813b0fb693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                  validation_data=(X_test_pad,y_test_manual))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 949, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 139, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1788, in categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5119, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 4) and (None, 3) are incompatible\n"
          ]
        }
      ],
      "source": [
        "hist_manual = model_manual.fit(X_train_pad, y_train_manual, \n",
        "                 batch_size=batch_size,\n",
        "                 epochs=epochs,\n",
        "                 validation_data=(X_test_pad,y_test_manual))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ7r3lSQonOt"
      },
      "outputs": [],
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(hist_manual.history['accuracy'])\n",
        "plt.plot(hist_manual.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# \"Loss\"\n",
        "plt.plot(hist_manual.history['loss'])\n",
        "plt.plot(hist_manual.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQUfZMTfPoxz"
      },
      "outputs": [],
      "source": [
        "predictions_manual = model_manual.predict(X_test_pad)\n",
        "predictions_manual = np.argmax(predictions_manual, axis=1)\n",
        "predictions_manual = [class_names[pred] for pred in predictions_manual]\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(test_df.label, predictions_manual) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions_manual, average='micro') * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezOXJCYx1-td"
      },
      "outputs": [],
      "source": [
        "hist_vader = model_vader.fit(X_train_pad, y_train_vader, \n",
        "                 batch_size=batch_size,\n",
        "                 epochs=epochs,\n",
        "                 validation_data=(X_test_pad,y_test_vader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZgdJySWoz8N"
      },
      "outputs": [],
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(hist_vader.history['accuracy'])\n",
        "plt.plot(hist_vader.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# \"Loss\"\n",
        "plt.plot(hist_vader.history['loss'])\n",
        "plt.plot(hist_vader.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4NFLLMAQNh4"
      },
      "outputs": [],
      "source": [
        "predictions_vader = model_vader.predict(X_test_pad)\n",
        "predictions_vader = np.argmax(predictions_vader, axis=1)\n",
        "predictions_vader = [class_names[pred] for pred in predictions_vader]\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(test_df.label, predictions_vader) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions_vader, average='micro') * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myi3qpRl6atM"
      },
      "outputs": [],
      "source": [
        "hist_afinn = model_afinn.fit(X_train_pad, y_train_afinn, \n",
        "                 batch_size=batch_size,\n",
        "                 epochs=epochs,\n",
        "                 validation_data=(X_test_pad,y_test_afinn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhGeFhWbpfpb"
      },
      "outputs": [],
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(hist_afinn.history['accuracy'])\n",
        "plt.plot(hist_afinn.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# \"Loss\"\n",
        "plt.plot(hist_afinn.history['loss'])\n",
        "plt.plot(hist_afinn.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkIg4IEbRAWQ"
      },
      "outputs": [],
      "source": [
        "predictions_afinn = model_afinn.predict(X_test_pad)\n",
        "predictions_afinn = np.argmax(predictions_afinn, axis=1)\n",
        "predictions_afinn = [class_names[pred] for pred in predictions_afinn]\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(test_df.label, predictions_afinn) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions_afinn, average='micro') * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weTNdg056d9t"
      },
      "outputs": [],
      "source": [
        "hist_swn = model_swn.fit(X_train_pad, y_train_swn, \n",
        "                 batch_size=batch_size,\n",
        "                 epochs=epochs,\n",
        "                 validation_data=(X_test_pad,y_test_swn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYYk1MnKpqIM"
      },
      "outputs": [],
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(hist_swn.history['accuracy'])\n",
        "plt.plot(hist_swn.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# \"Loss\"\n",
        "plt.plot(hist_swn.history['loss'])\n",
        "plt.plot(hist_swn.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biXrghU4RERH"
      },
      "outputs": [],
      "source": [
        "predictions_swn = model_swn.predict(X_test_pad)\n",
        "predictions_swn = np.argmax(predictions_swn, axis=1)\n",
        "predictions_swn = [class_names[pred] for pred in predictions_swn]\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(test_df.label, predictions_swn) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions_swn, average='micro') * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zohF5TeU6kM0"
      },
      "outputs": [],
      "source": [
        "hist_liu = model_liu.fit(X_train_pad, y_train_liu, \n",
        "                 batch_size=batch_size,\n",
        "                 epochs=epochs,\n",
        "                 validation_data=(X_test_pad,y_test_liu))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiCtJr2JRJD_"
      },
      "outputs": [],
      "source": [
        "predictions_liu = model_liu.predict(X_test_pad)\n",
        "predictions_liu = np.argmax(predictions_liu, axis=1)\n",
        "predictions_liu = [class_names[pred] for pred in predictions_liu]\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(test_df.label, predictions_liu) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions_liu, average='micro') * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLtQXvnwpgt-"
      },
      "outputs": [],
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# \"Loss\"\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQZ64cSawXCk"
      },
      "source": [
        "######Model Testing and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfPz7SR1pgt-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INtKi5tp6lyv"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X_test_pad)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "predictions = [class_names[pred] for pred in predictions]\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(test_df.label, predictions) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions, average='micro') * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjkql5r4yGgV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6uvbF-ZzN8l"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YK5llN9pgt_"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    '''\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    '''\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    \n",
        "    # Set size\n",
        "    fig.set_size_inches(12.5, 7.5)\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.grid(False)\n",
        "    \n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_dcqVHRpgt_"
      },
      "outputs": [],
      "source": [
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions, average='micro') * 100))\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix(test_df.label, predictions, classes=class_names, normalize=True, title='Normalized confusion matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njY8ZZiZwlU2"
      },
      "source": [
        "#Another Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtPlavDkMwgU"
      },
      "source": [
        "###Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBY9OhJbVn34"
      },
      "outputs": [],
      "source": [
        "data_aug = data[data['label'] == 'neutral']\n",
        "data_aug['text'] = data_aug['text'].apply(lambda x : eda_aug.augment(x))\n",
        "\n",
        "from random import randrange\n",
        "for row in data_aug.index:\n",
        "  try:\n",
        "    num = randrange(0,2)\n",
        "    data_aug['text'][row] = data_aug['text'][row][num]\n",
        "  except:\n",
        "    data_aug['text'][row] = data_aug['text'][row][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR1B83CWmjU8"
      },
      "outputs": [],
      "source": [
        "data = data.append(data_aug)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGanQIQMnzco"
      },
      "outputs": [],
      "source": [
        "data = data.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2GoK6ZIPn3on",
        "outputId": "3a3a2af6-f306-413f-f062-fb6e3f9c30c7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6c198ff1-4c2b-42ce-8bc4-43d70a87ae67\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment values</th>\n",
              "      <th>splitset_label</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>0.69444</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>0.83333</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>0.51389</td>\n",
              "      <td>2</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>0.73611</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>0.86111</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c198ff1-4c2b-42ce-8bc4-43d70a87ae67')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6c198ff1-4c2b-42ce-8bc4-43d70a87ae67 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6c198ff1-4c2b-42ce-8bc4-43d70a87ae67');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   index                                               text  sentiment values  \\\n",
              "0      0  The Rock is destined to be the 21st Century 's...           0.69444   \n",
              "1      1  The gorgeously elaborate continuation of `` Th...           0.83333   \n",
              "2      2                     Effective but too-tepid biopic           0.51389   \n",
              "3      3  If you sometimes like to go to the movies to h...           0.73611   \n",
              "4      4  Emerges as something rare , an issue movie tha...           0.86111   \n",
              "\n",
              "   splitset_label     label  \n",
              "0               1  positive  \n",
              "1               1  positive  \n",
              "2               2   neutral  \n",
              "3               2  positive  \n",
              "4               2  positive  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eatUrQi7_ZJZ",
        "outputId": "58eefdc4-5f83-4eb6-e606-9b337f507747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 21, 300)           5754900   \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (None, 21, 8)             9888      \n",
            "                                                                 \n",
            " global_max_pooling1d_4 (Glo  (None, 8)                0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 16)                144       \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,764,983\n",
            "Trainable params: 10,083\n",
            "Non-trainable params: 5,754,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#model\n",
        "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "\n",
        "model=Sequential()\n",
        "\n",
        "#lstm layer\n",
        "model.add(embedd_layer)\n",
        "\n",
        "model.add(LSTM(8,return_sequences=True,dropout=0.3))\n",
        "\n",
        "#Global Maxpooling\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "#Dense Layer\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M8PQnV5YhTI"
      },
      "source": [
        "### TextBlob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-tjpWLiYhTI"
      },
      "source": [
        "####Create Sentiment Score Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOd4bDAQYhTJ"
      },
      "outputs": [],
      "source": [
        "def textblob_score(sentence):\n",
        "    sntmnt = textblob.TextBlob(sentence).sentiment.polarity\n",
        "    return sntmnt\n",
        "\n",
        "def textblob_label(score):\n",
        "    if (score >= 0.1):\n",
        "        return 'positive'\n",
        "    elif ((score > -0.1) & (score < 0.1)):\n",
        "        return 'neutral'\n",
        "    elif (score <= -0.1):\n",
        "        return 'negative'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKUWJt5KYhTJ"
      },
      "source": [
        "####Calculate Sentiment Score & Create Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c-oN23WYhTK"
      },
      "outputs": [],
      "source": [
        "# start = time.time()\n",
        "\n",
        "# textblob_score =  []\n",
        "\n",
        "# for comment in data.text.to_list():\n",
        "#     snts_score = calculate_sentiment_scores_textblob(comment)\n",
        "#     textblob_score.append(snts_score)\n",
        "\n",
        "\n",
        "data['textblob_score'] = data.text.apply(textblob_score)\n",
        "data['textblob_label'] = data.textblob_score.apply(textblob_label)\n",
        "\n",
        "# end = time.time()\n",
        "\n",
        "# # total time taken\n",
        "# print(f\"Runtime of the program is {(end - start)/60} minutes or {(end - start)} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJQEwWPm_htu",
        "outputId": "14427fbf-b2d1-4258-8931-63b8306de6d1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-64f6f5b4-d2af-4a8e-87c7-32bd1916ea54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment values</th>\n",
              "      <th>splitset_label</th>\n",
              "      <th>label</th>\n",
              "      <th>vader_score</th>\n",
              "      <th>vader_label</th>\n",
              "      <th>textblob_score</th>\n",
              "      <th>textblob_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>0.69444</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3612</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.212121</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>0.51389</td>\n",
              "      <td>2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.2617</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>0.73611</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.8271</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The film provides some great insight into the ...</td>\n",
              "      <td>0.59722</td>\n",
              "      <td>2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.5994</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Perhaps no picture ever made has more literall...</td>\n",
              "      <td>0.69444</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.5994</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-64f6f5b4-d2af-4a8e-87c7-32bd1916ea54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-64f6f5b4-d2af-4a8e-87c7-32bd1916ea54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-64f6f5b4-d2af-4a8e-87c7-32bd1916ea54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text  sentiment values  \\\n",
              "0  The Rock is destined to be the 21st Century 's...           0.69444   \n",
              "2                     Effective but too-tepid biopic           0.51389   \n",
              "3  If you sometimes like to go to the movies to h...           0.73611   \n",
              "5  The film provides some great insight into the ...           0.59722   \n",
              "7  Perhaps no picture ever made has more literall...           0.69444   \n",
              "\n",
              "   splitset_label     label  vader_score vader_label  textblob_score  \\\n",
              "0               1  positive       0.3612    positive        0.212121   \n",
              "2               2   neutral       0.2617    positive        0.600000   \n",
              "3               2  positive       0.8271    positive        0.500000   \n",
              "5               2   neutral       0.5994    positive        0.275000   \n",
              "7               2  positive      -0.5994    negative        0.600000   \n",
              "\n",
              "  textblob_label  \n",
              "0       positive  \n",
              "2       positive  \n",
              "3       positive  \n",
              "5       positive  \n",
              "7       positive  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0dXckchVBHv",
        "outputId": "e715c86b-3260-43c5-abc4-3d0d2c0fec2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-1.0"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['textblob_score'].min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtMMkwCweCbU"
      },
      "source": [
        "#####Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpXVP8FxCUFw"
      },
      "outputs": [],
      "source": [
        "#model\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedd_layer)\n",
        "model.add(LSTM(8, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj6Po2a9ZAvc"
      },
      "outputs": [],
      "source": [
        "model_history = model.fit(X_train_pad, y_train_manual,\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=epochs,\n",
        "                          validation_data=(X_test_pad,y_test_manual))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DeB5ZNFdNML"
      },
      "outputs": [],
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(model_history.history['accuracy'])\n",
        "plt.plot(model_history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# \"Loss\"\n",
        "plt.plot(model_history.history['loss'])\n",
        "plt.plot(model_history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jra7grMtwrAq"
      },
      "source": [
        "######Model Testing and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AN_PbWfLwrAr"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X_test_pad)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "predictions = [class_names[pred] for pred in predictions]\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(test_df.label, predictions) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions, average='micro') * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IN3i4_0wrAr"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    '''\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    '''\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    \n",
        "    # Set size\n",
        "    fig.set_size_inches(12.5, 7.5)\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.grid(False)\n",
        "    \n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN5Bf9_fwrAr"
      },
      "outputs": [],
      "source": [
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions, average='micro') * 100))\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix(test_df.label, predictions, classes=class_names, normalize=True, title='Normalized confusion matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EUVlR7WeGqE"
      },
      "source": [
        "#####Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBd0te9sKVAm"
      },
      "outputs": [],
      "source": [
        "# keras layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, GRU, Dense\n",
        "# Parameters\n",
        "gru_output_size = 128\n",
        "bidirectional = True\n",
        "\n",
        "# Embedding Layer, LSTM or biLSTM, Dense, softmax\n",
        "model = Sequential()\n",
        "model.add(embedd_layer)\n",
        "\n",
        "if bidirectional:\n",
        "    model.add(Bidirectional(GRU(units=gru_output_size,\n",
        "                              dropout=0.2,\n",
        "                              recurrent_dropout=0.2)))\n",
        "else:\n",
        "     model.add(GRU(units=gru_output_size,\n",
        "                dropout=0.2, \n",
        "                recurrent_dropout=0.2))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO8tOJjiKpvO"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "hist = model.fit(X_train_pad, y_train_manual, \n",
        "                 batch_size=batch_size,\n",
        "                 epochs=epochs,\n",
        "                 validation_data=(X_test_pad,y_test_manual))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJHkG5Rxnnjk"
      },
      "outputs": [],
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# \"Loss\"\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_xybk4Nww0M"
      },
      "source": [
        "######Model Testing and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-3XBBxuww0N"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X_test_pad)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "predictions = [class_names[pred] for pred in predictions]\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(test_df.label, predictions) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions, average='micro') * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5UNB8teww0N"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    '''\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    '''\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    \n",
        "    # Set size\n",
        "    fig.set_size_inches(12.5, 7.5)\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.grid(False)\n",
        "    \n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKrcOXwnww0N"
      },
      "outputs": [],
      "source": [
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions, average='micro') * 100))\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix(test_df.label, predictions, classes=class_names, normalize=True, title='Normalized confusion matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6gKSbyBeK4L"
      },
      "source": [
        "#####Model 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4nXTItqeK4M"
      },
      "outputs": [],
      "source": [
        "#model\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedd_layer)\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKXoBaY_eK4N"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "hist = model.fit(X_train_pad, y_train_manual, \n",
        "                 batch_size=batch_size,\n",
        "                 epochs=epochs,\n",
        "                 validation_data=(X_test_pad,y_test_manual))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzAI-zTdeK4N"
      },
      "outputs": [],
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# \"Loss\"\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAbx4N4pwo4X"
      },
      "source": [
        "######Model Testing and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHaehi60wo4Y"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X_test_pad)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "predictions = [class_names[pred] for pred in predictions]\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(test_df.label, predictions) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions, average='micro') * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jDIfgRXwo4Y"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    '''\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    '''\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    \n",
        "    # Set size\n",
        "    fig.set_size_inches(12.5, 7.5)\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.grid(False)\n",
        "    \n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESM6L29Cwo4Z"
      },
      "outputs": [],
      "source": [
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(test_df.label, predictions, average='micro') * 100))\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix(test_df.label, predictions, classes=class_names, normalize=True, title='Normalized confusion matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai8Asi6QwjL3"
      },
      "outputs": [],
      "source": [
        "#Another Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlxxvtBLOBMG"
      },
      "source": [
        "##6. Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INey4yoAUtFn"
      },
      "outputs": [],
      "source": [
        "#embedding\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if word in w2v_model.wv:\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\n",
        "print(embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M70xYxyWUxJo"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_text_length, trainable=False))\n",
        "model.add(LSTM(8, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UBoHThalDN0"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_history=model.fit(X_train, y_train,batch_size=64,epochs=10,validation_split=0.1,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i27sdsPMcAdG"
      },
      "outputs": [],
      "source": [
        "acc = model_history.history['accuracy']\n",
        "val_acc = model_history.history['val_accuracy']\n",
        "loss = model_history.history['loss']\n",
        "val_loss = model_history.history['val_loss']\n",
        "epochs=range(len(acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5C_XWyIcEKd"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs,acc,label='Trainin_acc',color='blue')\n",
        "plt.plot(epochs,val_acc,label='Validation_acc',color='red')\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XnZtvRXcJJN"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs,loss,label='Training_loss',color='blue')\n",
        "plt.plot(epochs,val_loss,label='Validation_loss',color='red')\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYyW3mKlcOnN"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    review = casefolding(text)\n",
        "    review = filtering(review)\n",
        "    review = tokenize(review)\n",
        "    review = stopword_removal(review)\n",
        "    review = stemming(review)\n",
        "    print(review)\n",
        "    review=pad_sequences(tokenizer.texts_to_sequences([review]), maxlen=300)\n",
        "    return review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2JdA3HYcRF1"
      },
      "outputs": [],
      "source": [
        "def prediction(review):\n",
        "    review=preprocess(review)\n",
        "    score=model.predict(review)\n",
        "    score=score[0]\n",
        "    if score<0.4:\n",
        "        print(\"Negative\")\n",
        "    elif score>0.4 and score<0.6:\n",
        "        print(\"Neutral\")\n",
        "    else:\n",
        "        print(\"Positive\")\n",
        "    print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aknH2NdcU6e"
      },
      "outputs": [],
      "source": [
        "prediction(\"this disturbing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmZ9oOdwdJQ3"
      },
      "outputs": [],
      "source": [
        "kalimat = \"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side\";\n",
        "prediction(kalimat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcPoCREhdaK2"
      },
      "outputs": [],
      "source": [
        "scores = model.predict(X_test, verbose=1, batch_size=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip9pTHjadfRv"
      },
      "outputs": [],
      "source": [
        "y_pred=np.where(scores>0.5,1,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7hek9wVdhCV"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZKnp2E4div3"
      },
      "outputs": [],
      "source": [
        "cm=confusion_matrix(y_pred,y_test)\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQUHfNnZdmNf"
      },
      "outputs": [],
      "source": [
        "print(accuracy_score(y_pred,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjF1GpS1doL1"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Csw8QZcgdqAF"
      },
      "outputs": [],
      "source": [
        "joblib.dump(w2v_model,'word2vec.pkl')\n",
        "joblib.dump(tokenizer,'tokenizer.pkl')\n",
        "joblib.dump(model,'final_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z9I-wREI2c3"
      },
      "source": [
        "##7. Evaluasi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF7xOcCB-OAe"
      },
      "source": [
        "### Accuracy, Precision, Recall, F1-Score, Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_QYit5GIFAe"
      },
      "outputs": [],
      "source": [
        "print('SVM Accuracy : ', accuracy_score(y_test,predictedSVM))\n",
        "print('SVM Precision : ', precision_score(y_test,predictedSVM, average=\"binary\", pos_label=\"negative\"))\n",
        "print('SVM Recall : ', recall_score(y_test,predictedSVM, average=\"binary\", pos_label=\"negative\"))\n",
        "print('SVM f1_score : ', f1_score(y_test,predictedSVM, average=\"binary\", pos_label=\"negative\"))\n",
        "\n",
        "print(f'confusion_matrix : \\n {confusion_matrix(y_test,predictedSVM)}')\n",
        "print('=========================================================\\n')\n",
        "print(classification_report(y_test,predictedSVM,zero_division=0))\n",
        "plot_confusion_matrix(modelSVM, x_test, y_test)  \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fRhXoSZ-UPt"
      },
      "source": [
        "### Wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeV2p6WjVKFU"
      },
      "outputs": [],
      "source": [
        "#wordcloud positif\n",
        "df_pos = data_clean[data_clean['vader_sentiment_labels'].str.contains(\"positive\")]\n",
        "wordcloud_positif = WordCloud().generate(' '.join(df_pos['text']))\n",
        "plt.imshow(wordcloud_positif)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBIR6G4TI7BP"
      },
      "outputs": [],
      "source": [
        "#wordcloud negatif\n",
        "df_neg = data_clean[data_clean['vader_sentiment_labels'].str.contains(\"negative\")]\n",
        "wordcloud_negatif = WordCloud().generate(' '.join(df_neg['text']))\n",
        "plt.imshow(wordcloud_negatif)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jFzkvlU-WMs"
      },
      "source": [
        "### Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzyGqIGSWyAs"
      },
      "outputs": [],
      "source": [
        "#Cross Validation\n",
        "scores = cross_val_score(modelSVM, text_tf, data_clean['vader_sentiment_labels'], cv=10)\n",
        "print('Hasil Cross Validation : ', scores)\n",
        "scores_mean = np.mean(scores)\n",
        "print('Rata-rata Cross Validation : ', scores_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSJw7lm5_Ml0"
      },
      "source": [
        "### Export and Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epJWPOLq_OUa"
      },
      "outputs": [],
      "source": [
        "# save the model to disk\n",
        "filename = 'modelSVM.sav'\n",
        "pickle.dump(modelSVM, open(filename, 'wb'))\n",
        " \n",
        "# load the model from disk\n",
        "modelSVM_loaded = pickle.load(open(filename, 'rb'))\n",
        "result = modelSVM_loaded.score(x_test, y_test)\n",
        "\n",
        "predictSVM = modelSVM_loaded.predict(x_test)\n",
        "print('SVM Accuracy : ', accuracy_score(y_test,predictSVM))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmCk234X-nqG"
      },
      "source": [
        "##Other"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BoHvq7YqboP"
      },
      "source": [
        "###4. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBMrmKTfDxET"
      },
      "source": [
        "###Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V751j365Dz5D"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('labelled.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ywkzyc9oaTn"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VigXo_K8sZ9H"
      },
      "source": [
        "###Create Preprocessing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkpkqX0bx4XE"
      },
      "outputs": [],
      "source": [
        "#casefolding\n",
        "def casefolding(text):\n",
        "  text = text.lower()\n",
        "  return text\n",
        "\n",
        "# #filtering\n",
        "# def remove_urltag(text):\n",
        "#     # Remove link web\n",
        "#     text = re.sub(r'http\\S+', '', text)\n",
        "#     # Remove @username\n",
        "#     text = re.sub('@[^\\s]+', '', text)\n",
        "#     # Remove #tagger\n",
        "#     text = re.sub(r'#([^\\s]+)', '', text)\n",
        "#     # remove br tag\n",
        "#     text = re.sub('<br />', '', text)\n",
        "#     return text\n",
        "\n",
        "# def remove_num(text):\n",
        "#     text = re.sub(r'[0-9]', '', text)\n",
        "#     return text\n",
        "\n",
        "# def remove_punct(text):\n",
        "#     text = re.sub(r'[^\\w\\s]', '', text)\n",
        "#     return text\n",
        "\n",
        "def filtering(text):\n",
        "    # Remove link web\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # remove br tag\n",
        "    text = re.sub('<br />', '', text)\n",
        "    # Remove @username\n",
        "    text = re.sub('@[^\\s]+', '', text)\n",
        "    # Remove #tagger\n",
        "    text = re.sub(r'#([^\\s]+)', '', text)\n",
        "    # Remove angka termasuk angka yang berada dalam string\n",
        "    # Remove non ASCII chars\n",
        "    text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
        "    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)', r'', text)\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r'\\\\u\\w\\w\\w\\w', '', text)\n",
        "    # Remove simbol, angka dan karakter aneh\n",
        "    text = re.sub(r\"[.,:;+!\\-_<^/=?\\\"'\\(\\)\\d\\*]\", \" \", text)\n",
        "    return text    \n",
        "\n",
        "#tokenizing\n",
        "def tokenize(text):\n",
        "    token = nltk.word_tokenize(text)\n",
        "    return token\n",
        "\n",
        "#stopword\n",
        "def stopword_removal(text):\n",
        "    filtering = stopwords.words('english')\n",
        "    x = []\n",
        "    data = []\n",
        "    def myFunc(x):\n",
        "      if x in filtering:\n",
        "        return False\n",
        "      else:\n",
        "        return True\n",
        "    fit = filter(myFunc, text)\n",
        "    for x in fit:\n",
        "      data.append(x)\n",
        "    return data\n",
        "\n",
        "#stemming\n",
        "def stemming(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    do = []\n",
        "    for w in text:\n",
        "        dt = stemmer.stem(w)\n",
        "        do.append(dt)\n",
        "    d_clean=[]\n",
        "    d_clean=\" \".join(do)\n",
        "    return (d_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yO-Ce_Ksg31"
      },
      "source": [
        "###Start Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niHx2yTLXGwT"
      },
      "outputs": [],
      "source": [
        "data['text'] = data['text'].apply(casefolding)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3-uEU-OglsN"
      },
      "outputs": [],
      "source": [
        "data['text']=data['text'].apply(filtering)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StJEBax4MeaS"
      },
      "outputs": [],
      "source": [
        "data['text']=data['text'].apply(tokenize)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR_pefEP9mEM"
      },
      "outputs": [],
      "source": [
        "data['text']=data['text'].apply(stopword_removal)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CoAHrkyDU12"
      },
      "outputs": [],
      "source": [
        "data['text'] = data['text'].apply(stemming)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WumQUB8Mjb7"
      },
      "source": [
        "###Drop Neutral, Drop Duplicate & Null, Check Null, Reset Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRJu2zWkrkgn"
      },
      "outputs": [],
      "source": [
        "# data = data[(data.vader_label != \"neutral\")]\n",
        "# data = data[(data.textblob_label != \"neutral\")]\n",
        "# data = data[(data.afinn_label != \"neutral\")]\n",
        "# data = data[(data.swn_label != \"neutral\")]\n",
        "# data = data[(data.liu_label != \"neutral\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzUsJ7ajMm75"
      },
      "outputs": [],
      "source": [
        "# data = data.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)\n",
        "data = data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flsegWyr8DFw"
      },
      "outputs": [],
      "source": [
        "data = data.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxtmFmNtNJSR"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-30QZJ7eM0PT"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMi4hfIzM1Hi"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Awixxfgt4cA"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAeIZ1HTvPsZ"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oknnAIm3UCny"
      },
      "outputs": [],
      "source": [
        "tes = []\n",
        "\n",
        "tes.append(tokenizer.word_index.get('rock'))\n",
        "tes.append(tokenizer.word_index.get('destin'))\n",
        "tes.append(tokenizer.word_index.get('st'))\n",
        "tes.append(tokenizer.word_index.get('centuri'))\n",
        "tes.append(tokenizer.word_index.get('new'))\n",
        "tes.append(tokenizer.word_index.get('conan'))\n",
        "tes.append(tokenizer.word_index.get('go'))\n",
        "tes.append(tokenizer.word_index.get('make'))\n",
        "tes.append(tokenizer.word_index.get('splash'))\n",
        "tes.append(tokenizer.word_index.get('even'))\n",
        "tes.append(tokenizer.word_index.get('greater'))\n",
        "tes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9XgV9QCCS5C"
      },
      "outputs": [],
      "source": [
        "tes_seq = pad_sequences(tokenizer.texts_to_sequences('rock destin st centuri new conan go make splash even greater'), maxlen=300)\n",
        "tes_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSPveRJMbTyj"
      },
      "source": [
        "####Create Flair Classification Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0IcBDxAbTyk"
      },
      "outputs": [],
      "source": [
        "# def make_sentences(text):\n",
        "#     sentences = [sent for sent in split_single(text)]\n",
        "#     return sentences\n",
        "\n",
        "def flair_score(sentence):\n",
        "    if sentence == \"\":\n",
        "        return 0\n",
        "    text = Sentence(sentence)\n",
        "    classifier.predict(text)\n",
        "    label = text.labels[0].to_dict()['value'] \n",
        "    if label == 'POSITIVE':\n",
        "        confidence = text.to_dict()['labels'][0]['confidence']\n",
        "    else:\n",
        "        confidence = -(text.to_dict()['labels'][0]['confidence'])\n",
        "    return confidence\n",
        "\n",
        "def flair_label(score):\n",
        "    if (score > 0):\n",
        "        return 'positive'\n",
        "    elif (score < 0):\n",
        "        return 'negative'\n",
        "\n",
        "# def get_predict_result(sentences):\n",
        "#     \"\"\" Call predict on every sentence of a text \"\"\"\n",
        "#     flair_confidence = []\n",
        "    \n",
        "#     for i in range(0, len(sentences)): \n",
        "#         flair_confidence.append(flair_predict(sentences[i]))\n",
        "#     return flair_confidence\n",
        "\n",
        "# def flair_predict_confidence(sentence):\n",
        "#     text = Sentence(sentence)\n",
        "#     classifier.predict(text)\n",
        "#     return text.labels[0].to_dict()['confidence']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE8I3GYyFAmz"
      },
      "source": [
        "####Calculate Sentiment Score & Create Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkchBNOCZnrN"
      },
      "outputs": [],
      "source": [
        "# data['sentences'] = data.text.apply(make_sentences)\n",
        "data['flair_score'] = data.text.apply(flair_score)\n",
        "data['flair_label'] = data.flair_score.apply(flair_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYZi4me7ExRc"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QkcfM3SOMNc"
      },
      "source": [
        "####Remove Neutral Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7b3oX4aOO95"
      },
      "outputs": [],
      "source": [
        "data = data[data.vader_sentiment_labels != 'neutral']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-h7xIYLOP6w"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFYgDcRmOmWD"
      },
      "source": [
        "###Create w2v Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ89LlSiTnQg"
      },
      "outputs": [],
      "source": [
        "documents = [text.split() for text in train_df.text]\n",
        "documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV0YG6AfeWi7"
      },
      "outputs": [],
      "source": [
        "import seaborn as sn\n",
        "num_tokens = [len(tokens) for tokens in documents]\n",
        "num_tokens = np.array(num_tokens)\n",
        "sn.distplot( num_tokens )\n",
        "plt.grid(True)\n",
        "\n",
        "mean_num_tokens = num_tokens.mean()\n",
        "std_num_tokens = num_tokens.std()\n",
        "print(\"mean nmber of tokens: {}\".format(mean_num_tokens))\n",
        "print(\"std of tokens: {}\".format(std_num_tokens))\n",
        "plt.xlabel('number of tokens')\n",
        "plt.ylabel('fraction of reviews')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSLJMYHbfHjH"
      },
      "outputs": [],
      "source": [
        "len_data = [len(x.split()) for x in train_df.text]\n",
        "print(np.mean(len_data))\n",
        "print(np.median(len_data))\n",
        "print(np.std(len_data))\n",
        "print(np.min(len_data))\n",
        "print(np.max(len_data))\n",
        "print(np.percentile(len_data, 90))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVRNIVa27f1y"
      },
      "outputs": [],
      "source": [
        "unique_string = set()\n",
        "for x in train_df.text:\n",
        "    for y in x.split():\n",
        "        unique_string.add(y)\n",
        "        \n",
        "print(len(unique_string))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NQ5PQQ27UxZ"
      },
      "outputs": [],
      "source": [
        "max_features = 16500 # how many unique words to use (since the total of unique word is only 2816)\n",
        "maxlen = 16 # max number of words in a text to use (from 90th percentile)\n",
        "\n",
        "start_preprocess2 = time.time()\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(train_df.text)\n",
        "\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(train_df.text.values)\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(test_df.text.values)\n",
        "\n",
        "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
        "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
        "\n",
        "end_preprocess2 = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbMH1YkC9jUI"
      },
      "outputs": [],
      "source": [
        "embed_size = 100\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = max_features\n",
        "embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euDQ9U7PTvFY"
      },
      "outputs": [],
      "source": [
        "w2v_model = gensim.models.word2vec.Word2Vec(size=300, \n",
        "                                            window=7, \n",
        "                                            min_count=10, \n",
        "                                            workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlgNBgeOTwpg"
      },
      "outputs": [],
      "source": [
        "w2v_model.build_vocab(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHvCWrqFTzyI"
      },
      "outputs": [],
      "source": [
        "words = w2v_model.wv.vocab.keys()\n",
        "vocab_size = len(words)\n",
        "print(\"Vocab size\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXyKUWEGT2h_"
      },
      "outputs": [],
      "source": [
        "w2v_model.train(documents, total_examples=len(documents), epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFHExG3DT59H"
      },
      "outputs": [],
      "source": [
        "w2v_model.wv.most_similar(\"bad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHeiSk5nT9qf"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_df.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSy3HlZ2UGyH"
      },
      "outputs": [],
      "source": [
        "vocab_size=len(tokenizer.word_index)+1\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sems7rlUUMpY"
      },
      "outputs": [],
      "source": [
        "X_train = pad_sequences(tokenizer.texts_to_sequences(train_df.text), maxlen=max_text_length)\n",
        "y_train=train_df.label\n",
        "\n",
        "X_test = pad_sequences(tokenizer.texts_to_sequences(test_df.text), maxlen=max_text_length)\n",
        "y_test=test_df.label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P79CLIfiUncH"
      },
      "outputs": [],
      "source": [
        "labelencoder = LabelEncoder()\n",
        "y_train = labelencoder.fit_transform(y_train)\n",
        "y_test = labelencoder.fit_transform(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os-VjzRGFPcE"
      },
      "source": [
        "# Referensi Kode Program\n",
        "\n",
        "https://www.youtube.com/watch?v=8ApgsLvbTz4\n",
        "\n",
        "https://github.com/kungfumas/analisa-sentimen-support-vector-machine/blob/master/Sentiment%20Analysis%20with%20Python.ipynb"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "LYmQ0sSarBNK",
        "J0by_tBHOFyn",
        "HPe02Y2rjJBp",
        "yIYQQyvByDS6",
        "jcxQdGQuL6LR",
        "sG8uLDwVs_u1",
        "0xPV96Jhg6k1",
        "Gp6QKfdkNOKy",
        "gD0PmqweYQik",
        "Mtg_RQcBNbFB",
        "telLcPzPNtYI",
        "6M8PQnV5YhTI",
        "b-tjpWLiYhTI",
        "vKUWJt5KYhTJ",
        "zugYznWT8qfg",
        "j0IjCgBg8qfp",
        "kH7ITQHB8qfq",
        "Mt-VXnBTHuwt",
        "UD_QR1hMKEQs",
        "L4fYd7JXKNjH",
        "4J98M83DPvQs",
        "imioK5uLgJ26",
        "Y6TB7OtkVWWN",
        "4rUtJ2Jodg0c",
        "z_uOzmQDY1hW",
        "Y6fA3Y9sFbpj",
        "nMW5J9oX3nU9",
        "-BoHvq7YqboP",
        "bBMrmKTfDxET",
        "VigXo_K8sZ9H",
        "_yO-Ce_Ksg31",
        "0WumQUB8Mjb7",
        "2CAA9mzN1z04",
        "0KaxeqvYOge0",
        "PEmt7v9tQF9k",
        "-dTLj2eGQLfE",
        "rkX4Y-9URTLN",
        "skDzIhNbRWYU",
        "JtMMkwCweCbU",
        "-EUVlR7WeGqE",
        "G6gKSbyBeK4L",
        "fFYgDcRmOmWD",
        "UlxxvtBLOBMG",
        "6z9I-wREI2c3",
        "UF7xOcCB-OAe",
        "-fRhXoSZ-UPt",
        "1jFzkvlU-WMs",
        "JSJw7lm5_Ml0",
        "pmCk234X-nqG",
        "aSPveRJMbTyj",
        "BE8I3GYyFAmz",
        "2QkcfM3SOMNc",
        "os-VjzRGFPcE"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}